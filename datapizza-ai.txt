Directory structure:
â””â”€â”€ datapizza-labs-datapizza-ai/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CODE_OF_CONDUCT.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ pyrightconfig.json
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ datapizza-ai-cache/
    â”‚   â””â”€â”€ redis/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ pyproject.toml
    â”‚       â”œâ”€â”€ datapizza/
    â”‚       â”‚   â””â”€â”€ cache/
    â”‚       â”‚       â””â”€â”€ redis/
    â”‚       â”‚           â”œâ”€â”€ __init__.py
    â”‚       â”‚           â””â”€â”€ cache.py
    â”‚       â””â”€â”€ tests/
    â”‚           â””â”€â”€ test_redis_cache.py
    â”œâ”€â”€ datapizza-ai-clients/
    â”‚   â”œâ”€â”€ datapizza-ai-clients-anthropic/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ clients/
    â”‚   â”‚   â”‚       â””â”€â”€ anthropic/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ anthropic_client.py
    â”‚   â”‚   â”‚           â””â”€â”€ memory_adapter.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_anthropic_memory_adapter.py
    â”‚   â”œâ”€â”€ datapizza-ai-clients-bedrock/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ clients/
    â”‚   â”‚   â”‚       â””â”€â”€ bedrock/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ bedrock_client.py
    â”‚   â”‚   â”‚           â””â”€â”€ memory_adapter.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â”œâ”€â”€ test_bedrock_async.py
    â”‚   â”‚       â””â”€â”€ test_bedrock_memory_adapter.py
    â”‚   â”œâ”€â”€ datapizza-ai-clients-google/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ clients/
    â”‚   â”‚   â”‚       â””â”€â”€ google/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ google_client.py
    â”‚   â”‚   â”‚           â””â”€â”€ memory_adapter.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_memory_adapter.py
    â”‚   â”œâ”€â”€ datapizza-ai-clients-mistral/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ clients/
    â”‚   â”‚   â”‚       â””â”€â”€ mistral/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ memory_adapter.py
    â”‚   â”‚   â”‚           â””â”€â”€ mistral_client.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_mistral_client.py
    â”‚   â”œâ”€â”€ datapizza-ai-clients-openai/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ clients/
    â”‚   â”‚   â”‚       â””â”€â”€ openai/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ memory_adapter.py
    â”‚   â”‚   â”‚           â””â”€â”€ openai_client.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ test_base_client.py
    â”‚   â”‚       â””â”€â”€ test_memory_adapter.py
    â”‚   â”œâ”€â”€ datapizza-ai-clients-openai-like/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ clients/
    â”‚   â”‚   â”‚       â””â”€â”€ openai_like/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â”œâ”€â”€ memory_adapter.py
    â”‚   â”‚   â”‚           â””â”€â”€ openai_completion_client.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_openai_completion.py
    â”‚   â””â”€â”€ datapizza-ai-clients-watsonx/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ pyproject.toml
    â”‚       â”œâ”€â”€ datapizza/
    â”‚       â”‚   â””â”€â”€ clients/
    â”‚       â”‚       â””â”€â”€ watsonx/
    â”‚       â”‚           â”œâ”€â”€ __init__.py
    â”‚       â”‚           â”œâ”€â”€ memory_adapter.py
    â”‚       â”‚           â””â”€â”€ watsonx_client.py
    â”‚       â””â”€â”€ tests/
    â”‚           â””â”€â”€ test_watsonx.py
    â”œâ”€â”€ datapizza-ai-core/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â””â”€â”€ datapizza/
    â”‚       â”œâ”€â”€ agents/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ __version__.py
    â”‚       â”‚   â”œâ”€â”€ agent.py
    â”‚       â”‚   â”œâ”€â”€ client_manager.py
    â”‚       â”‚   â”œâ”€â”€ logger.py
    â”‚       â”‚   â””â”€â”€ tests/
    â”‚       â”‚       â””â”€â”€ test_base_agents.py
    â”‚       â”œâ”€â”€ cache/
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ clients/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ factory.py
    â”‚       â”‚   â””â”€â”€ mock_client.py
    â”‚       â”œâ”€â”€ core/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ __version__.py
    â”‚       â”‚   â”œâ”€â”€ constants.py
    â”‚       â”‚   â”œâ”€â”€ models.py
    â”‚       â”‚   â”œâ”€â”€ utils.py
    â”‚       â”‚   â”œâ”€â”€ cache/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â””â”€â”€ cache.py
    â”‚       â”‚   â”œâ”€â”€ clients/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ client.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ models.py
    â”‚       â”‚   â”‚   â””â”€â”€ tests/
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_mock_client.py
    â”‚       â”‚   â”‚       â””â”€â”€ test_token_usage.py
    â”‚       â”‚   â”œâ”€â”€ embedder/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â””â”€â”€ base.py
    â”‚       â”‚   â”œâ”€â”€ executors/
    â”‚       â”‚   â”‚   â””â”€â”€ async_executor.py
    â”‚       â”‚   â”œâ”€â”€ modules/
    â”‚       â”‚   â”‚   â”œâ”€â”€ captioner.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ metatagger.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ parser.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ prompt.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ reranker.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ rewriter.py
    â”‚       â”‚   â”‚   â””â”€â”€ splitter.py
    â”‚       â”‚   â””â”€â”€ vectorstore/
    â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”œâ”€â”€ vectorstore.py
    â”‚       â”‚       â””â”€â”€ tests/
    â”‚       â”‚           â””â”€â”€ test_vectorstore_models.py
    â”‚       â”œâ”€â”€ embedders/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ embedders.py
    â”‚       â”œâ”€â”€ memory/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ __version__.py
    â”‚       â”‚   â”œâ”€â”€ memory.py
    â”‚       â”‚   â”œâ”€â”€ memory_adapter.py
    â”‚       â”‚   â””â”€â”€ tests/
    â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â””â”€â”€ test_memory.py
    â”‚       â”œâ”€â”€ modules/
    â”‚       â”‚   â”œâ”€â”€ captioners/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â””â”€â”€ llm_captioner.py
    â”‚       â”‚   â”œâ”€â”€ metatagger/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ keyword_metatagger.py
    â”‚       â”‚   â”‚   â””â”€â”€ tests/
    â”‚       â”‚   â”‚       â””â”€â”€ test_keyword_metagger.py
    â”‚       â”‚   â”œâ”€â”€ parsers/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ md_parser.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ text_parser.py
    â”‚       â”‚   â”‚   â””â”€â”€ tests/
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_base_parser.py
    â”‚       â”‚   â”‚       â””â”€â”€ test_md_parser.py
    â”‚       â”‚   â”œâ”€â”€ prompt/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ image_rag.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ prompt.py
    â”‚       â”‚   â”‚   â””â”€â”€ tests/
    â”‚       â”‚   â”‚       â””â”€â”€ test_chat_prompt_template.py
    â”‚       â”‚   â”œâ”€â”€ rewriters/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ tool_rewriter.py
    â”‚       â”‚   â”‚   â””â”€â”€ tests/
    â”‚       â”‚   â”‚       â””â”€â”€ test_tool_rewriter.py
    â”‚       â”‚   â”œâ”€â”€ splitters/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ bbox_merger.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ node_splitter.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ pdf_image_splitter.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ recursive_splitter.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ text_splitter.py
    â”‚       â”‚   â”‚   â””â”€â”€ tests/
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_node_splitter.py
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_recursive_splitter.py
    â”‚       â”‚   â”‚       â””â”€â”€ test_text_splitter.py
    â”‚       â”‚   â””â”€â”€ treebuilder/
    â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â”œâ”€â”€ llm_treebuilder.py
    â”‚       â”‚       â””â”€â”€ test/
    â”‚       â”‚           â””â”€â”€ test_llm_treebuilder.py
    â”‚       â”œâ”€â”€ pipeline/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ dag_pipeline.py
    â”‚       â”‚   â”œâ”€â”€ functional_pipeline.py
    â”‚       â”‚   â”œâ”€â”€ pipeline.py
    â”‚       â”‚   â””â”€â”€ tests/
    â”‚       â”‚       â”œâ”€â”€ config.yaml
    â”‚       â”‚       â”œâ”€â”€ config_with_elements.yaml
    â”‚       â”‚       â”œâ”€â”€ dag_config.yaml
    â”‚       â”‚       â”œâ”€â”€ functional_pipeline_config.yaml
    â”‚       â”‚       â”œâ”€â”€ test_functional_pipeline.py
    â”‚       â”‚       â”œâ”€â”€ test_graph_pipeline.py
    â”‚       â”‚       â””â”€â”€ test_pipeline.py
    â”‚       â”œâ”€â”€ tools/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ google.py
    â”‚       â”‚   â”œâ”€â”€ mcp_client.py
    â”‚       â”‚   â”œâ”€â”€ tools.py
    â”‚       â”‚   â”œâ”€â”€ utils.py
    â”‚       â”‚   â””â”€â”€ tests/
    â”‚       â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”‚       â””â”€â”€ test_tools.py
    â”‚       â”œâ”€â”€ tracing/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ memory_exporter.py
    â”‚       â”‚   â”œâ”€â”€ tracing.py
    â”‚       â”‚   â””â”€â”€ tests/
    â”‚       â”‚       â””â”€â”€ test_tracing.py
    â”‚       â””â”€â”€ type/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ type.py
    â”‚           â””â”€â”€ tests/
    â”‚               â””â”€â”€ test_type.py
    â”œâ”€â”€ datapizza-ai-embedders/
    â”‚   â”œâ”€â”€ image_embedder.py
    â”‚   â”œâ”€â”€ cohere/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ embedders/
    â”‚   â”‚   â”‚       â””â”€â”€ cohere/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ cohere.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_base.py
    â”‚   â”œâ”€â”€ fastembedder/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ embedders/
    â”‚   â”‚   â”‚       â””â”€â”€ fastembedder/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ fastembedder.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_fastembedder.py
    â”‚   â”œâ”€â”€ google/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ embedders/
    â”‚   â”‚   â”‚       â””â”€â”€ google/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ google.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_google_embedder.py
    â”‚   â”œâ”€â”€ mistral/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ embedders/
    â”‚   â”‚   â”‚       â””â”€â”€ mistral/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ mistral.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_mistral_embedder.py
    â”‚   â””â”€â”€ openai/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ pyproject.toml
    â”‚       â”œâ”€â”€ datapizza/
    â”‚       â”‚   â””â”€â”€ embedders/
    â”‚       â”‚       â””â”€â”€ openai/
    â”‚       â”‚           â”œâ”€â”€ __init__.py
    â”‚       â”‚           â””â”€â”€ openai.py
    â”‚       â””â”€â”€ tests/
    â”‚           â””â”€â”€ test_openai_embedder.py
    â”œâ”€â”€ datapizza-ai-eval/
    â”‚   â””â”€â”€ metrics.py
    â”œâ”€â”€ datapizza-ai-modules/
    â”‚   â”œâ”€â”€ parsers/
    â”‚   â”‚   â”œâ”€â”€ azure/
    â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ modules/
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ parsers/
    â”‚   â”‚   â”‚   â”‚           â””â”€â”€ azure/
    â”‚   â”‚   â”‚   â”‚               â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚               â””â”€â”€ azure_parser.py
    â”‚   â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚   â”‚       â””â”€â”€ test_azure_parser.py
    â”‚   â”‚   â””â”€â”€ docling/
    â”‚   â”‚       â”œâ”€â”€ README.md
    â”‚   â”‚       â”œâ”€â”€ mypy.ini
    â”‚   â”‚       â”œâ”€â”€ pyproject.toml
    â”‚   â”‚       â””â”€â”€ datapizza/
    â”‚   â”‚           â””â”€â”€ modules/
    â”‚   â”‚               â””â”€â”€ parsers/
    â”‚   â”‚                   â””â”€â”€ docling/
    â”‚   â”‚                       â”œâ”€â”€ __init__.py
    â”‚   â”‚                       â”œâ”€â”€ docling_parser.py
    â”‚   â”‚                       â”œâ”€â”€ ocr_options.py
    â”‚   â”‚                       â”œâ”€â”€ utils.py
    â”‚   â”‚                       â””â”€â”€ tests/
    â”‚   â”‚                           â”œâ”€â”€ conftest.py
    â”‚   â”‚                           â””â”€â”€ test_docling_parser.py
    â”‚   â””â”€â”€ rerankers/
    â”‚       â”œâ”€â”€ cohere/
    â”‚       â”‚   â”œâ”€â”€ README.md
    â”‚       â”‚   â”œâ”€â”€ pyproject.toml
    â”‚       â”‚   â””â”€â”€ datapizza/
    â”‚       â”‚       â””â”€â”€ modules/
    â”‚       â”‚           â””â”€â”€ rerankers/
    â”‚       â”‚               â””â”€â”€ cohere/
    â”‚       â”‚                   â”œâ”€â”€ __init__.py
    â”‚       â”‚                   â””â”€â”€ cohere_reranker.py
    â”‚       â””â”€â”€ together/
    â”‚           â”œâ”€â”€ README.md
    â”‚           â”œâ”€â”€ pyproject.toml
    â”‚           â””â”€â”€ datapizza/
    â”‚               â””â”€â”€ modules/
    â”‚                   â””â”€â”€ rerankers/
    â”‚                       â””â”€â”€ together/
    â”‚                           â”œâ”€â”€ __init__.py
    â”‚                           â””â”€â”€ together_reranker.py
    â”œâ”€â”€ datapizza-ai-tools/
    â”‚   â”œâ”€â”€ duckduckgo/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ tools/
    â”‚   â”‚   â”‚       â””â”€â”€ duckduckgo/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ base.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_ddgs_tools.py
    â”‚   â”œâ”€â”€ filesystem/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ tools/
    â”‚   â”‚   â”‚       â””â”€â”€ filesystem/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ filesystem.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â”œâ”€â”€ test_file_path_matches_pattern.py
    â”‚   â”‚       â””â”€â”€ test_filesystem.py
    â”‚   â”œâ”€â”€ SQLDatabase/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ datapizza/
    â”‚   â”‚   â”‚   â””â”€â”€ tools/
    â”‚   â”‚   â”‚       â””â”€â”€ SQLDatabase/
    â”‚   â”‚   â”‚           â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚           â””â”€â”€ base.py
    â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚       â””â”€â”€ test_sql_database_tool.py
    â”‚   â””â”€â”€ web_fetch/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ pyproject.toml
    â”‚       â”œâ”€â”€ datapizza/
    â”‚       â”‚   â””â”€â”€ tools/
    â”‚       â”‚       â””â”€â”€ web_fetch/
    â”‚       â”‚           â”œâ”€â”€ __init__.py
    â”‚       â”‚           â””â”€â”€ base.py
    â”‚       â””â”€â”€ tests/
    â”‚           â””â”€â”€ test_web_fetch.py
    â”œâ”€â”€ datapizza-ai-vectorstores/
    â”‚   â”œâ”€â”€ datapizza-ai-vectorstores-milvus/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ datapizza/
    â”‚   â”‚       â””â”€â”€ vectorstores/
    â”‚   â”‚           â””â”€â”€ milvus/
    â”‚   â”‚               â”œâ”€â”€ __init__.py
    â”‚   â”‚               â”œâ”€â”€ milvus_vectorstore.py
    â”‚   â”‚               â””â”€â”€ tests/
    â”‚   â”‚                   â””â”€â”€ test_milvus_vectorstore.py
    â”‚   â””â”€â”€ datapizza-ai-vectorstores-qdrant/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ pyproject.toml
    â”‚       â””â”€â”€ datapizza/
    â”‚           â””â”€â”€ vectorstores/
    â”‚               â””â”€â”€ qdrant/
    â”‚                   â”œâ”€â”€ __init__.py
    â”‚                   â”œâ”€â”€ qdrant_vectorstore.py
    â”‚                   â””â”€â”€ tests/
    â”‚                       â””â”€â”€ test_qdrant_vectorstore.py
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ index.md
    â”‚   â”œâ”€â”€ .pages
    â”‚   â”œâ”€â”€ API Reference/
    â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ memory.md
    â”‚   â”‚   â”œâ”€â”€ .pages
    â”‚   â”‚   â”œâ”€â”€ Agents/
    â”‚   â”‚   â”‚   â””â”€â”€ agent.md
    â”‚   â”‚   â”œâ”€â”€ Clients/
    â”‚   â”‚   â”‚   â”œâ”€â”€ cache.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ client_factory.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ clients.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ models.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ .pages
    â”‚   â”‚   â”‚   â””â”€â”€ Avaiable_Clients/
    â”‚   â”‚   â”‚       â”œâ”€â”€ anthropic.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ AzureOpenai.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ google.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ mistral.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ openai-like.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ openai.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ watsonx.md
    â”‚   â”‚   â”‚       â””â”€â”€ .pages
    â”‚   â”‚   â”œâ”€â”€ Embedders/
    â”‚   â”‚   â”‚   â”œâ”€â”€ chunk_embedder.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ cohere_embedder.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ fast_embedder.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ google_embedder.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ mistral_embedder.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_embedder.md
    â”‚   â”‚   â”‚   â””â”€â”€ openai_embedder.md
    â”‚   â”‚   â”œâ”€â”€ Modules/
    â”‚   â”‚   â”‚   â”œâ”€â”€ captioners.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ metatagger.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ rewriters.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ treebuilder.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ .pages
    â”‚   â”‚   â”‚   â”œâ”€â”€ Parsers/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ azure_parser.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ docling_parser.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ text_parser.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ .pages
    â”‚   â”‚   â”‚   â”œâ”€â”€ Prompt/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPromptTemplate.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ImageRAGPrompt.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ .pages
    â”‚   â”‚   â”‚   â”œâ”€â”€ Rerankers/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cohere_reranker.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ index.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ together_reranker.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ .pages
    â”‚   â”‚   â”‚   â””â”€â”€ Splitters/
    â”‚   â”‚   â”‚       â”œâ”€â”€ index.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ node_splitter.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ pdf_image_splitter.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ recursive_splitter.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ text_splitter.md
    â”‚   â”‚   â”‚       â””â”€â”€ .pages
    â”‚   â”‚   â”œâ”€â”€ Pipelines/
    â”‚   â”‚   â”‚   â”œâ”€â”€ dag.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ functional.md
    â”‚   â”‚   â”‚   â””â”€â”€ ingestion.md
    â”‚   â”‚   â”œâ”€â”€ Tools/
    â”‚   â”‚   â”‚   â”œâ”€â”€ duckduckgo.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ filesystem.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ mcp.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ SQLDatabase.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ web_fetch.md
    â”‚   â”‚   â”‚   â””â”€â”€ .pages
    â”‚   â”‚   â”œâ”€â”€ Type/
    â”‚   â”‚   â”‚   â”œâ”€â”€ block.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ chunk.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ media.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ node.md
    â”‚   â”‚   â”‚   â””â”€â”€ tool.md
    â”‚   â”‚   â””â”€â”€ Vectorstore/
    â”‚   â”‚       â”œâ”€â”€ milvus_vectorstore.md
    â”‚   â”‚       â””â”€â”€ qdrant_vectorstore.md
    â”‚   â””â”€â”€ Guides/
    â”‚       â”œâ”€â”€ .pages
    â”‚       â”œâ”€â”€ Agents/
    â”‚       â”‚   â”œâ”€â”€ agent.md
    â”‚       â”‚   â””â”€â”€ mcp.md
    â”‚       â”œâ”€â”€ Clients/
    â”‚       â”‚   â”œâ”€â”€ chatbot.md
    â”‚       â”‚   â”œâ”€â”€ local_model.md
    â”‚       â”‚   â”œâ”€â”€ multimodality.md
    â”‚       â”‚   â”œâ”€â”€ quick_start.md
    â”‚       â”‚   â”œâ”€â”€ streaming.md
    â”‚       â”‚   â”œâ”€â”€ structured_responses.md
    â”‚       â”‚   â”œâ”€â”€ tools.md
    â”‚       â”‚   â””â”€â”€ .pages
    â”‚       â”œâ”€â”€ Monitoring/
    â”‚       â”‚   â”œâ”€â”€ log.md
    â”‚       â”‚   â”œâ”€â”€ tracing.md
    â”‚       â”‚   â””â”€â”€ .pages
    â”‚       â”œâ”€â”€ Pipeline/
    â”‚       â”‚   â”œâ”€â”€ functional_pipeline.md
    â”‚       â”‚   â”œâ”€â”€ ingestion_pipeline.md
    â”‚       â”‚   â”œâ”€â”€ retrieval_pipeline.md
    â”‚       â”‚   â””â”€â”€ .pages
    â”‚       â””â”€â”€ RAG/
    â”‚           â”œâ”€â”€ rag.md
    â”‚           â””â”€â”€ .pages
    â””â”€â”€ .github/
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ bug_report.md
        â”‚   â””â”€â”€ feature_request.md
        â””â”€â”€ workflows/
            â””â”€â”€ test.yml

================================================
FILE: README.md
================================================
<div align="center">

<img src="docs/assets/logo_bg_dark.png" alt="Datapizza AI Logo" width="200" height="200">

**Build reliable Gen AI solutions without overhead**

*Written in Python. Designed for speed. A no-fluff GenAI framework that gets your agents from dev to prod, fast*

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/datapizza-ai.svg)](https://pypi.org/project/datapizza-ai/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Downloads](https://img.shields.io/pypi/dm/datapizza-ai.svg)](https://pypi.org/project/datapizza-ai/)
[![GitHub stars](https://img.shields.io/github/stars/datapizza-labs/datapizza-ai.svg?style=social&label=Star)](https://github.com/datapizza-labs/datapizza-ai)

[ğŸ Homepage](https://datapizza.tech/en/ai-framework/) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](https://docs.datapizza.ai) â€¢ [ğŸ¯ Examples](#-examples) â€¢ [ğŸ¤ Community](#-community)

</div>

---

## ğŸŒŸ Why Datapizza AI?

A framework that keeps your agents predictable, your debugging fast, and your code trusted in production. Built by Engineers, trusted by Engineers.

<div align="center">

### âš¡ **Less abstraction, more control** | ğŸš€ **API-first design** | ğŸ”§ **Observable by design**

</div>

## How to install
```sh
pip install datapizza-ai
```

## Client invoke

```python
from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(api_key="YOUR_API_KEY")
result = client.invoke("Hi, how are u?")
print(result.text)
```

## âœ¨ Key Features

<table>
<tr>
<td width="50%" valign="top">

### ğŸ¯ **API-first**
- **Multi-Provider Support**: OpenAI, Google Gemini, Anthropic, Mistral, Azure
- **Tool Integration**: Built-in web search, document processing, custom tools
- **Memory Management**: Persistent conversations and context awareness

</td>
<td width="50%" valign="top">

### ğŸ” **Composable**
- **Reusable blocks**: Declarative configuration, easy overrides
- **Document Processing**: PDF, DOCX, images with Azure AI & Docling
- **Smart Chunking**: Context-aware text splitting and embedding
- **Built-in reranking**: Add a reranker (e.g., Cohere) to boost relevance

</td>
</tr>
<tr>
<td width="50%" valign="top">

### ğŸ”§ **Observable**
- **OpenTelemetry tracing**: Standards-based instrumentation
- **Client I/O tracing**: Optional toggle to log inputs, outputs, and in-memory context
- **Custom spans**: Trace fine-grained phases and sub-steps to pinpoint bottlenecks

</td>
<td width="50%" valign="top">

### ğŸš€ **Vendor-Agnostic**
- **Swap models**: Change providers without rewiring business logic
- **Clear Interfaces**: Predictable APIs across all components
- **Rich Ecosystem**: Modular design with optional components
- **Migration-friendly**: Quick migration from other frameworks

</td>
</tr>
</table>

## ğŸš€ Quick Start

### Installation

```bash
# Core framework
pip install datapizza-ai

# With specific providers (optional)
pip install datapizza-ai-clients-openai
pip install datapizza-ai-clients-google
pip install datapizza-ai-clients-anthropic
```

### Start with Agent

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools import tool

@tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

client = OpenAIClient(api_key="YOUR_API_KEY")
agent = Agent(name="assistant", client=client, tools = [get_weather])

response = agent.run("What is the weather in Rome?")
# output: The weather in Rome is sunny
```


## ğŸ“Š Detailed Tracing


A key requirement for principled development of LLM applications over your data (RAG systems, agents) is being able to observe and debug.

Datapizza-ai provides built-in observability with OpenTelemetry tracing to help you monitor performance and understand execution flow.

<summary><b>ğŸ” Trace Your AI Operations</b></summary>

```sh
pip install datapizza-ai-tools-duckduckgo
```

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.duckduckgo import DuckDuckGoSearchTool
from datapizza.tracing import ContextTracing

client = OpenAIClient(api_key="OPENAI_API_KEY")
agent = Agent(name="assistant", client=client, tools = [DuckDuckGoSearchTool()])

with ContextTracing().trace("my_ai_operation"):
    response = agent.run("Tell me some news about Bitcoin")

# Output shows:
# â•­â”€ Trace Summary of my_ai_operation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# â”‚ Total Spans: 3                                                      â”‚
# â”‚ Duration: 2.45s                                                     â”‚
# â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ |
# â”‚ â”ƒ Model       â”ƒ Prompt Tokens â”ƒ Completion Tokens â”ƒ Cached Tokens â”ƒ |
# â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© |
# â”‚ â”‚ gpt-4o-mini â”‚ 31            â”‚ 27                â”‚ 0             â”‚ |
# â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ |
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```


![Demo](https://github.com/user-attachments/assets/02742e87-aa48-4308-94c8-6f362e3218b4)


## ğŸ¯ Examples

### ğŸŒ Multi-Agent System

Build sophisticated AI systems where multiple specialized agents collaborate to solve complex tasks. This example shows how to create a trip planning system with dedicated agents for weather information, web search, and planning coordination.

```sh
# Install DuckDuckGo tool
pip install datapizza-ai-tools-duckduckgo
```


```python
from datapizza.agents.agent import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools import tool
from datapizza.tools.duckduckgo import DuckDuckGoSearchTool

client = OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4.1")

@tool
def get_weather(city: str) -> str:
    return f""" it's sunny all the week in {city}"""

weather_agent = Agent(
    name="weather_expert",
    client=client,
    system_prompt="You are a weather expert. Provide detailed weather information and forecasts.",
    tools=[get_weather]
)

web_search_agent = Agent(
    name="web_search_expert",
    client=client,
    system_prompt="You are a web search expert. You can search the web for information.",
    tools=[DuckDuckGoSearchTool()]
)

planner_agent = Agent(
    name="planner",
    client=client,
    system_prompt="You are a trip planner. You should provide a plan for the user. Make sure to provide a detailed plan with the best places to visit and the best time to visit them."
)

planner_agent.can_call([weather_agent, web_search_agent])

response = planner_agent.run(
    "I need to plan a hiking trip in Seattle next week. I want to see some waterfalls and a forest."
)
print(response.text)

```


### ğŸ“Š Document Ingestion

Process and index documents for retrieval-augmented generation (RAG). This pipeline automatically parses PDFs, splits them into chunks, generates embeddings, and stores them in a vector database for efficient similarity search.

```sh
pip install datapizza-ai-parsers-docling
```

```python
from datapizza.core.vectorstore import VectorConfig
from datapizza.embedders import ChunkEmbedder
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.modules.parsers.docling import DoclingParser
from datapizza.modules.splitters import NodeSplitter
from datapizza.pipeline import IngestionPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore

vectorstore = QdrantVectorstore(location=":memory:")
embedder = ChunkEmbedder(client=OpenAIEmbedder(api_key="YOUR_API_KEY", model_name="text-embedding-3-small"))
vectorstore.create_collection("my_documents",vector_config=[VectorConfig(name="embedding", dimensions=1536)])

pipeline = IngestionPipeline(
    modules=[
        DoclingParser(),
        NodeSplitter(max_char=1024),
        embedder,
    ],
    vector_store=vectorstore,
    collection_name="my_documents"
)

pipeline.run("sample.pdf")

results = vectorstore.search(query_vector = [0.0] * 1536, collection_name="my_documents", k=5)
print(results)
```


### ğŸ“Š RAG (Retrieval-Augmented Generation)

Create a complete RAG pipeline that enhances AI responses with relevant document context. This example demonstrates query rewriting, embedding generation, document retrieval, and response generation in a connected workflow.

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.modules.prompt import ChatPromptTemplate
from datapizza.modules.rewriters import ToolRewriter
from datapizza.pipeline import DagPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore

openai_client = OpenAIClient(
    model="gpt-4o-mini",
    api_key="YOUR_API_KEY"
)

dag_pipeline = DagPipeline()
dag_pipeline.add_module("rewriter", ToolRewriter(client=openai_client, system_prompt="Rewrite user queries to improve retrieval accuracy."))
dag_pipeline.add_module("embedder", OpenAIEmbedder(api_key= "YOUR_API_KEY", model_name="text-embedding-3-small"))
dag_pipeline.add_module("retriever", QdrantVectorstore(host="localhost", port=6333).as_retriever(collection_name="my_documents", k=5))
dag_pipeline.add_module("prompt", ChatPromptTemplate(user_prompt_template="User question: {{user_prompt}}\n:", retrieval_prompt_template="Retrieved content:\n{% for chunk in chunks %}{{ chunk.text }}\n{% endfor %}"))
dag_pipeline.add_module("generator", openai_client)

dag_pipeline.connect("rewriter", "embedder", target_key="text")
dag_pipeline.connect("embedder", "retriever", target_key="query_vector")
dag_pipeline.connect("retriever", "prompt", target_key="chunks")
dag_pipeline.connect("prompt", "generator", target_key="memory")

query = "tell me something about this document"
result = dag_pipeline.run({
    "rewriter": {"user_prompt": query},
    "prompt": {"user_prompt": query},
    "retriever": {"collection_name": "my_documents", "k": 3},
    "generator":{"input": query}
})

print(f"Generated response: {result['generator']}")
```


## ğŸŒ Ecosystem

### ğŸ¤– Supported AI Providers

<table >
<tr>
<td align="center"><img src="https://logosandtypes.com/wp-content/uploads/2022/07/OpenAI.png" width="32" style="border-radius: 50%"><br><b>OpenAI</b></td>
<td align="center"><img src="https://www.google.com/favicon.ico" width="32"><br><b>Google Gemini</b></td>
<td align="center"><img src="https://anthropic.com/favicon.ico" width="32"><br><b>Anthropic</b></td>
<td align="center"><img src="https://mistral.ai/favicon.ico" width="32"><br><b>Mistral</b></td>
<td align="center"><img src="https://azure.microsoft.com/favicon.ico" width="32"><br><b>Azure OpenAI</b></td>
</tr>
</table>

### ğŸ”§ Tools & Integrations

| Category | Components |
|----------|------------|
| **ğŸ“„ Document Parsers** | Azure AI Document Intelligence, Docling |
| **ğŸ” Vector Stores** | Qdrant |
| **ğŸ¯ Rerankers** | Cohere, Together AI |
| **ğŸŒ Tools** | DuckDuckGo Search, Custom Tools |
| **ğŸ’¾ Caching** | Redis integration for performance optimization |
| **ğŸ“Š Embedders** | OpenAI, Google, Cohere, FastEmbed |

## ğŸ“ Learning Resources

- ğŸ“– **[Complete Documentation](https://docs.datapizza.ai)** - Comprehensive guides and API reference
- ğŸ¯ **[RAG Tutorial](https://docs.datapizza.ai/latest/Guides/RAG/rag/)** - Build production RAG systems
- ğŸ¤– **[Agent Examples](https://docs.datapizza.ai/latest/Guides/Agents/agent/)** - Real-world agent implementations

## ğŸ¤ Community


- ğŸ’¬ **[Discord Community](https://discord.gg/s5sJNHz2C8)**
- ğŸ“š **[Documentation](https://docs.datapizza.ai)**
- ğŸ“§ **[GitHub Issues](https://github.com/datapizza-labs/datapizza-ai/issues)**
- ğŸ¦ **[Twitter](https://x.com/datapizza_ai)**

### ğŸŒŸ Contributing

We love contributions! Whether it's:

- ğŸ› **Bug Reports** - Help us improve
- ğŸ’¡ **Feature Requests** - Share your ideas
- ğŸ“ **Documentation** - Make it better for everyone
- ğŸ”§ **Code Contributions** - Build the future together

Check out our [Contributing Guide](CONTRIBUTING.md) to get started.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built by Datapizza, the AI native company**

*A framework made to be easy to learn, easy to maintain and ready for production* ğŸ•

[â­ Star us on GitHub](https://github.com/datapizza-labs/datapizza-ai) â€¢ [ğŸš€ Get Started](https://docs.datapizza.ai) â€¢ [ğŸ’¬ Join Discord](https://discord.gg/s5sJNHz2C8)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=datapizza-labs/datapizza-ai&type=Date)](https://www.star-history.com/#datapizza-labs/datapizza-ai&Date)

</div>



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
ai.support@datapizza.tech.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.



================================================
FILE: CONTRIBUTING.md
================================================

# Contributing to datapizza-ai

First off, thanks for taking the time to contribute! â¤ï¸

All types of contributions are encouraged and valued. See the [Table of Contents](#table-of-contents) for different ways to help and details about how this project handles them. Please make sure to read the relevant section before making your contribution. It will make it a lot easier for us maintainers and smooth out the experience for all involved. The community looks forward to your contributions. ğŸ‰

> And if you like the project, but just don't have time to contribute, that's fine. There are other easy ways to support the project and show your appreciation, which we would also be very happy about:
> - Star the project
> - Tweet about it
> - Refer this project in your project's readme
> - Mention the project at local meetups and tell your friends/colleagues

## Table of Contents

- [Code of Conduct](#code-of-conduct)
- [I Have a Question](#i-have-a-question)
- [I Want To Contribute](#i-want-to-contribute)
    - [Reporting Bugs](#reporting-bugs)
    - [Suggesting Enhancements](#suggesting-enhancements)
## Code of Conduct

This project and everyone participating in it is governed by the
[Datapizza Code of Conduct](https://github.com/datapizza-labs/datapizza-ai/blob/main/CODE_OF_CONDUCT.md).
By participating, you are expected to uphold this code. Please report unacceptable behavior
to <ai.support@datapizza.tech>.


## I Have a Question

> If you want to ask a question, we assume that you have read the available [Documentation](https://docs.datapizza.ai).

Before you ask a question, it is best to search for existing [Issues](https://github.com/datapizza-labs/datapizza-ai/issues) that might help you. In case you have found a suitable issue and still need clarification, you can write your question in this issue. It is also advisable to search the internet for answers first.

If you then still feel the need to ask a question and need clarification, we recommend the following:

- Open an [Issue](https://github.com/datapizza-labs/datapizza-ai/issues/new).
- Provide as much context as you can about what you're running into.
- Provide project and platform versions (docker, datapizza-ai, etc), depending on what seems relevant.

We will then take care of the issue as soon as possible.

## I Want To Contribute

> ### Legal Notice <!-- omit in toc -->
> When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license.

### Reporting Bugs

#### Before Submitting a Bug Report

A good bug report shouldn't leave others needing to chase you up for more information. Therefore, we ask you to investigate carefully, collect information and describe the issue in detail in your report. Please complete the following steps in advance to help us fix any potential bug as fast as possible.

- Make sure that you are using the latest version.
- Determine if your bug is really a bug and not an error on your side e.g. using incompatible environment components/versions (Make sure that you have read the [documentation](https://docs.datapizza.ai). If you are looking for support, you might want to check [this section](#i-have-a-question)).
- To see if other users have experienced (and potentially already solved) the same issue you are having, check if there is not already a bug report existing for your bug or error in the [bug tracker](https://github.com/datapizza-labs/datapizza-ai/issues?q=is%3Aissue%20state%3Aopen%20label%3Abug).
- Also make sure to search the internet (including Stack Overflow) to see if users outside of the GitHub community have discussed the issue.
- Collect information about the bug:
    - Stack trace (Traceback)
    - OS, Platform and Version (Windows, Linux, macOS, x86, ARM)
    - Version of the interpreter, compiler, SDK, runtime environment, package manager, depending on what seems relevant.
    - Possibly your input and the output
    - Can you reliably reproduce the issue? And can you also reproduce it with older versions?

#### How Do I Submit a Good Bug Report?

> You must never report security related issues, vulnerabilities or bugs including sensitive information to the issue tracker, or elsewhere in public. Instead sensitive bugs must be sent by email to <ai.support@datapizza.tech>.

We use GitHub issues to track bugs and errors. If you run into an issue with the project:

- Open an [Issue](https://github.com/datapizza-labs/datapizza-ai/issues/new?template=bug_report.md). (Since we can't be sure at this point whether it is a bug or not, we ask you not to talk about a bug yet and not to label the issue.)
- Explain the behavior you would expect and the actual behavior.
- Please provide as much context as possible and describe the *reproduction steps* that someone else can follow to recreate the issue on their own. This usually includes your code. For good bug reports you should isolate the problem and create a reduced test case.
- Provide the information you collected in the previous section.

Once it's filed:

- The project team will label the issue accordingly.
- A team member will try to reproduce the issue with your provided steps. If there are no reproduction steps or no obvious way to reproduce the issue, the team will ask you for those steps and mark the issue as `needs-repro`. Bugs with the `needs-repro` tag will not be addressed until they are reproduced.

<!-- You might want to create an issue template for bugs and errors that can be used as a guide and that defines the structure of the information to be included. If you do so, reference it here in the description. -->


### Suggesting Enhancements

This section guides you through submitting an enhancement suggestion for datapizza-ai, **including completely new features and minor improvements to existing functionality**. Following these guidelines will help maintainers and the community to understand your suggestion and find related suggestions.

<!-- omit in toc -->
#### Before Submitting an Enhancement

- Make sure that you are using the latest version.
- Read the [documentation](https://docs.datapizza.ai) carefully and find out if the functionality is already covered, maybe by an individual configuration.
- Perform a [search](https://github.com/datapizza-labs/datapizza-ai/issues) to see if the enhancement has already been suggested. If it has, add a comment to the existing issue instead of opening a new one.
- Find out whether your idea fits with the scope and aims of the project. It's up to you to make a strong case to convince the project's developers of the merits of this feature. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on/plugin library.

#### How Do I Submit a Good Enhancement Suggestion?

Enhancement suggestions are tracked as [GitHub issues](https://github.com/datapizza-labs/datapizza-ai/issues).

- Use a **clear and descriptive title** for the issue to identify the suggestion.
- Provide a **step-by-step description of the suggested enhancement** in as many details as possible.
- **Describe the current behavior** and **explain which behavior you expected to see instead** and why. At this point you can also tell which alternatives do not work for you.
- **Explain why this enhancement would be useful** to most datapizza-ai users. You may also want to point out the other projects that solved it better and which could serve as inspiration.



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 Datapizza

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: Makefile
================================================

test:
	uv run pytest --tb=short -v --ignore=datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus

watch-tests:
	find . -name "*.py" -not -path "*/site-packages/*" | entr uv run pytest --tb=short -v

format:
	uvx ruff format .

linter-check:
	uvx ruff check .

linter-fix:
	uvx ruff check --fix

linter-force-fix:
	uvx ruff check --fix --unsafe-fixes

dependency-check:
	uv run deptry .

run_docs:
	uv pip install mkdocs-material  pymdown-extensions mkdocs-awesome-pages-plugin mkdocstrings-python
	uv run mkdocs serve --livereload



================================================
FILE: pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai"
version = "0.0.9"
description = "Build reliable Gen AI solutions without overhead"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]

dependencies = [
    "datapizza-ai-core>=0.0.1",
    "datapizza-ai-clients-openai>=0.0.1",
    "datapizza-ai-embedders-openai>=0.0.1",
    "datapizza-ai-vectorstores-qdrant>=0.0.1",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# UV workspace configuration
[tool.uv.workspace]
members = [
    # Core
    "datapizza-ai-core",

    # Clients
    "datapizza-ai-clients/datapizza-ai-clients-anthropic",
    "datapizza-ai-clients/datapizza-ai-clients-bedrock",
    "datapizza-ai-clients/datapizza-ai-clients-google",
    "datapizza-ai-clients/datapizza-ai-clients-mistral",
    "datapizza-ai-clients/datapizza-ai-clients-openai",
    "datapizza-ai-clients/datapizza-ai-clients-openai-like",

    # Embedders
    "datapizza-ai-embedders/cohere",
    "datapizza-ai-embedders/fastembedder",
    "datapizza-ai-embedders/google",
    "datapizza-ai-embedders/mistral",
    "datapizza-ai-embedders/openai",

    # Modules
    "datapizza-ai-modules/parsers/azure",
    #"datapizza-ai-modules/parsers/docling",
    "datapizza-ai-modules/rerankers/cohere",
    "datapizza-ai-modules/rerankers/together",

    # Vectorstores
    "datapizza-ai-vectorstores/datapizza-ai-vectorstores-qdrant",
    "datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus",

    # Cache
    "datapizza-ai-cache/redis",

    # Tools
    "datapizza-ai-tools/duckduckgo",
    "datapizza-ai-tools/sqldatabase",
    "datapizza-ai-tools/filesystem",
    "datapizza-ai-tools/web_fetch",

]

[tool.uv.sources]
# Core
datapizza-ai-core = {workspace = true}

# Clients
datapizza-ai-clients-anthropic = {workspace = true}
datapizza-ai-clients-google = {workspace = true}
datapizza-ai-clients-mistral = {workspace = true}
datapizza-ai-clients-openai = {workspace = true}
datapizza-ai-clients-openai-like = {workspace = true}

# Embedders
datapizza-ai-embedders-cohere = {workspace = true}
datapizza-ai-embedders-fastembedder = {workspace = true}
datapizza-ai-embedders-google = {workspace = true}
datapizza-ai-embedders-mistral = {workspace = true}
datapizza-ai-embedders-openai = {workspace = true}

# Modules
datapizza-ai-parsers-azure = {workspace = true}
#datapizza-ai-parsers-docling = {workspace = true}
datapizza-ai-rerankers-cohere = {workspace = true}
datapizza-ai-rerankers-together = {workspace = true}

# Vectorstores
datapizza-ai-vectorstores-qdrant = {workspace = true}

# Cache
datapizza-ai-cache-redis = {workspace = true}

# Tools
datapizza-ai-tools-duckduckgo = {workspace = true}
datapizza-ai-tools-filesystem = {workspace = true}
datapizza-ai-tools-sqldatabase = {workspace = true}
datapizza-ai-tools-web-fetch = {workspace = true}


# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: pyrightconfig.json
================================================
{
  "include": [
    "datapizza-ai-core",
    "datapizza-ai-clients",
    "datapizza-ai-embedders",
    "datapizza-ai-modules",
    "datapizza-ai-vectorstores",
    "datapizza-ai-cache",
    "datapizza-ai-tools",
    "datapizza-ai-eval"
  ],
  "exclude": [
    "**/tests",
    "build",
    "**/voice"
  ],
  "reportMissingImports": true,
  "typeCheckingMode": "basic",
  "pythonVersion": "3.11",
  "venvPath": ".",
  "venv": ".venv"
}



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.14.1
    hooks:
      - id: ruff-check
        args: [ --fix ]
      - id: ruff-format



================================================
FILE: datapizza-ai-cache/redis/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-cache/redis/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-cache-redis"
version = "0.0.3"
description = "An implementation using Redis for datapizza-ai cache"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.0,<0.1.0",
    "redis>=5.2.1,<6.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-cache/redis/datapizza/cache/redis/__init__.py
================================================
# Import Redis cache implementation
from .cache import RedisCache

__all__ = ["RedisCache"]



================================================
FILE: datapizza-ai-cache/redis/datapizza/cache/redis/cache.py
================================================
import logging
import pickle

from datapizza.core.cache import Cache

import redis

log = logging.getLogger(__name__)


class RedisCache(Cache):
    """
    A Redis-based cache implementation.
    """

    def __init__(
        self, host="localhost", port=6379, db=0, expiration_time=3600
    ):  # 1 hour default
        self.redis = redis.Redis(host=host, port=port, db=db)
        self.expiration_time = expiration_time

    def get(self, key: str) -> str | None:
        """Retrieve and deserialize object"""
        pickled_obj = self.redis.get(key)
        if pickled_obj is None:
            return None
        return pickle.loads(pickled_obj)  # type: ignore

    def set(self, key: str, obj):
        """Serialize and store object"""
        pickled_obj = pickle.dumps(obj)
        self.redis.set(key, pickled_obj, ex=self.expiration_time)



================================================
FILE: datapizza-ai-cache/redis/tests/test_redis_cache.py
================================================
from datapizza.cache.redis import RedisCache


def test_redis_cache():
    cache = RedisCache(host="localhost", port=6379, db=0)
    assert cache is not None



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-anthropic/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-anthropic/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-anthropic"
version = "0.0.4"
description = "Anthropic (Claude) client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "anthropic>=0.40.0,<1.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-anthropic/datapizza/clients/anthropic/__init__.py
================================================
from .anthropic_client import AnthropicClient

__all__ = ["AnthropicClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-anthropic/datapizza/clients/anthropic/anthropic_client.py
================================================
from collections.abc import AsyncIterator, Iterator
from typing import Any, Literal

from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.type import FunctionCallBlock, TextBlock, ThoughtBlock

from anthropic import Anthropic, AsyncAnthropic

from .memory_adapter import AnthropicMemoryAdapter


class AnthropicClient(Client):
    """A client for interacting with the Anthropic API (Claude).

    This class provides methods for invoking the Anthropic API to generate responses
    based on given input data. It extends the Client class.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "claude-3-5-sonnet-latest",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
    ):
        """
        Args:
            api_key: The API key for the Anthropic API.
            model: The model to use for the Anthropic API.
            system_prompt: The system prompt to use for the Anthropic API.
            temperature: The temperature to use for the Anthropic API.
            cache: The cache to use for the Anthropic API.
        """
        if temperature and not 0 <= temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")

        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )
        self.api_key = api_key
        self.memory_adapter = AnthropicMemoryAdapter()
        self._set_client()

    def _set_client(self):
        if not self.client:
            self.client = Anthropic(api_key=self.api_key)

    def _set_a_client(self):
        if not self.a_client:
            self.a_client = AsyncAnthropic(api_key=self.api_key)

    def _convert_tools(self, tools: list[Tool]) -> list[dict[str, Any]]:
        """Convert tools to Anthropic tool format"""
        anthropic_tools = []
        for tool in tools:
            anthropic_tool = {
                "name": tool.name,
                "description": tool.description or "",
                "input_schema": {
                    "type": "object",
                    "properties": tool.properties,
                    "required": tool.required,
                },
            }
            anthropic_tools.append(anthropic_tool)
        return anthropic_tools

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> dict | Literal["auto", "required", "none"]:
        if isinstance(tool_choice, list) and len(tool_choice) > 1:
            raise NotImplementedError(
                "multiple function names is not supported by Anthropic"
            )
        elif isinstance(tool_choice, list):
            return {
                "type": "tool",
                "name": tool_choice[0],
            }
        elif tool_choice == "required":
            return {"type": "any"}
        elif tool_choice == "auto":
            return {"type": "auto"}
        else:
            return tool_choice

    def _response_to_client_response(
        self, response, tool_map: dict[str, Tool] | None = None
    ) -> ClientResponse:
        """Convert Anthropic response to ClientResponse"""
        blocks = []

        if hasattr(response, "content") and response.content:
            if isinstance(
                response.content, list
            ):  # Claude 3 returns a list of content blocks
                for content_block in response.content:
                    if content_block.type == "text":
                        blocks.append(TextBlock(content=content_block.text))
                    elif content_block.type == "thinking":
                        # Summarized thinking content
                        blocks.append(ThoughtBlock(content=content_block.thinking))
                    elif content_block.type == "tool_use":
                        tool = tool_map.get(content_block.name) if tool_map else None
                        if not tool:
                            raise ValueError(f"Tool {content_block.name} not found")

                        blocks.append(
                            FunctionCallBlock(
                                id=content_block.id,
                                name=content_block.name,
                                arguments=content_block.input,
                                tool=tool,
                            )
                        )
            else:  # Handle as string for compatibility
                blocks.append(TextBlock(content=str(response.content)))

        stop_reason = response.stop_reason if hasattr(response, "stop_reason") else None

        return ClientResponse(
            content=blocks,
            stop_reason=stop_reason,
            usage=TokenUsage(
                prompt_tokens=response.usage.input_tokens,
                completion_tokens=response.usage.output_tokens,
                cached_tokens=response.usage.cache_read_input_tokens,
            ),
        )

    def _invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int | None,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        """Implementation of the abstract _invoke method for Anthropic"""
        if tools is None:
            tools = []
        client = self._get_client()
        messages = self._memory_to_contents(None, input, memory)
        # remove the model from the messages
        messages = [message for message in messages if message.get("role") != "model"]

        tool_map = {tool.name: tool for tool in tools}

        request_params = {
            "model": self.model_name,
            "messages": messages,
            "max_tokens": max_tokens or 2048,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if system_prompt:
            request_params["system"] = system_prompt

        if tools:
            request_params["tools"] = self._convert_tools(tools)
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = client.messages.create(**request_params)
        return self._response_to_client_response(response, tool_map)

    async def _a_invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int | None,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        client = self._get_a_client()
        messages = self._memory_to_contents(None, input, memory)
        # remove the model from the messages
        messages = [message for message in messages if message.get("role") != "model"]

        tool_map = {tool.name: tool for tool in tools}

        request_params = {
            "model": self.model_name,
            "messages": messages,
            "max_tokens": max_tokens or 2048,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if system_prompt:
            request_params["system"] = system_prompt

        if tools:
            request_params["tools"] = self._convert_tools(tools)
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = await client.messages.create(**request_params)
        return self._response_to_client_response(response, tool_map)

    def _stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int | None,
        system_prompt: str | None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        """Implementation of the abstract _stream_invoke method for Anthropic"""
        if tools is None:
            tools = []
        messages = self._memory_to_contents(None, input, memory)
        client = self._get_client()

        request_params = {
            "model": self.model_name,
            "messages": messages,
            "stream": True,
            "max_tokens": max_tokens or 2048,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if system_prompt:
            request_params["system"] = system_prompt

        if tools:
            request_params["tools"] = self._convert_tools(tools)
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        stream = client.messages.create(**request_params)

        input_tokens = 0
        output_tokens = 0
        message_text = ""
        thought_text = ""

        for chunk in stream:
            if (
                chunk.type == "content_block_delta"
                and hasattr(chunk, "delta")
                and chunk.delta
            ):
                if hasattr(chunk.delta, "text") and chunk.delta.text:
                    message_text += chunk.delta.text
                    yield ClientResponse(
                        content=[
                            ThoughtBlock(content=thought_text),
                            TextBlock(content=message_text),
                        ],
                        delta=chunk.delta.text,
                    )
                elif hasattr(chunk.delta, "thinking") and chunk.delta.thinking:
                    thought_text += chunk.delta.thinking

            if chunk.type == "message_start":
                input_tokens = (
                    chunk.message.usage.input_tokens if chunk.message.usage else 0
                )

            if chunk.type == "message_delta":
                output_tokens = max(
                    output_tokens, chunk.usage.output_tokens if chunk.usage else 0
                )

        yield ClientResponse(
            content=[
                ThoughtBlock(content=thought_text),
                TextBlock(content=message_text),
            ],
            delta="",
            stop_reason="end_turn",
            usage=TokenUsage(
                prompt_tokens=input_tokens,
                completion_tokens=output_tokens,
                cached_tokens=0,
            ),
        )

    async def _a_stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        """Implementation of the abstract _a_stream_invoke method for Anthropic"""
        if tools is None:
            tools = []
        messages = self._memory_to_contents(None, input, memory)
        client = self._get_a_client()

        request_params = {
            "model": self.model_name,
            "messages": messages,
            "stream": True,
            "max_tokens": max_tokens or 2048,
            **kwargs,
        }
        if temperature:
            request_params["temperature"] = temperature
        if system_prompt:
            request_params["system"] = system_prompt

        if max_tokens:
            request_params["max_tokens"] = max_tokens

        if tools:
            request_params["tools"] = self._convert_tools(tools)
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        stream = await client.messages.create(**request_params)

        input_tokens = 0
        output_tokens = 0
        message_text = ""
        thought_text = ""

        async for chunk in stream:
            if (
                chunk.type == "content_block_delta"
                and hasattr(chunk, "delta")
                and chunk.delta
            ):
                if hasattr(chunk.delta, "text") and chunk.delta.text:
                    message_text += chunk.delta.text
                    yield ClientResponse(
                        content=[
                            ThoughtBlock(content=thought_text),
                            TextBlock(content=message_text),
                        ],
                        delta=chunk.delta.text,
                    )
                elif hasattr(chunk.delta, "thinking") and chunk.delta.thinking:
                    thought_text += chunk.delta.thinking

            if chunk.type == "message_start":
                input_tokens = (
                    chunk.message.usage.input_tokens if chunk.message.usage else 0
                )

            if chunk.type == "message_delta":
                output_tokens = max(
                    output_tokens, chunk.usage.output_tokens if chunk.usage else 0
                )

        yield ClientResponse(
            content=[
                ThoughtBlock(content=thought_text),
                TextBlock(content=message_text),
            ],
            delta="",
            stop_reason="end_turn",
            usage=TokenUsage(
                prompt_tokens=input_tokens,
                completion_tokens=output_tokens,
                cached_tokens=0,
            ),
        )

    def _structured_response(
        self,
        *args,
        **kwargs,
    ) -> ClientResponse:
        raise NotImplementedError("Anthropic does not support structured responses")

    async def _a_structured_response(self, *args, **kwargs):
        raise NotImplementedError("Anthropic does not support structured responses")



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-anthropic/datapizza/clients/anthropic/memory_adapter.py
================================================
import base64
import json

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)


class AnthropicMemoryAdapter(MemoryAdapter):
    """Adapter for converting Memory objects to Anthropic API message format"""

    def _turn_to_message(self, turn: Turn) -> dict:
        content = []
        for block in turn:
            block_dict = {}

            match block:
                case TextBlock():
                    block_dict = {"type": "text", "text": block.content}
                case FunctionCallBlock():
                    block_dict = json.dumps(
                        {
                            "type": "tool_call",
                            "id": block.id,
                            "tool_name": block.name,
                            "tool_args": block.arguments,
                        }
                    )

                case FunctionCallResultBlock():
                    block_dict = json.dumps(
                        {
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": block.result,
                        }
                    )
                case StructuredBlock():
                    block_dict = {
                        "type": "text",
                        "text": str(block.content),
                    }
                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            block_dict = self._process_image_block(block)
                        case "pdf":
                            block_dict = self._process_pdf_block(block)

                        case _:
                            raise NotImplementedError(
                                f"Unsupported media type: {block.media.media_type}"
                            )

            content.append(block_dict)

        if all(isinstance(block, dict) for block in content) and all(
            list(block.keys()) == ["type", "text"] for block in content
        ):
            content = "".join([block["text"] for block in content])

        if len(content) == 1:
            content = content[0]

        return {
            "role": turn.role.anthropic_role,
            "content": (content),
        }

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        """Convert text and role to Anthropic message format"""
        # Anthropic uses 'user', 'assistant', and 'system' roles

        return {"role": role.anthropic_role, "content": text}

    def _process_pdf_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return {
                    "type": "document",
                    "source": {
                        "type": "url",
                        "url": block.media.source,
                    },
                }

            case "base64":
                return {
                    "type": "document",
                    "source": {
                        "type": "base64",
                        "media_type": "application/pdf",
                        "data": block.media.source,
                    },
                }

            case "path":
                with open(block.media.source, "rb") as f:
                    base64_pdf = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "document",
                    "source": {
                        "type": "base64",
                        "media_type": "application/pdf",
                        "data": base64_pdf,
                    },
                }

            case _:
                raise NotImplementedError("Source type not supported")

    def _process_image_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return {
                    "type": "image",
                    "source": {
                        "type": "url",
                        "url": block.media.source,
                    },
                }

            case "base64":
                return {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": f"image/{block.media.extension}",
                        "data": block.media.source,
                    },
                }

            case "path":
                with open(block.media.source, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                return {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": f"image/{block.media.extension}",
                        "data": base64_image,
                    },
                }
            case _:
                raise NotImplementedError(
                    f"Unsupported media type: {block.media.media_type}"
                )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-anthropic/tests/test_anthropic_memory_adapter.py
================================================
from datapizza.memory.memory import Memory
from datapizza.type import ROLE, TextBlock

from datapizza.clients.anthropic import AnthropicClient
from datapizza.clients.anthropic.memory_adapter import AnthropicMemoryAdapter


def test_init_anthropic_client():
    client = AnthropicClient(api_key="test")
    assert client is not None


def test_anthropic_memory_adapter():
    memory_adapter = AnthropicMemoryAdapter()
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.ASSISTANT)

    messages = memory_adapter.memory_to_messages(memory)

    assert messages == [
        {
            "role": "user",
            "content": "Hello, world!",
        },
        {
            "role": "assistant",
            "content": "Hello, world!",
        },
    ]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/README.md
================================================
# DataPizza AI - AWS Bedrock Client

AWS Bedrock client implementation for the datapizza-ai framework. This client provides seamless integration with AWS Bedrock's Converse API, supporting various foundation models including Anthropic's Claude models.

## Features

- Full support for AWS Bedrock Converse API
- Multiple authentication methods (AWS Profile, Access Keys, Environment Variables)
- Streaming and non-streaming responses
- Tool/function calling support
- Memory/conversation history management
- Image and document (PDF) support
- Async support

## Installation

```bash
pip install datapizza-ai-clients-bedrock
```

Or install from source in editable mode:

```bash
cd datapizza-ai/datapizza-ai-clients/datapizza-ai-clients-bedrock
pip install -e .
```

## Quick Start

### Basic Usage

```python
from datapizza.clients.bedrock import BedrockClient

# Using AWS Profile
client = BedrockClient(
    profile_name="my-aws-profile",
    region_name="us-east-1"
)

# Or using access keys
client = BedrockClient(
    aws_access_key_id="YOUR_ACCESS_KEY",
    aws_secret_access_key="YOUR_SECRET_KEY",
    region_name="us-east-1"
)

# Simple invocation
result = client.invoke("What is AWS Bedrock?")

# Extract text from response
for block in result.content:
    if hasattr(block, 'content'):
        print(block.content)
```

## Authentication Methods

The client supports multiple authentication methods in the following priority order:

### 1. Explicit Credentials

```python
client = BedrockClient(
    aws_access_key_id="YOUR_ACCESS_KEY",
    aws_secret_access_key="YOUR_SECRET_KEY",
    aws_session_token="YOUR_SESSION_TOKEN",  # Optional, for temporary credentials
    region_name="us-east-1"
)
```

### 2. AWS Profile

```python
client = BedrockClient(
    profile_name="my-aws-profile",
    region_name="us-east-1"
)
```

### 3. Environment Variables

Set these environment variables:
```bash
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_SESSION_TOKEN="your-session-token"  # Optional
export AWS_PROFILE="my-aws-profile"  # Or use profile
```

Then initialize without parameters:
```python
client = BedrockClient(region_name="us-east-1")
```

### 4. Default AWS Credentials Chain

If no credentials are provided, boto3 will use the default credentials chain (IAM roles, ~/.aws/credentials, etc.)

```python
client = BedrockClient(region_name="us-east-1")
```

## Available Models

The client works with any Bedrock model that supports the Converse API. Popular models include:

- `anthropic.claude-3-5-sonnet-20241022-v2:0` (default)
- `anthropic.claude-3-5-sonnet-20240620-v1:0`
- `anthropic.claude-3-opus-20240229-v1:0`
- `anthropic.claude-3-sonnet-20240229-v1:0`
- `anthropic.claude-3-haiku-20240307-v1:0`
- `meta.llama3-70b-instruct-v1:0`
- `mistral.mistral-large-2402-v1:0`
- And many more...

```python
client = BedrockClient(
    model="anthropic.claude-3-opus-20240229-v1:0",
    region_name="us-east-1"
)
```

## Usage Examples

### With System Prompt

```python
client = BedrockClient(
    system_prompt="You are a helpful coding assistant specialized in Python.",
    region_name="us-east-1"
)

result = client.invoke("How do I read a CSV file?")
```

### Streaming Responses

```python
for chunk in client.stream_invoke("Tell me a long story"):
    if chunk.delta:
        print(chunk.delta, end="", flush=True)
print()
```

### With Memory (Conversation History)

```python
from datapizza.memory import Memory

memory = Memory()
client = BedrockClient(region_name="us-east-1")

# First message
result1 = client.invoke("My favorite color is blue", memory=memory)

# The conversation is tracked in memory
result2 = client.invoke("What's my favorite color?", memory=memory)
# Response: "Your favorite color is blue."
```

### With Temperature and Max Tokens

```python
result = client.invoke(
    "Write a creative story",
    temperature=0.9,  # Higher = more creative (0-1)
    max_tokens=1000
)
```

### With Tools/Function Calling

```python
from datapizza.tools import Tool

def get_weather(location: str, unit: str = "celsius") -> str:
    """Get the weather for a location"""
    return f"The weather in {location} is 22Â°{unit[0].upper()}"

weather_tool = Tool(
    name="get_weather",
    description="Get the current weather for a location",
    function=get_weather,
    properties={
        "location": {
            "type": "string",
            "description": "The city name"
        },
        "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit"
        }
    },
    required=["location"]
)

result = client.invoke(
    "What's the weather in Paris?",
    tools=[weather_tool]
)

# Check for function calls
for block in result.content:
    if isinstance(block, FunctionCallBlock):
        print(f"Function: {block.name}")
        print(f"Arguments: {block.arguments}")
```

### Async Support

```python
import asyncio

async def main():
    client = BedrockClient(region_name="us-east-1")
    result = await client.a_invoke("Hello!")
    print(result.content[0].content)

asyncio.run(main())
```

### Async Streaming

```python
async def stream_example():
    client = BedrockClient(region_name="us-east-1")
    async for chunk in client.a_stream_invoke("Count to 10"):
        if chunk.delta:
            print(chunk.delta, end="", flush=True)

asyncio.run(stream_example())
```

## Configuration

### Constructor Parameters

```python
BedrockClient(
    model: str = "anthropic.claude-3-5-sonnet-20241022-v2:0",
    system_prompt: str = "",
    temperature: float | None = None,  # 0-1 for most models
    cache: Cache | None = None,
    region_name: str = "us-east-1",
    aws_access_key_id: str | None = None,
    aws_secret_access_key: str | None = None,
    aws_session_token: str | None = None,
    profile_name: str | None = None,
)
```

### Invoke Parameters

```python
client.invoke(
    input: str,                    # The user message
    tools: list[Tool] | None = None,
    memory: Memory | None = None,
    tool_choice: "auto" | "required" | "none" | list[str] = "auto",
    temperature: float | None = None,
    max_tokens: int = 2048,
    system_prompt: str | None = None,  # Override instance system_prompt
)
```

## Response Format

All methods return a `ClientResponse` object:

```python
response = client.invoke("Hello")

# Access content blocks
for block in response.content:
    if isinstance(block, TextBlock):
        print(block.content)  # The text
    elif isinstance(block, FunctionCallBlock):
        print(block.name)      # Function name
        print(block.arguments) # Function arguments

# Token usage
print(f"Prompt tokens: {response.prompt_tokens_used}")
print(f"Completion tokens: {response.completion_tokens_used}")
print(f"Stop reason: {response.stop_reason}")
```

## IAM Permissions

Your AWS credentials need the following permissions:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel",
                "bedrock:InvokeModelWithResponseStream"
            ],
            "Resource": [
                "arn:aws:bedrock:*::foundation-model/*"
            ]
        }
    ]
}
```

## Model Access

Before using a model, you need to request access in the AWS Bedrock console:

1. Go to AWS Bedrock console
2. Navigate to "Model access"
3. Request access to the models you want to use
4. Wait for approval (usually instant for most models)

## Limitations

- Structured responses are not natively supported (unlike OpenAI's structured output)
- Some advanced features may vary by model
- Token usage metrics may not include caching information

## Error Handling

```python
from botocore.exceptions import BotoCoreError, ClientError

try:
    result = client.invoke("Hello")
except ClientError as e:
    if e.response['Error']['Code'] == 'AccessDeniedException':
        print("Model access not granted. Check Bedrock console.")
    elif e.response['Error']['Code'] == 'ResourceNotFoundException':
        print("Model not found in this region.")
    else:
        print(f"AWS Error: {e}")
except BotoCoreError as e:
    print(f"Boto3 Error: {e}")
```

## Development

### Running Tests

```bash
pip install -e ".[dev]"
pytest tests/
```

### Code Formatting

```bash
ruff check .
ruff format .
```

## License

MIT License - see LICENSE file for details

## Contributing

Contributions are welcome! Please see the main datapizza-ai repository for contribution guidelines.

## Support

For issues and questions:
- GitHub Issues: [datapizza-ai repository](https://github.com/datapizza/datapizza-ai)
- Documentation: [DataPizza AI Docs](https://docs.datapizza.ai)



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-bedrock"
version = "0.0.4"
description = "AWS Bedrock client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "boto3>=1.35.0",
    "botocore>=1.35.0",
    "aioboto3>=13.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
    "moto>=5.0.0",  # For mocking AWS services in tests
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/datapizza/clients/bedrock/__init__.py
================================================
from .bedrock_client import BedrockClient

__all__ = ["BedrockClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/datapizza/clients/bedrock/bedrock_client.py
================================================
import os
from collections.abc import AsyncIterator, Iterator
from typing import Any, Literal

import aioboto3
import boto3
from botocore.config import Config
from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.type import FunctionCallBlock, TextBlock

from .memory_adapter import BedrockMemoryAdapter


class BedrockClient(Client):
    """A client for interacting with AWS Bedrock API.

    This class provides methods for invoking AWS Bedrock models (Claude, etc.)
    to generate responses. It extends the Client class and supports authentication
    via AWS profile or access keys.
    """

    def __init__(
        self,
        model: str = "anthropic.claude-3-5-sonnet-20241022-v2:0",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
        region_name: str = "us-east-1",
        aws_access_key_id: str | None = None,
        aws_secret_access_key: str | None = None,
        aws_session_token: str | None = None,
        profile_name: str | None = None,
    ):
        """
        Args:
            model: The Bedrock model to use (e.g., 'anthropic.claude-3-5-sonnet-20241022-v2:0').
            system_prompt: The system prompt to use for the model.
            temperature: The temperature to use for generation (0-1 for most models).
            cache: The cache to use for responses.
            region_name: AWS region name (default: us-east-1).
            aws_access_key_id: AWS access key ID (optional, can use AWS_ACCESS_KEY_ID env var).
            aws_secret_access_key: AWS secret access key (optional, can use AWS_SECRET_ACCESS_KEY env var).
            aws_session_token: AWS session token (optional, for temporary credentials).
            profile_name: AWS profile name (optional, can use AWS_PROFILE env var).
        """
        if temperature and not 0 <= temperature <= 1:
            raise ValueError("Temperature must be between 0 and 1 for Bedrock models")

        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )

        self.region_name = region_name
        self.aws_access_key_id = aws_access_key_id or os.getenv("AWS_ACCESS_KEY_ID")
        self.aws_secret_access_key = aws_secret_access_key or os.getenv(
            "AWS_SECRET_ACCESS_KEY"
        )
        self.aws_session_token = aws_session_token or os.getenv("AWS_SESSION_TOKEN")
        self.profile_name = profile_name or os.getenv("AWS_PROFILE")

        self.memory_adapter = BedrockMemoryAdapter()
        self._set_client()

    def _set_client(self):
        if not self.client:
            session_kwargs = {}

            # Priority: explicit credentials > profile > default credentials
            if self.aws_access_key_id and self.aws_secret_access_key:
                session_kwargs["aws_access_key_id"] = self.aws_access_key_id
                session_kwargs["aws_secret_access_key"] = self.aws_secret_access_key
                if self.aws_session_token:
                    session_kwargs["aws_session_token"] = self.aws_session_token
            elif self.profile_name:
                session_kwargs["profile_name"] = self.profile_name

            session = boto3.Session(**session_kwargs)

            # Create bedrock-runtime client with retry configuration
            config = Config(
                retries={"max_attempts": 3, "mode": "adaptive"},
                read_timeout=300,
            )

            self.client = session.client(
                service_name="bedrock-runtime",
                region_name=self.region_name,
                config=config,
            )

    def _set_a_client(self):
        """Initialize async bedrock-runtime client using aioboto3"""
        if not self.a_client:
            session_kwargs = {}

            # Priority: explicit credentials > profile > default credentials
            if self.aws_access_key_id and self.aws_secret_access_key:
                session_kwargs["aws_access_key_id"] = self.aws_access_key_id
                session_kwargs["aws_secret_access_key"] = self.aws_secret_access_key
                if self.aws_session_token:
                    session_kwargs["aws_session_token"] = self.aws_session_token
            elif self.profile_name:
                session_kwargs["profile_name"] = self.profile_name

            # Create async session
            session = aioboto3.Session(**session_kwargs)

            # Create bedrock-runtime client with retry configuration
            config = Config(
                retries={"max_attempts": 3, "mode": "adaptive"},
                read_timeout=300,
            )

            # Store the session and config for async client creation
            self.a_session = session
            self.a_config = config
            self.a_region_name = self.region_name

    def _convert_tools(self, tools: list[Tool]) -> list[dict[str, Any]]:
        """Convert tools to Bedrock tool format (similar to Anthropic)"""
        bedrock_tools = []
        for tool in tools:
            bedrock_tool = {
                "toolSpec": {
                    "name": tool.name,
                    "description": tool.description or "",
                    "inputSchema": {
                        "json": {
                            "type": "object",
                            "properties": tool.properties,
                            "required": tool.required,
                        }
                    },
                }
            }
            bedrock_tools.append(bedrock_tool)
        return bedrock_tools

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> dict:
        """Convert tool choice to Bedrock format"""
        if isinstance(tool_choice, list):
            if len(tool_choice) > 1:
                raise NotImplementedError(
                    "Multiple function names not supported by Bedrock"
                )
            return {"tool": {"name": tool_choice[0]}}
        elif tool_choice == "required":
            return {"any": {}}
        elif tool_choice == "auto":
            return {"auto": {}}
        else:  # none
            return {}

    def _response_to_client_response(
        self, response: dict, tool_map: dict[str, Tool] | None = None
    ) -> ClientResponse:
        """Convert Bedrock response to ClientResponse"""
        blocks = []

        # Parse the response body
        response_body = response.get("output", {})

        # Handle message content
        message = response_body.get("message", {})
        content_items = message.get("content", [])

        for content_item in content_items:
            if "text" in content_item:
                blocks.append(TextBlock(content=content_item["text"]))
            elif "toolUse" in content_item:
                tool_use = content_item["toolUse"]
                tool = tool_map.get(tool_use["name"]) if tool_map else None
                if not tool:
                    raise ValueError(f"Tool {tool_use['name']} not found")

                blocks.append(
                    FunctionCallBlock(
                        id=tool_use["toolUseId"],
                        name=tool_use["name"],
                        arguments=tool_use["input"],
                        tool=tool,
                    )
                )

        # Extract usage information
        usage = response.get("usage", {})
        prompt_tokens = usage.get("inputTokens", 0)
        completion_tokens = usage.get("outputTokens", 0)

        # Extract stop reason
        stop_reason = response.get("stopReason")

        return ClientResponse(
            content=blocks,
            stop_reason=stop_reason,
            usage=TokenUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                cached_tokens=0,  # Bedrock doesn't expose cache metrics in the same way
            ),
        )

    def _invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        """Implementation of the abstract _invoke method for Bedrock"""
        if tools is None:
            tools = []

        client = self._get_client()
        messages = self._memory_to_contents(None, input, memory)

        # Remove model role messages (Bedrock doesn't support this)
        messages = [message for message in messages if message.get("role") != "model"]

        tool_map = {tool.name: tool for tool in tools}

        # Build the request body according to Bedrock Converse API
        request_body = {
            "modelId": self.model_name,
            "messages": messages,
            "inferenceConfig": {
                "maxTokens": max_tokens or 2048,
            },
        }

        if temperature is not None:
            request_body["inferenceConfig"]["temperature"] = temperature

        # Add system prompt if provided
        if system_prompt:
            request_body["system"] = [{"text": system_prompt}]

        # Add tools if provided
        if tools:
            request_body["toolConfig"] = {
                "tools": self._convert_tools(tools),
                "toolChoice": self._convert_tool_choice(tool_choice),
            }

        # Add any additional kwargs
        request_body.update(kwargs)

        # Call Bedrock Converse API
        response = client.converse(**request_body)

        return self._response_to_client_response(response, tool_map)

    async def _a_invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        """Async implementation using aioboto3"""
        if tools is None:
            tools = []

        # Ensure async client is initialized
        if not hasattr(self, "a_session"):
            self._set_a_client()

        messages = self._memory_to_contents(None, input, memory)

        # Remove model role messages (Bedrock doesn't support this)
        messages = [message for message in messages if message.get("role") != "model"]

        tool_map = {tool.name: tool for tool in tools}

        # Build the request body according to Bedrock Converse API
        request_body = {
            "modelId": self.model_name,
            "messages": messages,
            "inferenceConfig": {
                "maxTokens": max_tokens or 2048,
            },
        }

        if temperature is not None:
            request_body["inferenceConfig"]["temperature"] = temperature

        # Add system prompt if provided
        if system_prompt:
            request_body["system"] = [{"text": system_prompt}]

        # Add tools if provided
        if tools:
            request_body["toolConfig"] = {
                "tools": self._convert_tools(tools),
                "toolChoice": self._convert_tool_choice(tool_choice),
            }

        # Add any additional kwargs
        request_body.update(kwargs)

        # Call Bedrock Converse API asynchronously
        async with self.a_session.client(
            service_name="bedrock-runtime",
            region_name=self.a_region_name,
            config=self.a_config,
        ) as client:
            response = await client.converse(**request_body)

        return self._response_to_client_response(response, tool_map)

    def _stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        """Implementation of streaming for Bedrock"""
        if tools is None:
            tools = []

        client = self._get_client()
        messages = self._memory_to_contents(None, input, memory)

        # Remove model role messages
        messages = [message for message in messages if message.get("role") != "model"]

        # Build the request body
        request_body = {
            "modelId": self.model_name,
            "messages": messages,
            "inferenceConfig": {
                "maxTokens": max_tokens or 2048,
            },
        }

        if temperature is not None:
            request_body["inferenceConfig"]["temperature"] = temperature

        if system_prompt:
            request_body["system"] = [{"text": system_prompt}]

        if tools:
            request_body["toolConfig"] = {
                "tools": self._convert_tools(tools),
                "toolChoice": self._convert_tool_choice(tool_choice),
            }

        request_body.update(kwargs)

        # Call Bedrock ConverseStream API
        response = client.converse_stream(**request_body)

        # Process streaming response
        message_text = ""
        input_tokens = 0
        output_tokens = 0
        stop_reason = None

        stream = response.get("stream")
        if stream:
            for event in stream:
                if "contentBlockDelta" in event:
                    delta = event["contentBlockDelta"].get("delta", {})
                    if "text" in delta:
                        text_delta = delta["text"]
                        message_text += text_delta
                        yield ClientResponse(
                            content=[TextBlock(content=message_text)],
                            delta=text_delta,
                            stop_reason=None,
                        )

                elif "metadata" in event:
                    metadata = event["metadata"]
                    usage = metadata.get("usage", {})
                    input_tokens = usage.get("inputTokens", 0)
                    output_tokens = usage.get("outputTokens", 0)

                elif "messageStop" in event:
                    stop_reason = event["messageStop"].get("stopReason")

        # Final response with complete information
        yield ClientResponse(
            content=[TextBlock(content=message_text)],
            delta="",
            stop_reason=stop_reason,
            usage=TokenUsage(
                prompt_tokens=input_tokens,
                completion_tokens=output_tokens,
                cached_tokens=0,
            ),
        )

    async def _a_stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        """Async streaming implementation using aioboto3"""
        if tools is None:
            tools = []

        # Ensure async client is initialized
        if not hasattr(self, "a_session"):
            self._set_a_client()

        messages = self._memory_to_contents(None, input, memory)

        # Remove model role messages
        messages = [message for message in messages if message.get("role") != "model"]

        # Build the request body
        request_body = {
            "modelId": self.model_name,
            "messages": messages,
            "inferenceConfig": {
                "maxTokens": max_tokens or 2048,
            },
        }

        if temperature is not None:
            request_body["inferenceConfig"]["temperature"] = temperature

        if system_prompt:
            request_body["system"] = [{"text": system_prompt}]

        if tools:
            request_body["toolConfig"] = {
                "tools": self._convert_tools(tools),
                "toolChoice": self._convert_tool_choice(tool_choice),
            }

        request_body.update(kwargs)

        # Call Bedrock ConverseStream API asynchronously
        async with self.a_session.client(
            service_name="bedrock-runtime",
            region_name=self.a_region_name,
            config=self.a_config,
        ) as client:
            response = await client.converse_stream(**request_body)

            # Process streaming response
            message_text = ""
            input_tokens = 0
            output_tokens = 0
            stop_reason = None

            stream = response.get("stream")
            if stream:
                async for event in stream:
                    if "contentBlockDelta" in event:
                        delta = event["contentBlockDelta"].get("delta", {})
                        if "text" in delta:
                            text_delta = delta["text"]
                            message_text += text_delta
                            yield ClientResponse(
                                content=[TextBlock(content=message_text)],
                                delta=text_delta,
                                stop_reason=None,
                            )

                    elif "metadata" in event:
                        metadata = event["metadata"]
                        usage = metadata.get("usage", {})
                        input_tokens = usage.get("inputTokens", 0)
                        output_tokens = usage.get("outputTokens", 0)

                    elif "messageStop" in event:
                        stop_reason = event["messageStop"].get("stopReason")

            # Final response with complete information
            yield ClientResponse(
                content=[TextBlock(content=message_text)],
                delta="",
                stop_reason=stop_reason,
                usage=TokenUsage(
                    prompt_tokens=input_tokens,
                    completion_tokens=output_tokens,
                    cached_tokens=0,
                ),
            )

    def _structured_response(
        self,
        input: str,
        output_cls: type,
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        """Bedrock doesn't natively support structured output like OpenAI

        This would require prompting techniques or using Anthropic's prompt caching
        """
        raise NotImplementedError(
            "Bedrock doesn't natively support structured responses. "
            "Consider using prompt engineering or JSON mode with validation."
        )

    async def _a_structured_response(self, *args, **kwargs):
        raise NotImplementedError(
            "Bedrock doesn't natively support structured responses"
        )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/datapizza/clients/bedrock/memory_adapter.py
================================================
import base64
import json

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)


class BedrockMemoryAdapter(MemoryAdapter):
    """Adapter for converting Memory objects to AWS Bedrock Converse API message format

    The Bedrock Converse API uses a message format similar to Anthropic's Claude API.
    """

    def _turn_to_message(self, turn: Turn) -> dict:
        """Convert a Turn to Bedrock message format"""
        content = []

        for block in turn:
            block_dict = {}

            match block:
                case TextBlock():
                    block_dict = {"text": block.content}

                case FunctionCallBlock():
                    # Bedrock uses toolUse format
                    block_dict = {
                        "toolUse": {
                            "toolUseId": block.id,
                            "name": block.name,
                            "input": block.arguments,
                        }
                    }

                case FunctionCallResultBlock():
                    # Bedrock uses toolResult format
                    block_dict = {
                        "toolResult": {
                            "toolUseId": block.id,
                            "content": [{"text": str(block.result)}],
                        }
                    }

                case StructuredBlock():
                    # Convert structured content to text
                    block_dict = {
                        "text": json.dumps(block.content)
                        if not isinstance(block.content, str)
                        else block.content,
                    }

                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            block_dict = self._process_image_block(block)
                        case "pdf":
                            block_dict = self._process_document_block(block)
                        case _:
                            raise NotImplementedError(
                                f"Unsupported media type: {block.media.media_type}"
                            )

            content.append(block_dict)

        # Bedrock expects content as a list
        return {
            "role": self._convert_role(turn.role),
            "content": content,
        }

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        """Convert text and role to Bedrock message format"""
        return {
            "role": self._convert_role(role),
            "content": [{"text": text}],
        }

    def _convert_role(self, role: ROLE) -> str:
        """Convert ROLE to Bedrock role string

        Bedrock Converse API supports 'user' and 'assistant' roles
        """
        if role.value == "user":
            return "user"
        elif role.value == "assistant":
            return "assistant"
        else:
            # Default to user for system or other roles
            return "user"

    def _process_document_block(self, block: MediaBlock) -> dict:
        """Process document (PDF) blocks for Bedrock"""
        match block.media.source_type:
            case "base64":
                return {
                    "document": {
                        "format": "pdf",
                        "name": "document.pdf",
                        "source": {"bytes": base64.b64decode(block.media.source)},
                    }
                }

            case "path":
                with open(block.media.source, "rb") as f:
                    pdf_bytes = f.read()
                return {
                    "document": {
                        "format": "pdf",
                        "name": block.media.source.split("/")[-1],
                        "source": {"bytes": pdf_bytes},
                    }
                }

            case "url":
                raise NotImplementedError(
                    "Bedrock Converse API does not support document URLs directly. "
                    "Please download and provide as bytes."
                )

            case _:
                raise NotImplementedError(
                    f"Unsupported source type: {block.media.source_type}"
                )

    def _process_image_block(self, block: MediaBlock) -> dict:
        """Process image blocks for Bedrock"""
        match block.media.source_type:
            case "base64":
                return {
                    "image": {
                        "format": block.media.extension or "png",
                        "source": {"bytes": base64.b64decode(block.media.source)},
                    }
                }

            case "path":
                with open(block.media.source, "rb") as image_file:
                    image_bytes = image_file.read()
                return {
                    "image": {
                        "format": block.media.extension or "png",
                        "source": {"bytes": image_bytes},
                    }
                }

            case "url":
                raise NotImplementedError(
                    "Bedrock Converse API does not support image URLs directly. "
                    "Please download and provide as bytes."
                )

            case _:
                raise NotImplementedError(
                    f"Unsupported source type: {block.media.source_type}"
                )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/tests/test_bedrock_async.py
================================================
from datapizza.clients.bedrock import BedrockClient


def test_async_client_initialization():
    """Test that async client can be initialized"""
    client = BedrockClient(
        aws_access_key_id="test_key",
        aws_secret_access_key="test_secret",
        region_name="us-east-1",
    )

    # Initialize async client
    client._set_a_client()

    # Verify async session is created
    assert hasattr(client, "a_session")
    assert hasattr(client, "a_config")
    assert hasattr(client, "a_region_name")
    assert client.a_region_name == "us-east-1"


def test_a_invoke_method_exists():
    """Test that _a_invoke method is implemented and doesn't raise NotImplementedError"""
    client = BedrockClient(
        aws_access_key_id="test_key",
        aws_secret_access_key="test_secret",
        region_name="us-east-1",
    )

    # Verify the method exists and is async
    assert hasattr(client, "_a_invoke")
    assert callable(client._a_invoke)


def test_a_stream_invoke_method_exists():
    """Test that _a_stream_invoke method is implemented and doesn't raise NotImplementedError"""
    client = BedrockClient(
        aws_access_key_id="test_key",
        aws_secret_access_key="test_secret",
        region_name="us-east-1",
    )

    # Verify the method exists and is async
    assert hasattr(client, "_a_stream_invoke")
    assert callable(client._a_stream_invoke)



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-bedrock/tests/test_bedrock_memory_adapter.py
================================================
from datapizza.memory.memory import Memory
from datapizza.type import ROLE, TextBlock

from datapizza.clients.bedrock import BedrockClient
from datapizza.clients.bedrock.memory_adapter import BedrockMemoryAdapter


def test_init_bedrock_client():
    """Test that BedrockClient can be initialized"""
    client = BedrockClient()
    assert client is not None
    assert client.model_name == "anthropic.claude-3-5-sonnet-20241022-v2:0"


def test_init_bedrock_client_with_credentials():
    """Test BedrockClient initialization with explicit credentials"""
    client = BedrockClient(
        aws_access_key_id="test_key",
        aws_secret_access_key="test_secret",
        region_name="us-west-2",
    )
    assert client is not None
    assert client.aws_access_key_id == "test_key"
    assert client.region_name == "us-west-2"


def test_bedrock_memory_adapter():
    """Test that the memory adapter converts memory to Bedrock message format"""
    memory_adapter = BedrockMemoryAdapter()
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)
    memory.add_turn(
        blocks=[TextBlock(content="Hello! How can I help you?")], role=ROLE.ASSISTANT
    )

    messages = memory_adapter.memory_to_messages(memory)

    assert messages == [
        {
            "role": "user",
            "content": [{"text": "Hello, world!"}],
        },
        {
            "role": "assistant",
            "content": [{"text": "Hello! How can I help you?"}],
        },
    ]


def test_bedrock_memory_adapter_multiple_blocks():
    """Test memory adapter with multiple text blocks in a single turn"""
    memory_adapter = BedrockMemoryAdapter()
    memory = Memory()
    memory.add_turn(
        blocks=[
            TextBlock(content="First message."),
            TextBlock(content="Second message."),
        ],
        role=ROLE.USER,
    )

    messages = memory_adapter.memory_to_messages(memory)

    assert messages == [
        {
            "role": "user",
            "content": [{"text": "First message."}, {"text": "Second message."}],
        },
    ]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-google/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-google/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-google"
version = "0.0.7"
description = "Google (Gemini) client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"

classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "google-genai>=1.3.0,<2.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-google/datapizza/clients/google/__init__.py
================================================
from .google_client import GoogleClient

__all__ = ["GoogleClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-google/datapizza/clients/google/google_client.py
================================================
import base64
from collections.abc import AsyncIterator, Iterator
from typing import Any, Literal

from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.type import (
    FunctionCallBlock,
    Media,
    MediaBlock,
    Model,
    StructuredBlock,
    TextBlock,
    ThoughtBlock,
)

from google import genai
from google.genai import types
from google.oauth2 import service_account

from .memory_adapter import GoogleMemoryAdapter


class GoogleClient(Client):
    """A client for interacting with Google's Generative AI APIs.

    This class provides methods for invoking the Google GenAI API to generate responses
    based on given input data. It extends the Client class.
    """

    def __init__(
        self,
        api_key: str | None = None,
        model: str = "gemini-2.0-flash",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
        project_id: str | None = None,
        location: str | None = None,
        credentials_path: str | None = None,
        use_vertexai: bool = False,
    ):
        """
        Args:
            api_key: The API key for the Google API.
            model: The model to use for the Google API.
            system_prompt: The system prompt to use for the Google API.
            temperature: The temperature to use for the Google API.
            cache: The cache to use for the Google API.
            project_id: The project ID for the Google API.
            location: The location for the Google API.
            credentials_path: The path to the credentials for the Google API.
            use_vertexai: Whether to use Vertex AI for the Google API.
        """
        if temperature and not 0 <= temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")

        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )
        self.memory_adapter = GoogleMemoryAdapter()

        try:
            if use_vertexai:
                if not credentials_path:
                    raise ValueError("credentials_path must be provided")
                if not project_id:
                    raise ValueError("project_id must be provided")
                if not location:
                    raise ValueError("location must be provided")

                credentials = service_account.Credentials.from_service_account_file(
                    credentials_path,
                    scopes=["https://www.googleapis.com/auth/cloud-platform"],
                )
                self.client = genai.Client(
                    vertexai=True,
                    project=project_id,
                    location=location,
                    credentials=credentials,
                )
            else:
                if not api_key:
                    raise ValueError("api_key must be provided")

                self.client = genai.Client(api_key=api_key)

        except Exception as e:
            raise RuntimeError(
                f"Failed to initialize Google GenAI client: {e!s}"
            ) from None

    def _sanitize_schema(self, schema: Any) -> Any:
        """Remove JSON Schema keys unsupported by Gemini's function schema."""
        if isinstance(schema, dict):
            sanitized = {}
            for key, value in schema.items():
                if key in [
                    "additionalProperties",
                    "additional_properties",
                    "minLength",
                    "maxLength",
                    "pattern",
                    "format",
                    "minItems",
                    "maxItems",
                    "uniqueItems",
                ]:
                    continue
                if key == "exclusiveMinimum":
                    # For integers: exclusiveMinimum: 0 â†’ minimum: 1 (next valid int)
                    # For floats: just use the value (slight relaxation, allows boundary)
                    if isinstance(value, int) or (
                        isinstance(value, float) and value.is_integer()
                    ):
                        sanitized["minimum"] = int(value) + 1
                    else:
                        sanitized["minimum"] = value
                    continue
                if key == "exclusiveMaximum":
                    # Same logic for maximum
                    if isinstance(value, int) or (
                        isinstance(value, float) and value.is_integer()
                    ):
                        sanitized["maximum"] = int(value) - 1
                    else:
                        sanitized["maximum"] = value
                    continue

                sanitized[key] = self._sanitize_schema(value)
            return sanitized
        if isinstance(schema, list):
            return [self._sanitize_schema(item) for item in schema]
        return schema

    def _convert_tool(self, tool: Tool) -> dict:
        """Convert tools to Google function format"""
        parameters_schema = self._sanitize_schema(tool.schema["parameters"])
        parameters = {
            "type": parameters_schema.get("type"),
            "properties": parameters_schema.get("properties", {}),
            "required": parameters_schema.get("required", []),
        }

        return {
            "name": tool.schema["name"],
            "description": tool.schema["description"],
            "parameters": parameters,
        }

    def _prepare_tools(self, tools: list[Tool] | None) -> list[types.Tool] | None:
        if not tools:
            return None

        google_tools = []
        function_declarations = []
        has_google_search = False

        for tool in tools:
            # Check if tool has google search capability
            if hasattr(tool, "name") and "google_search" in tool.name.lower():
                has_google_search = True
            elif isinstance(tool, Tool):
                function_declarations.append(self._convert_tool(tool))
            elif isinstance(tool, dict):
                google_tools.append(tool)
            else:
                raise ValueError(f"Unknown tool type: {type(tool)}")

        if function_declarations:
            google_tools.append(types.Tool(function_declarations=function_declarations))

        if has_google_search:
            google_tools.append(types.Tool(google_search=types.GoogleSearch()))

        return google_tools if google_tools else None

    def _token_usage_from_metadata(self, usage_metadata: Any | None) -> TokenUsage:
        if not usage_metadata:
            return TokenUsage()

        return TokenUsage(
            prompt_tokens=getattr(usage_metadata, "prompt_token_count", 0) or 0,
            completion_tokens=getattr(usage_metadata, "candidates_token_count", 0) or 0,
            cached_tokens=getattr(usage_metadata, "cached_content_token_count", 0) or 0,
        )

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> types.ToolConfig:
        adjusted_tool_choice: types.ToolConfig
        if isinstance(tool_choice, list):
            adjusted_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode="ANY",  # type: ignore
                    allowed_function_names=tool_choice,
                )
            )
        elif tool_choice == "required":
            adjusted_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(mode="ANY")  # type: ignore
            )
        elif tool_choice == "none":
            adjusted_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(mode="NONE")  # type: ignore
            )
        elif tool_choice == "auto":
            adjusted_tool_choice = types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(mode="AUTO")  # type: ignore
            )
        return adjusted_tool_choice

    def _invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        """Implementation of the abstract _invoke method"""
        if tools is None:
            tools = []
        contents = self._memory_to_contents(None, input, memory)

        tool_map = {tool.name: tool for tool in tools if isinstance(tool, Tool)}

        prepared_tools = self._prepare_tools(tools)
        config = types.GenerateContentConfig(
            temperature=temperature or self.temperature,
            system_instruction=system_prompt or self.system_prompt,
            max_output_tokens=max_tokens or None,
            tools=prepared_tools,  # type: ignore
            tool_config=self._convert_tool_choice(tool_choice)
            if tools and any(isinstance(tool, Tool) for tool in tools)
            else None,
            **kwargs,
        )

        response = self.client.models.generate_content(
            model=self.model_name,
            contents=contents,  # type: ignore
            config=config,  # type: ignore
        )
        return self._response_to_client_response(response, tool_map)

    async def _a_invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        """Implementation of the abstract _invoke method"""
        if tools is None:
            tools = []
        contents = self._memory_to_contents(None, input, memory)

        tool_map = {tool.name: tool for tool in tools if isinstance(tool, Tool)}

        prepared_tools = self._prepare_tools(tools)
        config = types.GenerateContentConfig(
            temperature=temperature or self.temperature,
            system_instruction=system_prompt or self.system_prompt,
            max_output_tokens=max_tokens or None,
            tools=prepared_tools,  # type: ignore
            tool_config=self._convert_tool_choice(tool_choice)
            if tools and any(isinstance(tool, Tool) for tool in tools)
            else None,
            **kwargs,
        )

        response = await self.client.aio.models.generate_content(
            model=self.model_name,
            contents=contents,  # type: ignore
            config=config,  # type: ignore
        )
        return self._response_to_client_response(response, tool_map)

    def _stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        """Implementation of the abstract _stream_invoke method"""
        if tools is None:
            tools = []
        contents = self._memory_to_contents(None, input, memory)

        prepared_tools = self._prepare_tools(tools)
        config = types.GenerateContentConfig(
            temperature=temperature or self.temperature,
            system_instruction=system_prompt or self.system_prompt,
            max_output_tokens=max_tokens or None,
            tools=prepared_tools,  # type: ignore
            tool_config=self._convert_tool_choice(tool_choice)
            if tools and any(isinstance(tool, Tool) for tool in tools)
            else None,
            **kwargs,
        )

        message_text = ""
        stop_reason = ""
        thought_block = ThoughtBlock(content="")

        usage = TokenUsage()

        for chunk in self.client.models.generate_content_stream(
            model=self.model_name,
            contents=contents,  # type: ignore
            config=config,
        ):
            usage_metadata = getattr(chunk, "usage_metadata", None)
            usage += self._token_usage_from_metadata(usage_metadata)
            if not chunk.candidates:
                raise ValueError("No candidates in response")

            finish_reason = chunk.candidates[0].finish_reason
            stop_reason = (
                finish_reason.value.lower()
                if finish_reason is not None
                else finish_reason
            )

            if not chunk.candidates[0].content:
                raise ValueError("No content in response")

            if not chunk.candidates[0].content.parts:
                yield ClientResponse(
                    content=[],
                    delta=chunk.text or "",
                    stop_reason=stop_reason,
                    usage=usage,
                )
                continue

            for part in chunk.candidates[0].content.parts:
                if not part.text:
                    continue
                elif hasattr(part, "thought") and part.thought:
                    thought_block.content += part.text
                else:  # If it's not a thought, it's a message
                    if part.text:
                        message_text += str(chunk.text or "")

                        yield ClientResponse(
                            content=[],
                            delta=chunk.text or "",
                            stop_reason=stop_reason,
                        )
        yield ClientResponse(
            content=[TextBlock(content=message_text)],
            stop_reason=stop_reason,
            usage=usage,
        )

    async def _a_stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        """Implementation of the abstract _a_stream_invoke method for Google"""
        if tools is None:
            tools = []
        contents = self._memory_to_contents(None, input, memory)

        prepared_tools = self._prepare_tools(tools)
        config = types.GenerateContentConfig(
            temperature=temperature or self.temperature,
            system_instruction=system_prompt or self.system_prompt,
            max_output_tokens=max_tokens or None,
            tools=prepared_tools,  # type: ignore
            tool_config=self._convert_tool_choice(tool_choice)
            if tools and any(isinstance(tool, Tool) for tool in tools)
            else None,
            **kwargs,
        )

        usage = TokenUsage()
        message_text = ""
        stop_reason = ""
        thought_block = ThoughtBlock(content="")
        async for chunk in await self.client.aio.models.generate_content_stream(
            model=self.model_name,
            contents=contents,  # type: ignore
            config=config,
        ):  # type: ignore
            usage_metadata = getattr(chunk, "usage_metadata", None)
            usage += self._token_usage_from_metadata(usage_metadata)

            if not chunk.candidates:
                raise ValueError("No candidates in response")

            finish_reason = chunk.candidates[0].finish_reason
            stop_reason = (
                finish_reason.value.lower()
                if finish_reason is not None
                else finish_reason
            )

            # Handle the case where the response has no parts
            candidate_content = chunk.candidates[0].content
            if not candidate_content or not candidate_content.parts:
                yield ClientResponse(
                    content=[],
                    delta=chunk.text or "",
                    stop_reason=stop_reason,
                )
                continue

            for part in candidate_content.parts:
                if not part.text:
                    continue
                elif hasattr(part, "thought") and part.thought:
                    thought_block.content += part.text
                else:  # If it's not a thought, it's a message
                    if part.text:
                        message_text += chunk.text or ""
                        yield ClientResponse(
                            content=[],
                            delta=chunk.text or "",
                            stop_reason=stop_reason,
                        )
        yield ClientResponse(
            content=[TextBlock(content=message_text)],
            stop_reason=stop_reason,
            usage=usage,
        )

    def _structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        """Implementation of the abstract _structured_response method"""
        contents = self._memory_to_contents(self.system_prompt, input, memory)

        prepared_tools = self._prepare_tools(tools)
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=contents,  # type: ignore
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=temperature,
                max_output_tokens=max_tokens,
                response_mime_type="application/json",
                tools=prepared_tools,  # type: ignore
                tool_config=self._convert_tool_choice(tool_choice)
                if tools and any(isinstance(tool, Tool) for tool in tools)
                else None,
                response_schema=(
                    output_cls.model_json_schema()
                    if hasattr(output_cls, "model_json_schema")
                    else output_cls
                ),
            ),
        )
        if not response or not response.candidates:
            raise ValueError("No response from Google GenAI")

        structured_data = output_cls.model_validate_json(str(response.text))
        usage_metadata = getattr(response, "usage_metadata", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=[StructuredBlock(content=structured_data)],
            stop_reason=response.candidates[0].finish_reason.value.lower()
            if response.candidates[0].finish_reason
            else None,
            usage=token_usage,
        )

    async def _a_structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        """Implementation of the abstract _structured_response method"""
        contents = self._memory_to_contents(self.system_prompt, input, memory)
        prepared_tools = self._prepare_tools(tools)
        response = await self.client.aio.models.generate_content(
            model=self.model_name,
            contents=contents,  # type: ignore
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=temperature,
                max_output_tokens=max_tokens,
                response_mime_type="application/json",
                tools=prepared_tools,  # type: ignore
                tool_config=self._convert_tool_choice(tool_choice)
                if tools and any(isinstance(tool, Tool) for tool in tools)
                else None,
                response_schema=(
                    output_cls.model_json_schema()
                    if hasattr(output_cls, "model_json_schema")
                    else output_cls
                ),
            ),
        )

        if not response or not response.candidates:
            raise ValueError("No response from Google GenAI")

        structured_data = output_cls.model_validate_json(str(response.text))
        usage_metadata = getattr(response, "usage_metadata", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=[StructuredBlock(content=structured_data)],
            stop_reason=response.candidates[0].finish_reason.value.lower()
            if response.candidates[0].finish_reason
            else None,
            usage=token_usage,
        )

    def _embed(
        self,
        text: str | list[str],
        model_name: str | None,
        task_type: str = "RETRIEVAL_DOCUMENT",
        output_dimensionality: int = 768,
        title: str | None = None,
        **kwargs,
    ) -> list[float] | list[list[float] | None]:
        """Embed a text using the model"""
        response = self.client.models.embed_content(
            model=model_name or self.model_name,
            contents=text,  # type: ignore
            config=types.EmbedContentConfig(
                task_type=task_type,
                output_dimensionality=output_dimensionality,
                title=title,
                **kwargs,
            ),
        )
        # Extract the embedding values from the response
        if not response.embeddings:
            return []

        embeddings = [embedding.values for embedding in response.embeddings]

        if isinstance(text, str) and embeddings[0]:
            return embeddings[0]

        return embeddings

    async def _a_embed(
        self,
        text: str | list[str],
        model_name: str | None,
        task_type: str = "RETRIEVAL_DOCUMENT",
        output_dimensionality: int = 768,
        title: str | None = None,
        **kwargs,
    ) -> list[float] | list[list[float] | None]:
        """Embed a text using the model"""
        response = await self.client.aio.models.embed_content(
            model=model_name or self.model_name,
            contents=text,  # type: ignore
            config=types.EmbedContentConfig(
                task_type=task_type,
                output_dimensionality=output_dimensionality,
                title=title,
                **kwargs,
            ),
        )
        # Extract the embedding values from the response
        if not response.embeddings:
            return []
        embeddings = [embedding.values for embedding in response.embeddings]

        if isinstance(text, str) and embeddings[0]:
            return embeddings[0]

        return embeddings

    def _response_to_client_response(
        self, response, tool_map: dict[str, Tool] | None = None
    ) -> ClientResponse:
        blocks = []
        # Handle function calls if present
        if hasattr(response, "function_calls") and response.function_calls:
            for fc in response.function_calls:
                if not tool_map:
                    raise ValueError("Tool map is required")

                tool = tool_map.get(fc.name, None)
                if not tool:
                    raise ValueError(f"Tool {fc.name} not found in tool map")

                blocks.append(
                    FunctionCallBlock(
                        name=fc.name,
                        arguments=fc.args,
                        id=f"fc_{id(fc)}",
                        tool=tool,
                    )
                )
        else:
            if (
                hasattr(response, "candidates")
                and response.candidates
                and response.candidates[0].content.parts
            ):
                for part in response.candidates[0].content.parts:
                    # Handle inline_data (images from generation or code execution)
                    if hasattr(part, "inline_data") and part.inline_data is not None:
                        media = Media(
                            media_type="image",
                            source_type="base64",
                            source=base64.b64encode(part.inline_data.data).decode(
                                "utf-8"
                            ),
                            extension=(part.inline_data.mime_type.split("/")[-1])
                            if part.inline_data.mime_type
                            else "png",
                        )
                        blocks.append(MediaBlock(media=media))
                    elif hasattr(part, "thought") and part.thought and part.text:
                        blocks.append(ThoughtBlock(content=part.text))
                    elif hasattr(part, "text") and part.text:
                        blocks.append(TextBlock(content=part.text))

        usage_metadata = getattr(response, "usage_metadata", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=blocks,
            stop_reason=(response.candidates[0].finish_reason.value.lower())
            if hasattr(response, "candidates") and response.candidates
            else None,
            usage=token_usage,
        )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-google/datapizza/clients/google/memory_adapter.py
================================================
import base64

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)

from google.genai import types


class GoogleMemoryAdapter(MemoryAdapter):
    def _turn_to_message(self, turn: Turn) -> dict:
        content = []
        for block in turn:
            block_dict = {}

            match block:
                case TextBlock():
                    block_dict = {"text": block.content}
                case FunctionCallBlock():
                    block_dict = {
                        "function_call": {"name": block.name, "args": block.arguments}
                    }
                case FunctionCallResultBlock():
                    block_dict = types.Part.from_function_response(
                        name=block.tool.name,
                        response={"result": block.result},
                    )
                case StructuredBlock():
                    block_dict = {"text": str(block.content)}
                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            block_dict = self._process_image_block(block)
                        case "pdf":
                            block_dict = self._process_pdf_block(block)

                        case "audio":
                            block_dict = self._process_audio_block(block)

                        case _:
                            raise NotImplementedError(
                                f"Unsupported media type: {block.media.media_type}"
                            )

            content.append(block_dict)

        return {
            "role": turn.role.google_role,
            "parts": (content),
        }

    def _process_audio_block(self, block: MediaBlock) -> types.Part:
        match block.media.source_type:
            case "raw":
                return types.Part.from_bytes(
                    data=block.media.source,
                    mime_type="audio/mp3",
                )

            case "path":
                with open(block.media.source, "rb") as f:
                    audio_bytes = f.read()

                return types.Part.from_bytes(
                    data=audio_bytes,
                    mime_type="audio/mp3",
                )

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type} for audio, source type supported: raw, path"
                )

    def _process_pdf_block(self, block: MediaBlock) -> types.Part | dict:
        match block.media.source_type:
            case "raw":
                return types.Part.from_bytes(
                    data=block.media.source,
                    mime_type="application/pdf",
                )
            case "base64":
                return {
                    "inline_data": {
                        "mime_type": "application/pdf",
                        "data": block.media.source,
                    }
                }
            case "path":
                with open(block.media.source, "rb") as f:
                    pdf_bytes = f.read()

                return {
                    "inline_data": {
                        "mime_type": "application/pdf",
                        "data": pdf_bytes,
                    }
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type} only supported: raw, base64, path"
                )

    def _process_image_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return types.Part.from_uri(
                    file_uri=block.media.source,
                    mime_type=f"image/{block.media.extension}",
                )  # type: ignore
            case "base64":
                return {
                    "inline_data": {
                        "mime_type": f"image/{block.media.extension}",
                        "data": block.media.source,
                    }
                }
            case "path":
                with open(block.media.source, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                return {
                    "inline_data": {
                        "mime_type": f"image/{block.media.extension}",
                        "data": base64_image,
                    }
                }
            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type} for image, only url, base64, path are supported"
                )

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        return {"role": role.google_role, "parts": [{"text": text}]}



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-google/tests/test_memory_adapter.py
================================================
import pytest
from datapizza.memory.memory import Memory
from datapizza.type import ROLE, StructuredBlock, TextBlock

from datapizza.clients.google.memory_adapter import GoogleMemoryAdapter


def test_google_memory_to_messages_structured_block():
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(StructuredBlock(content={"key": "value"}))
    messages = GoogleMemoryAdapter().memory_to_messages(memory)
    # Google adapter may serialize as string or dict in "parts"
    assert "key" in str(messages[0]["parts"][0])


def test_google_memory_to_messages_with_system_prompt():
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="Hello!"))
    system_prompt = "You are a helpful assistant."
    messages = GoogleMemoryAdapter().memory_to_messages(
        memory, system_prompt=system_prompt
    )
    assert messages[0]["role"] == "model"
    assert system_prompt in str(messages[0]["parts"])
    assert messages[1]["role"] == "user"


def test_google_memory_to_messages_with_input_str():
    memory = Memory()
    input_str = "What is the weather?"
    messages = GoogleMemoryAdapter().memory_to_messages(memory, input=input_str)
    assert messages[-1]["role"] == "user"
    assert input_str in str(messages[-1]["parts"])


def test_google_memory_to_messages_with_input_block():
    memory = Memory()
    input_block = TextBlock(content="This is a block input.")
    messages = GoogleMemoryAdapter().memory_to_messages(memory, input=input_block)
    assert messages[-1]["role"] == "user"
    assert "block input" in str(messages[-1]["parts"])


def test_google_memory_to_messages_unsupported_input():
    memory = Memory()

    class Dummy:
        pass

    with pytest.raises(ValueError):
        GoogleMemoryAdapter().memory_to_messages(memory, input=Dummy())



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-mistral/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-mistral/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-mistral"
version = "0.0.5"
description = "Mistral AI client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "mistralai>=1.2.0,<2.0.0",
    "requests>=2.25.0,<3.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-mistral/datapizza/clients/mistral/__init__.py
================================================
from datapizza.clients.mistral.mistral_client import MistralClient

__all__ = ["MistralClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-mistral/datapizza/clients/mistral/memory_adapter.py
================================================
import base64
import json

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)


class MistralMemoryAdapter(MemoryAdapter):
    def _turn_to_message(self, turn: Turn) -> dict:
        content = []
        tool_calls = []
        tool_call_id = None

        for block in turn:
            block_dict = {}

            match block:
                case TextBlock():
                    block_dict = {"type": "text", "text": block.content}
                case FunctionCallBlock():
                    tool_calls.append(
                        {
                            "id": block.id,
                            "function": {
                                "name": block.name,
                                "arguments": json.dumps(block.arguments),
                            },
                            "type": "function",
                        }
                    )
                case FunctionCallResultBlock():
                    tool_call_id = block.id
                    block_dict = {"type": "text", "text": block.result}
                case StructuredBlock():
                    block_dict = {"type": "text", "text": str(block.content)}
                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            block_dict = self._process_image_block(block)
                        # case "pdf":
                        #    block_dict = self._process_pdf_block(block)

                        case _:
                            raise NotImplementedError(
                                f"Unsupported media type: {block.media.media_type}, only image are supported"
                            )

            if block_dict:
                content.append(block_dict)

        messages: dict = {
            "role": turn.role.value,
        }

        if content:
            messages["content"] = content

        if tool_calls:
            messages["tool_calls"] = tool_calls

        if tool_call_id:
            messages["tool_call_id"] = tool_call_id

        return messages

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        return {"role": role.value, "content": text}

    def _process_image_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return {
                    "type": "image_url",
                    "image_url": {"url": block.media.source},
                }
            case "base64":
                return {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/{block.media.extension};base64,{block.media.source}"
                    },
                }
            case "path":
                with open(block.media.source, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                return {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/{block.media.extension};base64,{base64_image}"
                    },
                }
            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type}, only url, base64, path are supported"
                )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-mistral/datapizza/clients/mistral/mistral_client.py
================================================
import json
import logging
import os
from collections.abc import AsyncIterator, Iterator
from typing import Any, Literal

import requests
from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.type import (
    FunctionCallBlock,
    Media,
    MediaBlock,
    Model,
    StructuredBlock,
    TextBlock,
)
from mistralai import Mistral
from mistralai.models.ocrresponse import OCRResponse

from .memory_adapter import MistralMemoryAdapter

log = logging.getLogger(__name__)


class MistralClient(Client):
    """A client for interacting with the Mistral API.

    This class provides methods for invoking the Mistral API to generate responses
    based on given input data. It extends the Client class.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "mistral-large-latest",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
    ):
        """
        Args:
            api_key: The API key for the Mistral API.
            model: The model to use for the Mistral API.
            system_prompt: The system prompt to use for the Mistral API.
            temperature: The temperature to use for the Mistral API.
            cache: The cache to use for the Mistral API.
        """
        if temperature and not 0 <= temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")

        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )

        self.api_key = api_key
        self.memory_adapter = MistralMemoryAdapter()
        self._set_client()

    def _set_client(self):
        self.client = Mistral(api_key=self.api_key)

    def _token_usage_from_metadata(self, usage_metadata: Any | None) -> TokenUsage:
        if not usage_metadata:
            return TokenUsage()

        return TokenUsage(
            prompt_tokens=getattr(usage_metadata, "prompt_tokens", 0) or 0,
            completion_tokens=getattr(usage_metadata, "completion_tokens", 0) or 0,
            cached_tokens=getattr(usage_metadata, "cached_tokens", 0) or 0,
        )

    def _response_to_client_response(
        self, response, tool_map: dict[str, Tool] | None = None
    ) -> ClientResponse:
        blocks = []
        for choice in response.choices:
            if choice.message.content:
                blocks.append(TextBlock(content=choice.message.content))

            if choice.message.tool_calls:
                for tool_call in choice.message.tool_calls:
                    tool = tool_map.get(tool_call.function.name) if tool_map else None

                    if tool is None:
                        raise ValueError(f"Tool {tool_call.function.name} not found")

                    blocks.append(
                        FunctionCallBlock(
                            id=tool_call.id,
                            name=tool_call.function.name,
                            arguments=json.loads(tool_call.function.arguments),
                            tool=tool,
                        )
                    )

            # Handle media content if present
            if hasattr(choice.message, "media") and choice.message.media:
                for media_item in choice.message.media:
                    media = Media(
                        media_type=media_item.type,
                        source_type="url" if media_item.source_url else "base64",
                        source=media_item.source_url or media_item.data,
                        detail=getattr(media_item, "detail", "high"),
                    )
                    blocks.append(MediaBlock(media=media))

        log.debug(f"{self.__class__.__name__} response = {response}")
        usage_metadata = getattr(response, "usage", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=blocks,
            stop_reason=response.choices[0].finish_reason,
            usage=token_usage,
        )

    def _convert_tools(self, tools: Tool) -> dict:
        """Convert tools to Mistral function format"""
        return {"type": "function", "function": tools.schema}

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> dict | Literal["auto", "required", "none"]:
        if isinstance(tool_choice, list) and len(tool_choice) > 1:
            raise NotImplementedError(
                "multiple function names is not supported by Mistral"
            )
        elif isinstance(tool_choice, list):
            return {
                "type": "function",
                "function": {"name": tool_choice[0]},
            }
        else:
            return tool_choice

    def _invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        log.debug(f"{self.__class__.__name__} input = {input}")
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        request_params = {
            "model": self.model_name,
            "messages": messages,
            "stream": False,
            "max_tokens": max_tokens,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if tools:
            request_params["tools"] = [self._convert_tools(tool) for tool in tools]
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = self.client.chat.complete(**request_params)
        return self._response_to_client_response(response, tool_map)

    async def _a_invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        log.debug(f"{self.__class__.__name__} input = {input}")
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        request_params = {
            "model": self.model_name,
            "messages": messages,
            "stream": False,
            "max_tokens": max_tokens,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if tools:
            request_params["tools"] = [self._convert_tools(tool) for tool in tools]
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = await self.client.chat.complete_async(**request_params)
        return self._response_to_client_response(response, tool_map)

    def _stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)
        request_params = {
            "model": self.model_name,
            "messages": messages,
            "max_tokens": max_tokens,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if tools:
            request_params["tools"] = [self._convert_tools(tool) for tool in tools]
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = self.client.chat.stream(**request_params)
        text = ""
        usage = TokenUsage()
        stop_reason = None
        for chunk in response:
            token_usage = self._token_usage_from_metadata(
                getattr(chunk.data, "usage", None)
            )
            usage += token_usage
            stop_reason = chunk.data.choices[0].finish_reason
            delta_content = chunk.data.choices[0].delta.content
            delta = str(delta_content or "")
            text += delta
            yield ClientResponse(
                content=[],
                delta=delta,
                stop_reason=stop_reason,
            )

        yield ClientResponse(
            content=[TextBlock(content=text)],
            stop_reason=stop_reason,
            usage=usage,
        )

    async def _a_stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)
        request_params = {
            "model": self.model_name,
            "messages": messages,
            "max_tokens": max_tokens or 1024,
            **kwargs,
        }

        if temperature:
            request_params["temperature"] = temperature

        if tools:
            request_params["tools"] = [self._convert_tools(tool) for tool in tools]
            request_params["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = await self.client.chat.stream_async(**request_params)
        text = ""
        usage = TokenUsage()
        stop_reason = None
        async for chunk in response:
            token_usage = self._token_usage_from_metadata(
                getattr(chunk.data, "usage", None)
            )
            usage += token_usage
            stop_reason = chunk.data.choices[0].finish_reason
            delta_content = chunk.data.choices[0].delta.content
            delta = str(delta_content or "")
            text += delta
            yield ClientResponse(
                content=[],
                delta=delta,
                stop_reason=stop_reason,
            )

        yield ClientResponse(
            content=[TextBlock(content=text)],
            stop_reason=stop_reason,
            usage=usage,
        )

    def _structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        # Add system message to enforce JSON output
        messages = self._memory_to_contents(system_prompt, input, memory)

        if not tools:
            tools = []

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = self.client.chat.parse(
            model=self.model_name,
            messages=messages,
            response_format=output_cls,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs,
        )

        if not response.choices:
            raise ValueError("No response from Mistral")

        log.debug(f"{self.__class__.__name__} structured response: {response}")
        stop_reason = response.choices[0].finish_reason if response.choices else None
        if hasattr(output_cls, "model_validate_json"):
            structured_data = output_cls.model_validate_json(
                str(response.choices[0].message.content)  # type: ignore
            )
        else:
            structured_data = json.loads(str(response.choices[0].message.content))  # type: ignore
        usage_metadata = getattr(response, "usage", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=[StructuredBlock(content=structured_data)],
            stop_reason=stop_reason,
            usage=token_usage,
        )

    async def _a_structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        # Add system message to enforce JSON output
        messages = self._memory_to_contents(system_prompt, input, memory)

        if not tools:
            tools = []

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = await self.client.chat.parse_async(
            model=self.model_name,
            messages=messages,
            response_format=output_cls,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs,
        )

        if not response.choices:
            raise ValueError("No response from Mistral")

        log.debug(f"{self.__class__.__name__} structured response: {response}")
        stop_reason = response.choices[0].finish_reason if response.choices else None
        if hasattr(output_cls, "model_validate_json"):
            structured_data = output_cls.model_validate_json(
                str(response.choices[0].message.content)  # type: ignore
            )
        else:
            structured_data = json.loads(str(response.choices[0].message.content))  # type: ignore
        usage_metadata = getattr(response, "usage", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=[StructuredBlock(content=structured_data)],
            stop_reason=stop_reason,
            usage=token_usage,
        )

    def _embed(
        self, text: str | list[str], model_name: str | None, **kwargs
    ) -> list[float] | list[list[float]]:
        """Embed a text using the model"""
        response = self.client.embeddings.create(
            inputs=text, model=model_name or self.model_name, **kwargs
        )

        embeddings = [
            item.embedding for item in response.data if item.embedding is not None
        ]

        if not embeddings:
            return []

        if isinstance(text, str) and embeddings[0]:
            return embeddings[0]

        return embeddings

    async def _a_embed(
        self, text: str | list[str], model_name: str | None, **kwargs
    ) -> list[float] | list[list[float]]:
        """Embed a text using the model"""
        response = await self.client.embeddings.create_async(
            inputs=text, model=model_name or self.model_name, **kwargs
        )

        embeddings = [
            item.embedding for item in response.data if item.embedding is not None
        ]

        if not embeddings:
            return []

        if isinstance(text, str) and embeddings[0]:
            return embeddings[0]

        return embeddings

    def parse_document(
        self,
        document_path: str,
        autodelete: bool = True,
        include_image_base64: bool = True,
    ) -> OCRResponse:
        filename = os.path.basename(document_path)
        with open(document_path, "rb") as f:
            uploaded_pdf = self.client.files.upload(
                file={"file_name": filename, "content": f}, purpose="ocr"
            )

        signed_url = self.client.files.get_signed_url(file_id=uploaded_pdf.id)

        response = self.client.ocr.process(
            model="mistral-ocr-latest",
            document={
                "type": "document_url",
                "document_url": signed_url.url,
            },
            include_image_base64=include_image_base64,
        )

        if autodelete:
            url = f"https://api.mistral.ai/v1/files/{uploaded_pdf.id}"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}",
            }

            requests.delete(url, headers=headers, timeout=30)

        return response



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-mistral/tests/test_mistral_client.py
================================================
from datapizza.clients.mistral import MistralClient


def test_init_mistral_client():
    client = MistralClient(api_key="test")
    assert client is not None



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-openai"
version = "0.0.9"
description = "OpenAI client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "openai>=2,<3.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/datapizza/clients/openai/__init__.py
================================================
from .openai_client import OpenAIClient

__all__ = ["OpenAIClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/datapizza/clients/openai/memory_adapter.py
================================================
import base64
import json

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)

from openai.types.responses import ResponseFunctionToolCall


class OpenAIMemoryAdapter(MemoryAdapter):
    def _turn_to_message(self, turn: Turn) -> dict:
        content = []
        tool_calls = []
        tool_call_id = None
        response_function_tool_call = None

        for block in turn:
            block_dict = {}

            match block:
                case TextBlock():
                    block_dict = {
                        "type": "input_text"
                        if turn.role == ROLE.USER
                        else "output_text",
                        "text": block.content,
                    }
                case FunctionCallBlock():
                    return ResponseFunctionToolCall(
                        call_id=block.id,
                        name=block.name,
                        arguments=json.dumps(block.arguments),
                        type="function_call",
                        # id="fc_" + block.id,
                        status="completed",
                    )
                    # block_dict = {
                    #    "id": block.id,
                    #    "name": block.name,
                    #    "arguments": json.dumps(block.arguments),
                    #    "type": "function_call",
                    # }
                case FunctionCallResultBlock():
                    tool_call_id = block.id
                    return {
                        "type": "function_call_output",
                        "call_id": block.id,
                        "output": block.result,
                    }

                case StructuredBlock():
                    block_dict = {"type": "text", "text": str(block.content)}
                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            block_dict = self._process_image_block(block)
                        case "pdf":
                            block_dict = self._process_pdf_block(block)
                        case "audio":
                            block_dict = self._process_audio_block(block)

                        case _:
                            raise NotImplementedError(
                                f"Unsupported media type: {block.media.media_type}"
                            )

            if block_dict:
                content.append(block_dict)

        messages = {
            "role": turn.role.value,
            "content": (content),
        }

        if tool_calls:
            messages["tool_calls"] = tool_calls

        if tool_call_id:
            messages["tool_call_id"] = tool_call_id

        if response_function_tool_call:
            return response_function_tool_call

        return messages

    def _process_audio_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "path":
                with open(block.media.source, "rb") as f:
                    base64_audio = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_audio,
                        "format": block.media.extension,
                    },
                }

            case "base_64":
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": block.media.source,
                        "format": block.media.extension,
                    },
                }
            case "raw":
                base64_audio = base64.b64encode(block.media.source).decode("utf-8")
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_audio,
                        "format": block.media.extension,
                    },
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type} for audio, source type supported: raw, path"
                )

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        return {"role": role.value, "content": text}

    def _process_pdf_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "base64":
                return {
                    "type": "input_file",
                    "filename": "file.pdf",
                    "file_data": f"data:application/{block.media.extension};base64,{block.media.source}",
                }
            case "path":
                with open(block.media.source, "rb") as f:
                    base64_pdf = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "input_file",
                    "filename": "file.pdf",
                    "file_data": f"data:application/{block.media.extension};base64,{base64_pdf}",
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type}"
                )

    def _process_image_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return {
                    "type": "input_image",
                    "image_url": block.media.source,
                }

            case "base64":
                return {
                    "type": "input_image",
                    "image_url": f"data:image/{block.media.extension};base64,{block.media.source}",
                }

            case "path":
                with open(block.media.source, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                    return {
                        "type": "input_image",
                        "image_url": f"data:image/{block.media.extension};base64,{base64_image}",
                    }

            case _:
                raise ValueError(
                    f"Unsupported media source type: {block.media.source_type}"
                )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/datapizza/clients/openai/openai_client.py
================================================
import json
from collections.abc import AsyncIterator, Iterator
from typing import Literal

import httpx
from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.type import (
    FunctionCallBlock,
    Model,
    StructuredBlock,
    TextBlock,
    ThoughtBlock,
)

from openai import (
    AsyncOpenAI,
    AzureOpenAI,
    OpenAI,
)
from openai.types.responses import (
    ParsedResponseOutputMessage,
    ResponseCompletedEvent,
    ResponseFunctionToolCall,
    ResponseOutputMessage,
    ResponseReasoningItem,
    ResponseTextDeltaEvent,
)

from .memory_adapter import OpenAIMemoryAdapter


class OpenAIClient(Client):
    """A client for interacting with the OpenAI API.

    This class provides methods for invoking the OpenAI API to generate responses
    based on given input data. It extends the Client class.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4o-mini",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
        base_url: str | httpx.URL | None = None,
        organization: str | None = None,
        project: str | None = None,
        webhook_secret: str | None = None,
        websocket_base_url: str | httpx.URL | None = None,
        timeout: float | httpx.Timeout | None = None,
        max_retries: int = 2,
        default_headers: dict[str, str] | None = None,
        default_query: dict[str, object] | None = None,
        http_client: httpx.Client | None = None,
    ):
        """
        Args:
            api_key: The API key for the OpenAI API.
            model: The model to use for the OpenAI API.
            system_prompt: The system prompt to use for the OpenAI API.
            temperature: The temperature to use for the OpenAI API.
            cache: The cache to use for the OpenAI API.
            base_url: The base URL for the OpenAI API.
            organization: The organization ID for the OpenAI API.
            project: The project ID for the OpenAI API.
            webhook_secret: The webhook secret for the OpenAI API.
            websocket_base_url: The websocket base URL for the OpenAI API.
            timeout: The timeout for the OpenAI API.
            max_retries: The max retries for the OpenAI API.
            default_headers: The default headers for the OpenAI API.
            default_query: The default query for the OpenAI API.
            http_client: The http_client for the OpenAI API.
        """

        if temperature and not 0 <= temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")

        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )

        self.api_key = api_key
        self.base_url = base_url
        self.organization = organization
        self.project = project
        self.webhook_secret = webhook_secret
        self.websocket_base_url = websocket_base_url
        self.timeout = timeout
        self.max_retries = max_retries
        self.default_headers = default_headers
        self.default_query = default_query
        self.http_client = http_client

        self.memory_adapter = OpenAIMemoryAdapter()
        self._set_client()

    def _set_client(self):
        if not self.client:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                organization=self.organization,
                project=self.project,
                webhook_secret=self.webhook_secret,
                websocket_base_url=self.websocket_base_url,
                timeout=self.timeout,
                max_retries=self.max_retries,
                default_headers=self.default_headers,
                default_query=self.default_query,
                http_client=self.http_client,
            )

    def _set_a_client(self):
        if not self.a_client:
            self.a_client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                organization=self.organization,
                project=self.project,
                webhook_secret=self.webhook_secret,
                websocket_base_url=self.websocket_base_url,
                timeout=self.timeout,
                max_retries=self.max_retries,
                default_headers=self.default_headers,
                default_query=self.default_query,
            )

    def _response_to_client_response(
        self, response, tool_map: dict[str, Tool] | None
    ) -> ClientResponse:
        blocks = []

        # Handle new response format with direct content array
        if hasattr(response, "output_parsed"):
            blocks.append(StructuredBlock(content=response.output_parsed))

        if hasattr(response, "output") and response.output:
            for content_item in response.output:
                if isinstance(content_item, ResponseOutputMessage) and not isinstance(
                    content_item, ParsedResponseOutputMessage
                ):
                    for content in content_item.content:
                        if content.type == "output_text":
                            blocks.append(TextBlock(content=content.text))
                elif isinstance(content_item, ResponseReasoningItem):
                    if content_item.summary:
                        blocks.append(
                            ThoughtBlock(content=content_item.summary[0].text)
                        )

                elif isinstance(content_item, ResponseFunctionToolCall):
                    if not tool_map:
                        raise ValueError("Tool map is required")

                    tool = tool_map.get(content_item.name)
                    if not tool:
                        raise ValueError(f"Tool {content_item.name} not found")
                    blocks.append(
                        FunctionCallBlock(
                            id=content_item.call_id,
                            name=content_item.name,
                            arguments=json.loads(content_item.arguments)
                            if isinstance(content_item.arguments, str)
                            else content_item.arguments,
                            tool=tool,
                        )
                    )

        # Handle usage from new format
        usage = getattr(response, "usage", None)
        if usage:
            prompt_tokens = getattr(usage, "input_tokens", 0)
            completion_tokens = getattr(usage, "output_tokens", 0)
            cached_tokens = 0
            # Handle input_tokens_details for cached tokens
            if hasattr(usage, "input_tokens_details") and usage.input_tokens_details:
                cached_tokens = getattr(usage.input_tokens_details, "cached_tokens", 0)

        # Handle stop reason - use status from new format
        stop_reason = getattr(response, "status", None)
        if not stop_reason and hasattr(response, "choices") and response.choices:
            stop_reason = response.choices[0].finish_reason

        return ClientResponse(
            content=blocks,
            stop_reason=stop_reason,
            usage=TokenUsage(
                prompt_tokens=prompt_tokens or 0,
                completion_tokens=completion_tokens or 0,
                cached_tokens=cached_tokens or 0,
            ),
        )

    def _convert_tools(self, tool: Tool) -> dict:
        """Convert tools to OpenAI function format"""
        return {
            "type": "function",
            "name": tool.name,
            "description": tool.description,
            "parameters": {
                "type": "object",
                "properties": tool.properties,
                "required": tool.required,
            },
        }

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> dict | Literal["auto", "required", "none"]:
        if isinstance(tool_choice, list) and len(tool_choice) > 1:
            raise NotImplementedError(
                "multiple function names is not supported by OpenAI"
            )
        elif isinstance(tool_choice, list):
            return {
                "type": "function",
                "name": tool_choice[0],
            }
        else:
            return tool_choice

    def _invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        kwargs = {
            **kwargs,
            "model": self.model_name,
            "input": messages,
            "stream": False,
            "max_output_tokens": max_tokens,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        client: OpenAI = self._get_client()
        response = client.responses.create(**kwargs)
        return self._response_to_client_response(response, tool_map)

    async def _a_invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        kwargs = {
            **kwargs,
            "model": self.model_name,
            "input": messages,
            "stream": False,
            "max_output_tokens": max_tokens,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        a_client = self._get_a_client()
        response = await a_client.responses.create(**kwargs)
        return self._response_to_client_response(response, tool_map)

    def _stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        kwargs = {
            **kwargs,
            "model": self.model_name,
            "input": messages,
            "stream": True,
            "max_output_tokens": max_tokens,
            # "stream_options": {"include_usage": True},
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = self.client.responses.create(**kwargs)
        for chunk in response:
            if isinstance(chunk, ResponseTextDeltaEvent):
                yield ClientResponse(
                    content=[],
                    delta=chunk.delta,
                    stop_reason=None,
                    usage=TokenUsage(
                        prompt_tokens=0,
                        completion_tokens=0,
                        cached_tokens=0,
                    ),
                )

            if isinstance(chunk, ResponseCompletedEvent):
                yield self._response_to_client_response(chunk.response, tool_map)

    async def _a_stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}
        kwargs = {
            **kwargs,
            "model": self.model_name,
            "input": messages,
            "stream": True,
            "max_output_tokens": max_tokens,
            # "stream_options": {"include_usage": True},
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        a_client = self._get_a_client()

        async for chunk in await a_client.responses.create(**kwargs):
            if isinstance(chunk, ResponseTextDeltaEvent):
                yield ClientResponse(
                    content=[],
                    delta=chunk.delta,
                    stop_reason=None,
                    usage=TokenUsage(
                        prompt_tokens=0,
                        completion_tokens=0,
                        cached_tokens=0,
                    ),
                )

            if isinstance(chunk, ResponseCompletedEvent):
                yield self._response_to_client_response(chunk.response, tool_map)

    def _structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        # Add system message to enforce JSON output

        if tools is None:
            tools = []

        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}
        kwargs = {
            "model": self.model_name,
            "input": messages,
            "text_format": output_cls,
            "max_output_tokens": max_tokens,
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)
            # Structured response needs strict mode and no additional properties
            for tool in kwargs["tools"]:
                tool["strict"] = True
                tool["parameters"]["additionalProperties"] = False

        response = self.client.responses.parse(**kwargs)

        return self._response_to_client_response(response, tool_map)

    async def _a_structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float,
        max_tokens: int,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ):
        if tools is None:
            tools = []

        messages = self._memory_to_contents(system_prompt, input, memory)
        tool_map = {tool.name: tool for tool in tools}

        kwargs = {
            "model": self.model_name,
            "input": messages,
            "text_format": output_cls,
            "max_output_tokens": max_tokens,
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)
            # Structured response needs strict mode and no additional properties
            for tool in kwargs["tools"]:
                tool["strict"] = True
                tool["parameters"]["additionalProperties"] = False

        a_client = self._get_a_client()
        response = await a_client.responses.parse(**kwargs)

        return self._response_to_client_response(response, tool_map)

    def _embed(
        self, text: str | list[str], model_name: str | None, **kwargs
    ) -> list[float] | list[list[float]]:
        """Embed a text using the model"""
        response = self.client.embeddings.create(
            input=text, model=model_name or self.model_name, **kwargs
        )

        embeddings = [item.embedding for item in response.data]

        if isinstance(text, str):
            return embeddings[0] if embeddings else []

        return embeddings or []

    async def _a_embed(
        self, text: str | list[str], model_name: str | None, **kwargs
    ) -> list[float] | list[list[float]]:
        """Embed a text using the model"""

        a_client = self._get_a_client()
        response = await a_client.embeddings.create(
            input=text, model=model_name or self.model_name, **kwargs
        )

        embeddings = [item.embedding for item in response.data]

        if isinstance(text, str):
            return embeddings[0] if embeddings else []

        return embeddings or []

    def _is_azure_client(self) -> bool:
        return isinstance(self._get_client(), AzureOpenAI)



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/tests/__init__.py
================================================
[Empty file]


================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/tests/test_base_client.py
================================================
from unittest.mock import MagicMock, patch

import httpx
from datapizza.core.clients import ClientResponse

from datapizza.clients.openai import (
    OpenAIClient,
)


def test_client_init():
    client = OpenAIClient(
        model="gpt-4o-mini",
        api_key="test_api_key",
    )
    assert client is not None


@patch("datapizza.clients.openai.openai_client.OpenAI")
def test_client_init_with_extra_args(mock_openai):
    """Tests that extra arguments are passed to the OpenAI client."""
    OpenAIClient(
        api_key="test_api_key",
        organization="test-org",
        project="test-project",
        timeout=30.0,
        max_retries=3,
    )
    mock_openai.assert_called_once_with(
        api_key="test_api_key",
        base_url=None,
        organization="test-org",
        project="test-project",
        webhook_secret=None,
        websocket_base_url=None,
        timeout=30.0,
        max_retries=3,
        default_headers=None,
        default_query=None,
        http_client=None,
    )


@patch("datapizza.clients.openai.openai_client.OpenAI")
def test_client_init_with_http_client(mock_openai):
    """Tests that a custom http_client is passed to the OpenAI client."""
    custom_http_client = httpx.Client()
    OpenAIClient(
        api_key="test_api_key",
        http_client=custom_http_client,
    )
    mock_openai.assert_called_once_with(
        api_key="test_api_key",
        base_url=None,
        organization=None,
        project=None,
        webhook_secret=None,
        websocket_base_url=None,
        timeout=None,
        max_retries=2,
        default_headers=None,
        default_query=None,
        http_client=custom_http_client,
    )


@patch("datapizza.clients.openai.openai_client.OpenAI")
def test_invoke_kwargs_override(mock_openai_class):
    """
    Tests that kwargs like 'stream' are not overridden by user input
    in non-streaming methods, but other kwargs are passed through.
    """
    mock_openai_instance = mock_openai_class.return_value
    mock_openai_instance.responses.create.return_value = MagicMock()

    client = OpenAIClient(api_key="test")
    client._response_to_client_response = MagicMock(
        return_value=ClientResponse(content=[])
    )

    client.invoke("hello", stream=True, top_p=0.5)

    mock_openai_instance.responses.create.assert_called_once()
    called_kwargs = mock_openai_instance.responses.create.call_args.kwargs

    assert called_kwargs.get("top_p") == 0.5
    assert called_kwargs.get("stream") is False


@patch("datapizza.clients.openai.openai_client.OpenAI")
def test_stream_invoke_kwargs_override(mock_openai_class):
    """
    Tests that kwargs like 'stream' are not overridden by user input
    in streaming methods.
    """
    mock_openai_instance = mock_openai_class.return_value
    mock_openai_instance.responses.create.return_value = []

    client = OpenAIClient(api_key="test")

    list(client.stream_invoke("hello", stream=False, top_p=0.5))

    mock_openai_instance.responses.create.assert_called_once()
    called_kwargs = mock_openai_instance.responses.create.call_args.kwargs

    assert called_kwargs.get("top_p") == 0.5
    assert called_kwargs.get("stream") is True



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai/tests/test_memory_adapter.py
================================================
import json

import pytest
from datapizza.memory.memory import Memory
from datapizza.tools.tools import tool
from datapizza.type.type import (
    ROLE,
    FunctionCallBlock,
    Media,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)
from openai.types.responses import ResponseFunctionToolCall

from datapizza.clients.openai.memory_adapter import OpenAIMemoryAdapter


@pytest.fixture(
    params=[
        OpenAIMemoryAdapter(),
    ]
)
def adapter(request):
    """Parameterized fixture that provides different memory adapter implementations.

    Each test using this fixture will run once for each adapter in the params list.
    """
    return request.param


@pytest.fixture
def memory():
    return Memory()


def test_empty_memory_to_messages(adapter, memory):
    """Test that an empty memory converts to an empty list of messages."""
    messages = adapter.memory_to_messages(memory)
    assert messages == []


def test_turn_with_some_text():
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="Hello!"))
    memory.add_to_last_turn(TextBlock(content="Hi, how are u?"))
    messages = OpenAIMemoryAdapter().memory_to_messages(memory)
    assert messages == [
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": "Hello!"},
                {"type": "input_text", "text": "Hi, how are u?"},
            ],
        }
    ]


def test_memory_to_messages_multiple_turns():
    """Test conversion of a memory with multiple turns to messages."""
    # First turn: user asks a question
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="What's 2+2?"))

    # Second turn: assistant responds
    memory.new_turn(role=ROLE.ASSISTANT)
    memory.add_to_last_turn(TextBlock(content="The answer is 4."))

    # Third turn: user follows up
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="Thanks!"))

    messages = OpenAIMemoryAdapter().memory_to_messages(memory)

    expected = [
        {"role": "user", "content": [{"type": "input_text", "text": "What's 2+2?"}]},
        {
            "role": "assistant",
            "content": [{"type": "output_text", "text": "The answer is 4."}],
        },
        {"role": "user", "content": [{"type": "input_text", "text": "Thanks!"}]},
    ]
    assert messages == expected


def test_memory_to_messages_function_call():
    @tool
    def add(a: int, b: int) -> int:
        return a + b

    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="Call the add function."))
    memory.new_turn(role=ROLE.ASSISTANT)
    memory.add_to_last_turn(
        FunctionCallBlock(id="call_1", name="add", arguments={"a": 2, "b": 2}, tool=add)
    )

    messages = OpenAIMemoryAdapter().memory_to_messages(memory)
    assert messages[0]["role"] == "user"
    assert isinstance(messages[1], ResponseFunctionToolCall)
    assert json.loads(messages[1].arguments) == {
        "a": 2,
        "b": 2,
    }


def test_memory_to_messages_media_blocks():
    image = Media(
        media_type="image",
        source_type="url",
        source="http://example.com/image.png",
        extension="png",
    )
    pdf = Media(
        media_type="pdf",
        source_type="base64",
        source="THIS_IS_A_PDF_BASE64",
        extension="pdf",
    )
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(MediaBlock(media=image))
    memory.add_to_last_turn(MediaBlock(media=pdf))
    messages = OpenAIMemoryAdapter().memory_to_messages(memory)
    assert messages[0]["role"] == "user"
    # Should contain both image and pdf blocks

    # TODO: Check if the image and pdf blocks are correct
    assert messages[0]["content"][1] == {
        "type": "input_file",
        "filename": "file.pdf",
        "file_data": "data:application/pdf;base64,THIS_IS_A_PDF_BASE64",
    }


def test_memory_to_messages_structured_block():
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(StructuredBlock(content={"key": "value"}))
    messages = OpenAIMemoryAdapter().memory_to_messages(memory)
    assert messages[0]["content"] == "{'key': 'value'}" or messages[0]["content"] == [
        {
            "type": "text",
            "text": "{'key': 'value'}",
        }
    ]


def test_memory_to_messages_with_system_prompt():
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="Hello!"))
    system_prompt = "You are a helpful assistant."
    messages = OpenAIMemoryAdapter().memory_to_messages(
        memory, system_prompt=system_prompt
    )
    assert messages[0]["role"] == "system"
    assert messages[0]["content"] == system_prompt
    assert messages[1]["role"] == "user"


def test_memory_to_messages_with_input_str():
    memory = Memory()
    input_str = "What is the weather?"
    messages = OpenAIMemoryAdapter().memory_to_messages(memory, input=input_str)
    assert messages[-1]["role"] == "user"
    assert messages[-1]["content"] == input_str


def test_memory_to_messages_with_input_block():
    memory = Memory()
    input_block = TextBlock(content="This is a block input.")
    messages = OpenAIMemoryAdapter().memory_to_messages(memory, input=input_block)
    assert messages[-1]["role"] == "user"
    assert "block input" in str(messages[-1]["content"])


def test_google_empty_memory_to_messages():
    messages = OpenAIMemoryAdapter().memory_to_messages(Memory())
    assert messages == []


def test_google_memory_to_messages_multiple_turns():
    memory = Memory()
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="What's 2+2?"))
    memory.new_turn(role=ROLE.ASSISTANT)
    memory.add_to_last_turn(TextBlock(content="The answer is 4."))
    memory.new_turn(role=ROLE.USER)
    memory.add_to_last_turn(TextBlock(content="Thanks!"))
    messages = OpenAIMemoryAdapter().memory_to_messages(memory)
    assert messages[0]["role"] == "user"
    assert messages[1]["role"] == "assistant"
    assert messages[2]["role"] == "user"



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai-like/README.md
================================================
# DataPizza AI - OpenAI-Like Client

A versatile client for DataPizza AI that supports OpenAI-compatible APIs, including local models through Ollama, Together AI, and other OpenAI-compatible services.

## Installation

```bash
pip install datapizza-ai-clients-openai-like
```

## Quick Start

### With Ollama (Local Models)

```python
from datapizza.clients.openai_like import OpenAILikeClient

# Create client for Ollama
client = OpenAILikeClient(
    api_key="",  # Ollama doesn't require an API key
    model="gemma2:2b",
    system_prompt="You are a helpful assistant.",
    base_url="http://localhost:11434/v1",
)

response = client.invoke("What is the capital of France?")
print(response.content)
```

### With Together AI

```python
import os
from datapizza.clients.openai_like import OpenAILikeClient

client = OpenAILikeClient(
    api_key=os.getenv("TOGETHER_API_KEY"),
    model="meta-llama/Llama-2-7b-chat-hf",
    system_prompt="You are a helpful assistant.",
    base_url="https://api.together.xyz/v1",
)

response = client.invoke("Explain quantum computing")
print(response.content)
```

### With OpenRouter

```python
import os
from datapizza.clients.openai_like import OpenAILikeClient

client = OpenAILikeClient(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    model="google/gemma-7b-it",
    system_prompt="You are a helpful assistant.",
    base_url="https://openrouter.ai/api/v1",
)

response = client.invoke("What is OpenRouter?")
print(response.content)
```

### With Other OpenAI-Compatible Services

```python
import os
from datapizza.clients.openai_like import OpenAILikeClient

client = OpenAILikeClient(
    api_key=os.getenv("YOUR_API_KEY"),
    model="your-model-name",
    system_prompt="You are a helpful assistant.",
    base_url="https://your-service-url/v1",
)

response = client.invoke("Your question here")
print(response.content)
```

## Features

- **OpenAI-Compatible**: Works with any service that implements the OpenAI API standard
- **Local Models**: Perfect for running with Ollama for privacy and cost control
- **Memory Support**: Built-in memory adapter for conversation history
- **Streaming**: Support for real-time streaming responses
- **Structured Outputs**: Generate structured data with Pydantic models
- **Tool Calling**: Function calling capabilities where supported

## Supported Services

- **Ollama** - Local model inference
- **Together AI** - Cloud-based model hosting
- **OpenRouter** - Access a variety of models through a single API
- **Perplexity AI** - Search-augmented models
- **Groq** - Fast inference API
- **Any OpenAI-compatible API**

## Advanced Usage

### With Memory

```python
from datapizza.clients.openai_like import OpenAILikeClient
from datapizza.memory import Memory

client = OpenAILikeClient(
    api_key="",
    model="llama3.1:8b",
    base_url="http://localhost:11434/v1",
)

memory = Memory(client=client)
memory.add("I'm working on a Python project about machine learning.")
response = memory.query("What libraries should I use?")
```

### Streaming Responses

```python
client = OpenAILikeClient(
    api_key="",
    model="gemma2:7b",
    base_url="http://localhost:11434/v1",
)

for chunk in client.stream("Tell me a story about AI"):
    print(chunk.content, end="", flush=True)
```

### Structured Outputs

```python
from pydantic import BaseModel
from datapizza.clients.openai_like import OpenAILikeClient

class Person(BaseModel):
    name: str
    age: int
    occupation: str

client = OpenAILikeClient(
    api_key="",
    model="llama3.1:8b",
    base_url="http://localhost:11434/v1",
)

response = client.invoke(
    "Generate a person profile",
    response_format=Person
)
print(response.parsed)  # Person object
```

## Configuration Options

| Parameter | Description | Default |
|-----------|-------------|---------|
| `api_key` | API key for the service | Required (empty string for Ollama) |
| `model` | Model name to use | Required |
| `base_url` | Base URL for the API | Required |
| `system_prompt` | System message for the model | None |
| `temperature` | Sampling temperature (0-2) | 0.7 |
| `max_tokens` | Maximum tokens in response | None |
| `timeout` | Request timeout in seconds | 30 |

## Ollama Setup

1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Pull a model: `ollama pull gemma2:2b`
3. Start Ollama: `ollama serve`
4. Use with DataPizza AI as shown in the examples above

## Popular Ollama Models

- `gemma2:2b` - Lightweight, fast responses
- `gemma2:7b` - Balanced performance
- `llama3.1:8b` - High quality, more resource intensive
- `codellama:7b` - Specialized for coding tasks
- `mistral:7b` - Good general purpose model

## Error Handling

```python
from datapizza.clients.openai_like import OpenAILikeClient
from datapizza.core.clients.exceptions import ClientError

try:
    client = OpenAILikeClient(
        api_key="",
        model="nonexistent-model",
        base_url="http://localhost:11434/v1",
    )
    response = client.invoke("Hello")
except ClientError as e:
    print(f"Client error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

## Contributing

Contributions are welcome! Please see our [Contributing Guide](../../CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai-like/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-openai-like"
version = "0.0.8"
description = "OpenAI client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "httpx>=0.28.1",
    "openai>=2.0.0,<3.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai-like/datapizza/clients/openai_like/__init__.py
================================================
from .openai_completion_client import OpenAILikeClient

__all__ = ["OpenAILikeClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai-like/datapizza/clients/openai_like/memory_adapter.py
================================================
import base64
import json

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)


class OpenAILikeMemoryAdapter(MemoryAdapter):
    def _turn_to_message(self, turn: Turn) -> dict:
        content = []
        tool_calls = []
        tool_call_id = None

        for block in turn:
            block_dict = {}

            match block:
                case TextBlock():
                    block_dict = {"type": "text", "text": block.content}
                case FunctionCallBlock():
                    tool_calls.append(
                        {
                            "id": block.id,
                            "function": {
                                "name": block.name,
                                "arguments": json.dumps(block.arguments),
                            },
                            "type": "function",
                        }
                    )
                case FunctionCallResultBlock():
                    tool_call_id = block.id
                    block_dict = {"type": "text", "text": block.result}

                case StructuredBlock():
                    block_dict = {"type": "text", "text": str(block.content)}
                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            block_dict = self._process_image_block(block)
                        case "pdf":
                            block_dict = self._process_pdf_block(block)
                        case "audio":
                            block_dict = self._process_audio_block(block)

                        case _:
                            raise NotImplementedError(
                                f"Unsupported media type: {block.media.media_type}"
                            )

            if block_dict:
                content.append(block_dict)

        messages = {
            "role": turn.role.value,
            "content": (content),
        }

        if tool_calls:
            messages["tool_calls"] = tool_calls

        if tool_call_id:
            messages["tool_call_id"] = tool_call_id

        return messages

    def _process_audio_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "path":
                with open(block.media.source, "rb") as f:
                    base64_audio = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_audio,
                        "format": block.media.extension,
                    },
                }

            case "base_64":
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": block.media.source,
                        "format": block.media.extension,
                    },
                }
            case "raw":
                base64_audio = base64.b64encode(block.media.source).decode("utf-8")
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_audio,
                        "format": block.media.extension,
                    },
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type} for audio, source type supported: raw, path"
                )

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        return {"role": role.value, "content": text}

    def _process_pdf_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "base64":
                return {
                    "type": "file",
                    "file": {
                        "filename": "file.pdf",
                        "file_data": f"data:application/{block.media.extension};base64,{block.media.source}",
                    },
                }
            case "path":
                with open(block.media.source, "rb") as f:
                    base64_pdf = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "file",
                    "file": {
                        "filename": "file.pdf",
                        "file_data": f"data:application/{block.media.extension};base64,{base64_pdf}",
                    },
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type}"
                )

    def _process_image_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return {
                    "type": "image_url",
                    "image_url": {"url": block.media.source},
                }

            case "base64":
                return {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/{block.media.extension};base64,{block.media.source}"
                    },
                }

            case "path":
                with open(block.media.source, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                    return {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/{block.media.extension};base64,{base64_image}"
                        },
                    }

            case _:
                raise ValueError(
                    f"Unsupported media source type: {block.media.source_type}"
                )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai-like/datapizza/clients/openai_like/openai_completion_client.py
================================================
import json
from collections.abc import AsyncIterator, Iterator
from typing import Literal

import httpx
from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools.tools import Tool
from datapizza.type import (
    FunctionCallBlock,
    Media,
    MediaBlock,
    Model,
    StructuredBlock,
    TextBlock,
)
from openai import AsyncOpenAI, OpenAI

from .memory_adapter import OpenAILikeMemoryAdapter


class OpenAILikeClient(Client):
    """A client for interacting with the OpenAI API.

    This class provides methods for invoking the OpenAI API to generate responses
    based on given input data. It extends the Client class.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4o-mini",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
        base_url: str | httpx.URL | None = None,
    ):
        if temperature and not 0 <= temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")

        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )

        self.base_url = base_url
        self.api_key = api_key
        self.memory_adapter = OpenAILikeMemoryAdapter()
        self._set_client()

    def _set_client(self):
        if not self.client:
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def _set_a_client(self):
        if not self.a_client:
            self.a_client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

    def _token_usage_from_metadata(self, usage_metadata) -> TokenUsage:
        if not usage_metadata:
            return TokenUsage()

        return TokenUsage(
            prompt_tokens=getattr(usage_metadata, "prompt_tokens", 0) or 0,
            completion_tokens=getattr(usage_metadata, "completion_tokens", 0) or 0,
            cached_tokens=getattr(
                getattr(usage_metadata, "prompt_tokens_details", None),
                "cached_tokens",
                0,
            )
            or 0,
        )

    def _response_to_client_response(
        self, response, tool_map: dict[str, Tool] | None
    ) -> ClientResponse:
        blocks = []
        for choice in response.choices:
            if choice.message.content:
                blocks.append(TextBlock(content=choice.message.content))

            if choice.message.tool_calls and tool_map:
                for tool_call in choice.message.tool_calls:
                    tool = tool_map.get(tool_call.function.name)

                    if not tool:
                        raise ValueError(f"Tool {tool_call.function.name} not found")

                    blocks.append(
                        FunctionCallBlock(
                            id=tool_call.id,
                            name=tool_call.function.name,
                            arguments=json.loads(tool_call.function.arguments),
                            tool=tool,
                        )
                    )

            # Handle media content if present
            if hasattr(choice.message, "media") and choice.message.media:
                for media_item in choice.message.media:
                    media = Media(
                        media_type=media_item.type,
                        source_type="url" if media_item.source_url else "base64",
                        source=media_item.source_url or media_item.data,
                        detail=getattr(media_item, "detail", "high"),
                    )
                    blocks.append(MediaBlock(media=media))

        usage_metadata = getattr(response, "usage", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=blocks,
            stop_reason=response.choices[0].finish_reason,
            usage=token_usage,
        )

    def _convert_tools(self, tools: Tool) -> dict:
        """Convert tools to OpenAI function format"""
        return {"type": "function", "function": tools.schema}

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> dict | Literal["auto", "required", "none"]:
        if isinstance(tool_choice, list) and len(tool_choice) > 1:
            raise NotImplementedError(
                "multiple function names is not supported by OpenAI"
            )
        elif isinstance(tool_choice, list):
            return {
                "type": "function",
                "function": {"name": tool_choice[0]},
            }
        else:
            return tool_choice

    def _invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "stream": False,
            "max_completion_tokens": max_tokens,
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        client: OpenAI = self._get_client()
        response = client.chat.completions.create(**kwargs)
        return self._response_to_client_response(response, tool_map)

    async def _a_invoke(
        self,
        *,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> ClientResponse:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)

        tool_map = {tool.name: tool for tool in tools}

        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "stream": False,
            "max_completion_tokens": max_tokens,
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        a_client = self._get_a_client()
        response = await a_client.chat.completions.create(**kwargs)
        return self._response_to_client_response(response, tool_map)

    def _stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None,
        memory: Memory | None,
        tool_choice: Literal["auto", "required", "none"] | list[str],
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)
        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "stream": True,
            "max_completion_tokens": max_tokens,
            "stream_options": {"include_usage": True},
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        response = self.client.chat.completions.create(**kwargs)
        message_content = ""
        usage = TokenUsage()
        finish_reason = None

        for chunk in response:
            usage_metadata = getattr(chunk, "usage", None)
            token_usage = self._token_usage_from_metadata(usage_metadata)
            usage += token_usage

            if len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                finish_reason = chunk.choices[0].finish_reason

            delta_content = delta.content if delta and delta.content else ""
            message_content = message_content + delta_content
            yield ClientResponse(
                content=[TextBlock(content=message_content)],
                delta=delta_content,
                stop_reason=finish_reason or None,
            )
        yield ClientResponse(
            content=[TextBlock(content=message_content)],
            stop_reason=finish_reason or None,
            usage=usage,
        )

    async def _a_stream_invoke(
        self,
        input: str,
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        if tools is None:
            tools = []
        messages = self._memory_to_contents(system_prompt, input, memory)
        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "stream": True,
            "max_completion_tokens": max_tokens,
            "stream_options": {"include_usage": True},
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)

        a_client = self._get_a_client()
        message_content = ""
        usage = TokenUsage()
        finish_reason = None

        async for chunk in await a_client.chat.completions.create(**kwargs):
            usage_metadata = getattr(chunk, "usage", None)
            token_usage = self._token_usage_from_metadata(usage_metadata)
            usage += token_usage

            if len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                finish_reason = chunk.choices[0].finish_reason

            delta_content = delta.content if delta and delta.content else ""
            message_content = message_content + delta_content

            yield ClientResponse(
                content=[TextBlock(content=message_content)],
                delta=delta_content,
                stop_reason=finish_reason or None,
            )
        yield ClientResponse(
            content=[TextBlock(content=message_content)],
            stop_reason=finish_reason or None,
            usage=usage,
        )

    def _structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float | None,
        max_tokens: int,
        system_prompt: str | None,
        tools: list[Tool] | None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        # Add system message to enforce JSON output
        messages = self._memory_to_contents(system_prompt, input, memory)

        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "response_format": output_cls,
            "max_completion_tokens": max_tokens,
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)
            # Structured response needs strict mode and no additional properties
            for tool in kwargs["tools"]:
                tool["function"]["strict"] = True
                tool["function"]["parameters"]["additionalProperties"] = False

        response = self.client.beta.chat.completions.parse(**kwargs)

        stop_reason = response.choices[0].finish_reason

        if not response.choices[0].message.content:
            raise ValueError("No content in response")

        if hasattr(output_cls, "model_validate_json"):
            structured_data = output_cls.model_validate_json(
                response.choices[0].message.content
            )
        else:
            structured_data = json.loads(response.choices[0].message.content)
        usage_metadata = getattr(response, "usage", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=[StructuredBlock(content=structured_data)],
            stop_reason=stop_reason,
            usage=token_usage,
        )

    async def _a_structured_response(
        self,
        input: str,
        output_cls: type[Model],
        memory: Memory | None,
        temperature: float,
        max_tokens: int,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ):
        messages = self._memory_to_contents(system_prompt, input, memory)

        kwargs = {
            "model": self.model_name,
            "messages": messages,
            "response_format": output_cls,
            "max_completion_tokens": max_tokens,
            **kwargs,
        }
        if temperature:
            kwargs["temperature"] = temperature

        if tools:
            kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)
            # Structured response needs strict mode and no additional properties
            for tool in kwargs["tools"]:
                tool["function"]["strict"] = True
                tool["function"]["parameters"]["additionalProperties"] = False

        a_client = self._get_a_client()
        response = await a_client.beta.chat.completions.parse(**kwargs)

        stop_reason = response.choices[0].finish_reason
        if hasattr(output_cls, "model_validate_json"):
            structured_data = output_cls.model_validate_json(
                response.choices[0].message.content
            )
        else:
            structured_data = json.loads(response.choices[0].message.content)
        usage_metadata = getattr(response, "usage", None)
        token_usage = self._token_usage_from_metadata(usage_metadata)
        return ClientResponse(
            content=[StructuredBlock(content=structured_data)],
            stop_reason=stop_reason,
            usage=token_usage,
        )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-openai-like/tests/test_openai_completion.py
================================================
from datapizza.clients.openai_like import OpenAILikeClient


def test_init():
    client = OpenAILikeClient(
        api_key="test_api_key",
        model="gpt-4o-mini",
        system_prompt="You are a helpful assistant that can answer questions about piadina only in italian.",
    )
    assert client is not None



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-watsonx/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-watsonx/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-clients-watsonx"
version = "0.0.1"
description = "Ibm WatsonX client for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.11.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.7,<0.1.0",
    "ibm-watsonx-ai>=1.4.0,<2.0.0"
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-watsonx/datapizza/clients/watsonx/__init__.py
================================================
from .watsonx_client import WatsonXClient

__all__ = ["WatsonXClient"]



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-watsonx/datapizza/clients/watsonx/memory_adapter.py
================================================
import base64
import json
import logging
from typing import Any

from datapizza.memory.memory import Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)

log = logging.getLogger(__name__)


class WatsonXMemoryAdapter(MemoryAdapter):
    def _turn_to_message(self, turn: Turn) -> dict:
        content: list[dict[str, Any]] = []
        tool_calls: list[dict[str, Any]] = []

        for block in turn:
            match block:
                case TextBlock():
                    content.append({"type": "text", "text": block.content})

                case FunctionCallBlock():
                    tool_calls.append(
                        {
                            "id": block.id,
                            "type": "function",
                            "function": {
                                "name": block.name,
                                "arguments": json.dumps(block.arguments),
                            },
                        }
                    )

                case FunctionCallResultBlock():
                    # Tool output is a separate message type usually
                    return {
                        "role": "tool",
                        "tool_call_id": block.id,
                        "content": str(block.result),
                    }

                case StructuredBlock():
                    content.append({"type": "text", "text": str(block.content)})

                case MediaBlock():
                    match block.media.media_type:
                        case "image":
                            content.append(self._process_image_block(block))
                        case "pdf":
                            content.append(self._process_pdf_block(block))
                        case "audio":
                            content.append(self._process_audio_block(block))
                        case _:
                            log.warning(
                                f"Unsupported media type: {block.media.media_type}"
                            )

        message: dict[str, Any] = {
            "role": turn.role.value,
        }

        if tool_calls:
            message["tool_calls"] = tool_calls
            if content:
                message["content"] = content
            else:
                message["content"] = ""
        else:
            # Optimization: if single text block, use string content
            if len(content) == 1 and content[0]["type"] == "text":
                message["content"] = content[0]["text"]
            else:
                message["content"] = content

        return message

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        return {"role": role.value, "content": text}

    def _process_audio_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "path":
                with open(block.media.source, "rb") as f:
                    base64_audio = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_audio,
                        "format": block.media.extension,
                    },
                }

            case "base_64":
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": block.media.source,
                        "format": block.media.extension,
                    },
                }
            case "raw":
                base64_audio = base64.b64encode(block.media.source).decode("utf-8")
                return {
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_audio,
                        "format": block.media.extension,
                    },
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type} for audio"
                )

    def _process_pdf_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "base64":
                return {
                    "type": "input_file",
                    "filename": "file.pdf",
                    "file_data": f"data:application/{block.media.extension};base64,{block.media.source}",
                }
            case "path":
                with open(block.media.source, "rb") as f:
                    base64_pdf = base64.b64encode(f.read()).decode("utf-8")
                return {
                    "type": "input_file",
                    "filename": "file.pdf",
                    "file_data": f"data:application/{block.media.extension};base64,{base64_pdf}",
                }

            case _:
                raise NotImplementedError(
                    f"Unsupported media source type: {block.media.source_type}"
                )

    def _process_image_block(self, block: MediaBlock) -> dict:
        match block.media.source_type:
            case "url":
                return {
                    "type": "image_url",
                    "image_url": {"url": block.media.source},
                }

            case "base64":
                return {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/{block.media.extension};base64,{block.media.source}"
                    },
                }

            case "path":
                with open(block.media.source, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                    return {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/{block.media.extension};base64,{base64_image}"
                        },
                    }

            case _:
                raise ValueError(
                    f"Unsupported media source type: {block.media.source_type}"
                )



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-watsonx/datapizza/clients/watsonx/watsonx_client.py
================================================
import json
import logging
from collections.abc import AsyncIterator, Iterator
from typing import Any, Literal

try:
    from ibm_watsonx_ai import APIClient, Credentials
    from ibm_watsonx_ai.foundation_models import ModelInference
except ImportError:
    raise ImportError(
        "ibm-watsonx-ai is not installed. Please install it with `pip install ibm-watsonx-ai`"
    ) from None

from datapizza.core.cache import Cache
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.type import Block, FunctionCallBlock, Model, TextBlock

from .memory_adapter import WatsonXMemoryAdapter

log = logging.getLogger(__name__)


class WatsonXClient(Client):
    """
    A client for interacting with the IBM WatsonX API.
    """

    def __init__(
        self,
        api_key: str,
        url: str,
        project_id: str,
        model: str = "ibm/granite-3-3-8b-instruct",
        system_prompt: str = "",
        temperature: float | None = None,
        cache: Cache | None = None,
    ):
        """
        Args:
            api_key: The API key for WatsonX.
            url: The endpoint URL for WatsonX.
            project_id: The project ID for WatsonX.
            model: The model ID to use.
            system_prompt: The system prompt to use.
            temperature: The temperature to use.
            cache: The cache to use.
        """
        super().__init__(
            model_name=model,
            system_prompt=system_prompt,
            temperature=temperature,
            cache=cache,
        )
        self.api_key = api_key
        self.url = url
        self.project_id = project_id

        self.memory_adapter = WatsonXMemoryAdapter()
        self._set_client()

    def _set_client(self):
        if not self.client:
            credentials = Credentials(url=self.url, api_key=self.api_key)
            api_client = APIClient(credentials, project_id=self.project_id)
            self.client = ModelInference(
                model_id=self.model_name, api_client=api_client
            )

    def _set_a_client(self):
        # Async client implementation if available in SDK, otherwise raise or wrap
        # Not strictly needed if we just use self.client.achat
        pass

    def _convert_tools(self, tool: Tool) -> dict:
        return {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {
                    "type": "object",
                    "properties": tool.properties,
                    "required": tool.required,
                },
            },
        }

    def _convert_tool_choice(
        self, tool_choice: Literal["auto", "required", "none"] | list[str]
    ) -> dict | str:
        if isinstance(tool_choice, list):
            if len(tool_choice) > 1:
                raise NotImplementedError("multiple function names is not supported")
            if tool_choice:
                return {"type": "function", "function": {"name": tool_choice[0]}}
            return "none"
        return tool_choice

    def _parse_response(
        self, response: dict, tool_map: dict[str, Tool] | None
    ) -> ClientResponse:
        """Helper method to parse the response from WatsonX API."""
        content_blocks: list[Block] = []
        usage = TokenUsage()
        stop_reason = None

        if "choices" in response and len(response["choices"]) > 0:
            choice = response["choices"][0]
            message = choice.get("message", {})
            text_content = message.get("content")
            if text_content:
                content_blocks.append(TextBlock(content=text_content))

            stop_reason = choice.get("finish_reason")

            tool_calls = message.get("tool_calls")
            if tool_calls:
                for tool_call in tool_calls:
                    if tool_call.get("type") == "function":
                        function = tool_call.get("function", {})
                        name = function.get("name")

                        arguments: dict[str, Any] = {}
                        raw_args = function.get("arguments")

                        if isinstance(raw_args, str):
                            try:
                                arguments = json.loads(raw_args)
                            except json.JSONDecodeError:
                                log.warning(
                                    f"Failed to parse arguments for tool {name}: {raw_args}"
                                )
                        elif isinstance(raw_args, dict):
                            arguments = raw_args

                        if tool_map and name in tool_map:
                            content_blocks.append(
                                FunctionCallBlock(
                                    id=tool_call.get("id"),
                                    name=name,
                                    arguments=arguments,
                                    tool=tool_map[name],
                                )
                            )

        if "usage" in response:
            resp_usage = response["usage"]
            usage = TokenUsage(
                prompt_tokens=resp_usage.get("prompt_tokens", 0),
                completion_tokens=resp_usage.get("completion_tokens", 0),
                cached_tokens=0,
            )

        return ClientResponse(
            content=content_blocks, stop_reason=stop_reason, usage=usage
        )

    def _invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> ClientResponse:
        tool_map = {tool.name: tool for tool in tools} if tools else None

        # Prepare messages using memory adapter
        messages = self._memory_to_contents(
            system_prompt,
            input,
            memory,  # type: ignore
        )

        params = {}
        if temperature is not None:
            params["temperature"] = temperature
        if max_tokens is not None:
            params["max_tokens"] = max_tokens

        chat_kwargs = {}
        if tools:
            chat_kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            chat_kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)  # type: ignore

        # Invoke
        response = self.client.chat(
            messages=messages, params=params if params else None, **chat_kwargs
        )

        return self._parse_response(response, tool_map)

    async def _a_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> ClientResponse:
        tool_map = {tool.name: tool for tool in tools} if tools else None

        messages = self._memory_to_contents(
            system_prompt,
            input,
            memory,  # type: ignore
        )

        params = {}
        if temperature is not None:
            params["temperature"] = temperature
        if max_tokens is not None:
            params["max_tokens"] = max_tokens

        chat_kwargs = {}
        if tools:
            chat_kwargs["tools"] = [self._convert_tools(tool) for tool in tools]
            chat_kwargs["tool_choice"] = self._convert_tool_choice(tool_choice)  # type: ignore

        # Invoke Async
        response = await self.client.achat(
            messages=messages, params=params if params else None, **chat_kwargs
        )

        return self._parse_response(response, tool_map)

    def _stream_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        raise NotImplementedError("Stream invoke not implemented")

    async def _a_stream_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        raise NotImplementedError("Async stream invoke not implemented")

    def _structured_response(
        self,
        input: list[Block],
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        raise NotImplementedError("Structured response not implemented")

    async def _a_structured_response(
        self,
        input: list[Block] | None,
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        raise NotImplementedError("Async structured response not implemented")



================================================
FILE: datapizza-ai-clients/datapizza-ai-clients-watsonx/tests/test_watsonx.py
================================================
def test_init():
    assert 1 == 1



================================================
FILE: datapizza-ai-core/README.md
================================================

This is the core of datapizza-ai framework



================================================
FILE: datapizza-ai-core/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-core"
version = "0.0.18"
description = "Core components for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<3.14"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "opentelemetry-api>=1.33.1",
    "opentelemetry-sdk>=1.33.1",
    "pydantic>=2.10.5,<3.0.0",
    "python-dotenv>=1.0.1",
    "pyyaml>=6.0.2",
    "rich>=14.1.0",
    "typing-extensions>=4.12.2,<5.0.0",
    "jsonref>=1.1.0",
    "jinja2>=3.1.6",
    "mcp>=1.20.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-core/datapizza/agents/__init__.py
================================================
from .agent import Agent, StepResult
from .client_manager import ClientManager

__all__ = [
    "Agent",
    "ClientManager",
    "StepResult",
]



================================================
FILE: datapizza-ai-core/datapizza/agents/__version__.py
================================================
VERSION = (3, 0, 8)

__version__ = ".".join(map(str, VERSION))



================================================
FILE: datapizza-ai-core/datapizza/agents/agent.py
================================================
import inspect
from collections.abc import AsyncGenerator, Callable, Generator
from functools import wraps
from threading import Lock
from typing import Any, Literal, Union, cast

from pydantic import BaseModel

from datapizza.agents.logger import AgentLogger
from datapizza.core.clients import Client, ClientResponse
from datapizza.core.clients.models import TokenUsage
from datapizza.core.executors.async_executor import AsyncExecutor
from datapizza.core.utils import sum_token_usage
from datapizza.memory import Memory
from datapizza.tools import Tool
from datapizza.tracing.tracing import agent_span, tool_span
from datapizza.type import (
    ROLE,
    Block,
    FunctionCallBlock,
    FunctionCallResultBlock,
    TextBlock,
)

PLANNING_PROMT = """in this moment you just tell me what you are going to do.
You need to define the next steps to solve the task.
Do not use tools to solve the task.
Do not solve the task, just plan the next steps.
"""


class StepResult:
    def __init__(
        self,
        index: int,
        content: list[Block],
        usage: TokenUsage | None = None,
    ):
        self.index = index
        self.content = content
        self.usage = usage or TokenUsage()

    @property
    def text(self) -> str:
        return "\n".join(
            block.content for block in self.content if isinstance(block, TextBlock)
        )

    @property
    def tools_used(self) -> list[FunctionCallBlock]:
        return [block for block in self.content if isinstance(block, FunctionCallBlock)]


class Plan(BaseModel):
    task: str
    steps: list[str]

    def __str__(self):
        separator = "\n - "
        return f"I need to solve the task:\n\n{self.task}\n\nHere is the plan:\n\n - {separator.join(self.steps)}"


class Agent:
    name: str
    system_prompt: str = "You are a helpful assistant."

    def __init__(
        self,
        name: str | None = None,
        client: Client | None = None,
        *,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        max_steps: int | None = None,
        terminate_on_text: bool | None = True,
        stateless: bool = True,
        gen_args: dict[str, Any] | None = None,
        memory: Memory | None = None,
        stream: bool | None = None,
        # action_on_stop_reason: dict[str, Action] | None = None,
        can_call: list["Agent"] | None = None,
        logger: AgentLogger | None = None,
        planning_interval: int = 0,
        planning_prompt: str = PLANNING_PROMT,
    ):
        """
        Initialize the agent.

        Args:
            name (str, optional): The name of the agent. Defaults to None.
            client (Client): The client to use for the agent. Defaults to None.
            system_prompt (str, optional): The system prompt to use for the agent. Defaults to None.

            tools (list[Tool], optional): A list of tools to use with the agent. Defaults to None.
            max_steps (int, optional): The maximum number of steps to execute. Defaults to None.
            terminate_on_text (bool, optional): Whether to terminate the agent on text. Defaults to True.
            stateless (bool, optional): Whether to use stateless execution. Defaults to True.
            gen_args (dict[str, Any], optional): Additional arguments to pass to the agent's execution. Defaults to None.
            memory (Memory, optional): The memory to use for the agent. Defaults to None.
            stream (bool, optional): Whether to stream the agent's execution. Defaults to None.
            can_call (list[Agent], optional): A list of agents that can call the agent. Defaults to None.
            logger (AgentLogger, optional): The logger to use for the agent. Defaults to None.
            planning_interval (int, optional): The planning interval to use for the agent. Defaults to 0.
            planning_prompt (str, optional): The planning prompt to use for the agent planning steps. Defaults to PLANNING_PROMT.

        """
        if not client:
            raise ValueError("Client is required")

        if not name and not getattr(self, "name", None):
            raise ValueError(
                "Name is required, you can pass it as a parameter or set it in the agent class"
            )

        if not system_prompt and not getattr(self, "system_prompt", None):
            raise ValueError(
                "System prompt is required, you can pass it as a parameter or set it in the agent class"
            )

        self.name = name or self.name
        if not isinstance(self.name, str):
            raise ValueError("Name must be a string")

        self.system_prompt = system_prompt or self.system_prompt
        if not isinstance(self.system_prompt, str):
            raise ValueError("System prompt must be a string")

        self._client = client
        self._tools = tools or []
        self._planning_interval = planning_interval
        self._planning_prompt = planning_prompt
        self._memory = memory or Memory()
        self._stateless = stateless

        if can_call:
            self.can_call(can_call)

        self._max_steps = max_steps
        self._terminate_on_text = terminate_on_text
        self._stream = stream

        if not logger:
            self._logger = AgentLogger(agent_name=self.name)
        else:
            self._logger = logger

        for tool in self._decorator_tools():
            self._add_tool(tool)

        self._lock = Lock()

    def can_call(self, agent: Union[list["Agent"], "Agent"]):
        if isinstance(agent, Agent):
            agent = [agent]

        for a in agent:
            self._tools.append(a.as_tool())

    @classmethod
    def _tool_from_agent(cls, agent: "Agent"):
        async def invoke_agent(input_task: str):
            return cast(StepResult, await agent.a_run(input_task)).text

        a_tool = Tool(
            func=invoke_agent,
            name=agent.name,
            description=agent.__doc__,
        )
        return a_tool

    @staticmethod
    def _lock_if_not_stateless(func: Callable):
        @wraps(func)
        def decorated(self, *args, **kwargs):
            if not self._stateless and inspect.isgeneratorfunction(func):
                # For generators, we need a locking wrapper
                def locking_generator():
                    with self._lock:
                        yield from func(self, *args, **kwargs)

                return locking_generator()
            elif not self._stateless:
                with self._lock:
                    return func(self, *args, **kwargs)
            else:
                return func(self, *args, **kwargs)

        return decorated

    @staticmethod
    def _contains_ending_tool(step: StepResult) -> bool:
        content = step.content
        return any(
            block.tool.end_invoke
            for block in content
            if isinstance(block, FunctionCallBlock)
        )

    def as_tool(self):
        return Agent._tool_from_agent(self)

    def _add_tool(self, tool: Tool):
        self._tools.append(tool)

    def _decorator_tools(self):
        tools = []
        for attr_name in dir(self):
            attr = getattr(self, attr_name)
            # Check for tool methods
            if isinstance(attr, Tool):
                tools.append(attr)

        return tools

    @_lock_if_not_stateless
    def stream_invoke(
        self,
        task_input: str,
        tool_choice: Literal["auto", "required", "none", "required_first"]
        | list[str] = "auto",
        **gen_kwargs,
    ) -> Generator[ClientResponse | StepResult | Plan | None, None]:
        """
        Stream the agent's execution, yielding intermediate steps and final result.

        Args:
            task_input (str): The input text/prompt to send to the model
            tool_choice (Literal["auto", "required", "none", "required_first"] | list[str], optional): Controls which tool to use ("auto" by default)
            **gen_kwargs: Additional keyword arguments to pass to the agent's execution

        Yields:
            The intermediate steps and final result of the agent's execution

        """
        yield from self._invoke_stream(task_input, tool_choice, **gen_kwargs)

    @_lock_if_not_stateless
    async def a_stream_invoke(
        self,
        task_input: str,
        tool_choice: Literal["auto", "required", "none", "required_first"]
        | list[str] = "auto",
        **gen_kwargs,
    ) -> AsyncGenerator[ClientResponse | StepResult | Plan | None]:
        """
        Stream the agent's execution asynchronously, yielding intermediate steps and final result.

        Args:
            task_input (str): The input text/prompt to send to the model
            tool_choice (Literal["auto", "required", "none", "required_first"] | list[str], optional): Controls which tool to use ("auto" by default)
            **gen_kwargs: Additional keyword arguments to pass to the agent's execution

        Yields:
            The intermediate steps and final result of the agent's execution

        """
        async for step in self._a_invoke_stream(task_input, tool_choice, **gen_kwargs):
            yield step

    def _invoke_stream(
        self, task_input: str, tool_choice, **kwargs
    ) -> Generator[ClientResponse | StepResult | Plan | None, None]:
        self._logger.debug("STARTING AGENT")
        final_answer = None
        current_steps = 1
        memory = self._memory.copy()
        original_task = task_input

        while final_answer is None and (
            self._max_steps is None
            or (self._max_steps and current_steps <= self._max_steps)
        ):
            kwargs["tool_choice"] = tool_choice
            if tool_choice == "required_first":
                if current_steps == 1:
                    kwargs["tool_choice"] = "required"
                else:
                    kwargs["tool_choice"] = "auto"

            self._logger.debug(f"--- STEP {current_steps} ---")

            # Planning step if interval is set
            if self._planning_interval and (
                current_steps == 1 or (current_steps - 1) % self._planning_interval == 0
            ):
                plan = self._create_planning_prompt(
                    original_task, memory, current_steps
                )
                assert isinstance(plan, Plan)
                memory.add_turn(
                    TextBlock(content=str(plan)),
                    role=ROLE.ASSISTANT,
                )
                memory.add_turn(
                    TextBlock(content="Ok, go ahead and now execute the plan."),
                    role=ROLE.USER,
                )

                yield plan

                self._logger.log_panel(str(plan), title="PLAN")

            # Execute planning step
            step_output = None
            for result in self._execute_planning_step(
                current_steps, original_task, memory, **kwargs
            ):
                if isinstance(result, ClientResponse):
                    yield result
                elif isinstance(result, StepResult):
                    step_output = result.text
                    yield result

            if step_output and self._terminate_on_text:
                final_answer = step_output
                break

            if (
                result
                and isinstance(result, StepResult)
                and Agent._contains_ending_tool(result)
            ):
                self._logger.debug("ending tool found, ending agent")
                break

            current_steps += 1
            original_task = ""

        # Yield final answer if we have one
        if final_answer:
            self._logger.log_panel(final_answer, title="FINAL ANSWER")

        if not self._stateless:
            self._memory = memory

    async def _a_invoke_stream(
        self, task_input: str, tool_choice, **kwargs
    ) -> AsyncGenerator[ClientResponse | StepResult | Plan | None]:
        self._logger.debug("STARTING AGENT")
        final_answer = None
        current_steps = 1
        memory = self._memory.copy()
        original_task = task_input

        while final_answer is None and (
            self._max_steps is None
            or (self._max_steps and current_steps <= self._max_steps)
        ):
            kwargs["tool_choice"] = tool_choice
            if tool_choice == "required_first":
                if current_steps == 1:
                    kwargs["tool_choice"] = "required"
                else:
                    kwargs["tool_choice"] = "auto"

            # step_action = StepResult(index=current_steps)
            self._logger.debug(f"--- STEP {current_steps} ---")
            # yield step_action

            # Planning step if interval is set
            if self._planning_interval and (
                current_steps == 1 or (current_steps - 1) % self._planning_interval == 0
            ):
                plan = await self._a_create_planning_prompt(
                    original_task, memory, current_steps
                )
                assert isinstance(plan, Plan)
                memory.add_turn(
                    TextBlock(content=str(plan)),
                    role=ROLE.ASSISTANT,
                )
                memory.add_turn(
                    TextBlock(content="Ok, go ahead and now execute the plan."),
                    role=ROLE.USER,
                )

                yield plan

                self._logger.log_panel(str(plan), title="PLAN")

            # Execute planning step
            step_output = None
            async for result in self._a_execute_planning_step(
                current_steps, original_task, memory, **kwargs
            ):
                if isinstance(result, ClientResponse):
                    yield result
                elif isinstance(result, StepResult):
                    step_output = result.text
                    yield result

            if step_output and self._terminate_on_text:
                final_answer = step_output
                break

            if (
                result
                and isinstance(result, StepResult)
                and Agent._contains_ending_tool(result)
            ):
                self._logger.debug("ending tool found, ending agent")
                break

            current_steps += 1
            original_task = ""

        # Yield final answer if we have one
        if final_answer:
            self._logger.log_panel(final_answer, title="FINAL ANSWER")

        if not self._stateless:
            self._memory = memory

    def _create_planning_prompt(
        self, original_task: str, memory: Memory, step_number: int
    ) -> Plan:
        """Create a planning prompt that asks the agent to define next steps."""

        prompt = self.system_prompt + self._planning_prompt

        client_response = self._client.structured_response(
            input=original_task,
            tools=self._tools,
            tool_choice="none",
            memory=memory,
            system_prompt=prompt,
            output_cls=Plan,
        )
        return Plan(**client_response.structured_data[0].model_dump())

    async def _a_create_planning_prompt(
        self, original_task: str, memory: Memory, step_number: int
    ) -> Plan:
        """Create a planning prompt that asks the agent to define next steps."""
        prompt = self.system_prompt + self._planning_prompt

        client_response = await self._client.a_structured_response(
            input=original_task,
            tools=self._tools,
            tool_choice="none",
            memory=memory,
            system_prompt=prompt,
            output_cls=Plan,
        )
        return Plan(**client_response.structured_data[0].model_dump())

    def _execute_planning_step(
        self, current_step, planning_prompt: str, memory: Memory, **kwargs
    ) -> Generator[StepResult | ClientResponse, None, None]:
        """Execute a planning step with streaming support."""
        tool_results = []
        step_usage = TokenUsage()

        # Check if streaming is enabled
        response: ClientResponse
        if self._stream:
            for chunk in self._client.stream_invoke(
                input=planning_prompt,
                tools=self._tools,
                memory=memory,
                system_prompt=self.system_prompt,
                **kwargs,
            ):
                step_usage += chunk.usage
                response = chunk
                if chunk.delta:
                    yield chunk

        else:
            # Use regular non-streaming generation
            response = self._client.invoke(
                input=planning_prompt,
                tools=self._tools,
                memory=memory,
                system_prompt=self.system_prompt,
                **kwargs,
            )
            step_usage += response.usage

        if not response:
            raise RuntimeError("No response from client")

        if planning_prompt:
            memory.add_turn(TextBlock(content=planning_prompt), role=ROLE.USER)

        if response and response.text:
            memory.add_turn(TextBlock(content=response.text), role=ROLE.ASSISTANT)

        if response and response.function_calls:
            memory.add_turn(response.function_calls, role=ROLE.ASSISTANT)

        for tool_call in response.function_calls:
            tool_results.append(self._execute_tool(tool_call))

        if tool_results:
            for x in tool_results:
                memory.add_turn(x, role=ROLE.TOOL)

        step_action = StepResult(
            index=current_step,
            content=response.content + tool_results,
            usage=response.usage,
        )

        yield step_action

    async def _a_execute_planning_step(
        self, current_step, planning_prompt: str, memory: Memory, **kwargs
    ) -> AsyncGenerator[StepResult | ClientResponse, None]:
        """Execute a planning step with streaming support."""
        tool_results = []
        step_usage = TokenUsage()
        # Check if streaming is enabled
        response: ClientResponse
        if self._stream:
            async for chunk in self._client.a_stream_invoke(
                input=planning_prompt,
                tools=self._tools,
                memory=memory,
                system_prompt=self.system_prompt,
                **kwargs,
            ):
                step_usage += chunk.usage
                response = chunk
                if chunk.delta:
                    yield chunk

        else:
            # Use regular non-streaming generation
            response = await self._client.a_invoke(
                input=planning_prompt,
                tools=self._tools,
                memory=memory,
                system_prompt=self.system_prompt,
                **kwargs,
            )
            step_usage += response.usage

        if planning_prompt:
            memory.add_turn(TextBlock(content=planning_prompt), role=ROLE.USER)

        if response.text:
            memory.add_turn(TextBlock(content=response.text), role=ROLE.ASSISTANT)

        if response.function_calls:
            memory.add_turn(response.function_calls, role=ROLE.ASSISTANT)

        for tool_call in response.function_calls:
            tool_results.append(await self._a_execute_tool(tool_call))

        if tool_results:
            for x in tool_results:
                memory.add_turn(x, role=ROLE.TOOL)

        step_action = StepResult(
            index=current_step,
            content=response.content + tool_results,
            usage=response.usage,
        )

        yield step_action

    def _execute_tool(
        self, function_call: FunctionCallBlock
    ) -> FunctionCallResultBlock:
        with tool_span(f"Tool {function_call.tool.name}"):
            result = function_call.tool(**function_call.arguments)

            if inspect.iscoroutine(result):
                result = AsyncExecutor.get_instance().run(result)

            if result:
                self._logger.log_panel(
                    result,
                    title=f"TOOL {function_call.tool.name.upper()} RESULT",
                    subtitle="args: " + str(function_call.arguments),
                )
            return FunctionCallResultBlock(
                id=function_call.id,
                tool=function_call.tool,
                result=result,
            )

    async def _a_execute_tool(
        self, function_call: FunctionCallBlock
    ) -> FunctionCallResultBlock:
        with tool_span(f"Tool {function_call.tool.name}"):
            result = function_call.tool(**function_call.arguments)

            if inspect.iscoroutine(result):
                result = await result

            if result:
                self._logger.log_panel(
                    result,
                    title=f"TOOL {function_call.tool.name.upper()} RESULT",
                    subtitle="args: " + str(function_call.arguments),
                )
            return FunctionCallResultBlock(
                id=function_call.id,
                tool=function_call.tool,
                result=result,
            )

    @_lock_if_not_stateless
    def run(
        self,
        task_input: str,
        tool_choice: Literal["auto", "required", "none", "required_first"]
        | list[str] = "auto",
        **gen_kwargs,
    ) -> StepResult | None:
        """
        Run the agent on a task input.

        Args:
            task_input (str): The input text/prompt to send to the model
            tool_choice (Literal["auto", "required", "none", "required_first"] | list[str], optional): Controls which tool to use ("auto" by default)
            **gen_kwargs: Additional keyword arguments to pass to the agent's execution

        Returns:
            The final result of the agent's execution
        """
        with agent_span(f"Agent {self.name}"):
            usage = TokenUsage()
            steps = list[ClientResponse | StepResult | Plan | None](
                self._invoke_stream(task_input, tool_choice, **gen_kwargs)
            )
            usage += sum_token_usage(
                [step.usage for step in steps if isinstance(step, StepResult)]
            )

            last_step = cast(
                StepResult,
                steps[-1],
            )
            last_step.usage = usage
            return last_step

    @_lock_if_not_stateless
    async def a_run(
        self,
        task_input: str,
        tool_choice: Literal["auto", "required", "none", "required_first"]
        | list[str] = "auto",
        **gen_kwargs,
    ) -> StepResult | None:
        """
        Run the agent on a task input asynchronously.

        Args:
            task_input (str): The input text/prompt to send to the model
            tool_choice (Literal["auto", "required", "none", "required_first"] | list[str], optional): Controls which tool to use ("auto" by default)
            **gen_kwargs: Additional keyword arguments to pass to the agent's execution

        Returns:
            The final result of the agent's execution
        """
        with agent_span(f"Agent {self.name}"):
            total_usage = TokenUsage()
            results = []
            async for result in self._a_invoke_stream(
                task_input, tool_choice, **gen_kwargs
            ):
                results.append(result)

            total_usage += sum_token_usage(
                [result.usage for result in results if isinstance(result, StepResult)]
            )
            last_result = results[-1] if results else None
            if last_result:
                last_result.usage = total_usage
            return last_result



================================================
FILE: datapizza-ai-core/datapizza/agents/client_manager.py
================================================
from threading import Lock

from datapizza.core.clients import Client


class ClientManager:
    _instance: Client | None = None
    _lock = Lock()

    @classmethod
    def set_global_client(cls, client: Client) -> None:
        """Set the global Client instance.

        Args:
            config: Client instance to be used globally
        """
        with cls._lock:
            cls._instance = client

    @classmethod
    def get_global_client(cls) -> Client | None:
        """Get the current global Client instance.

        Returns:
            The global client instance if set, None otherwise
        """
        return cls._instance

    @classmethod
    def clear_global_client(cls) -> None:
        """Clear the global Client instance."""
        with cls._lock:
            cls._instance = None



================================================
FILE: datapizza-ai-core/datapizza/agents/logger.py
================================================
import logging
import os
from datetime import datetime

from rich.console import Console
from rich.panel import Panel

log = logging.getLogger(__name__)


class AgentLogger:
    def __init__(self, agent_name: str):
        self.agent_name = agent_name
        # Use a deterministic hash function based on the string
        color_num = sum(ord(c) * i for i, c in enumerate(self.agent_name, 1)) % 255
        self.color = f"color({color_num})"
        self.console = Console()

    def _colored_log(self, log_text: str, *args, **kwargs) -> None:
        if not log_text:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.console.print(
            f"[white]{timestamp}[/] [{self.color}]<{self.agent_name}> {log_text} [/]",
            *args,
            **kwargs,
        )

    def _log(self, log_text: int, *args, **kwargs) -> None:
        log.log(log_text, *args, **kwargs)

    def log_panel(self, *args, **kwargs) -> None:
        if self._isEnabledFor(logging.DEBUG) and args:
            self.console.print(
                f"<[{self.color}]{self.agent_name}[/]>",
                Panel(*args, **kwargs, border_style=self.color, subtitle_align="left"),
            )

    def _isEnabledFor(self, level: int) -> bool:
        env_level = os.getenv("DATAPIZZA_AGENT_LOG_LEVEL", "DEBUG")
        numeric_level = getattr(logging, env_level.upper(), logging.INFO)
        return level >= numeric_level

    def debug(self, msg, *args, **kwargs):
        if self._isEnabledFor(logging.DEBUG):
            self._colored_log(msg, *args, **kwargs)

    def info(self, msg, *args, **kwargs):
        if self._isEnabledFor(logging.INFO):
            self._log(logging.INFO, msg, args, **kwargs)

    def warning(self, msg, *args, **kwargs):
        if self._isEnabledFor(logging.WARNING):
            self._log(logging.WARNING, msg, args, **kwargs)

    def error(self, msg, *args, **kwargs):
        if self._isEnabledFor(logging.ERROR):
            self._log(logging.ERROR, msg, args, **kwargs)

    def critical(self, msg, *args, **kwargs):
        if self._isEnabledFor(logging.CRITICAL):
            self._log(logging.CRITICAL, msg, args, **kwargs)

    def fatal(self, msg, *args, **kwargs):
        if self._isEnabledFor(logging.FATAL):
            self._log(logging.FATAL, msg, args, **kwargs)



================================================
FILE: datapizza-ai-core/datapizza/agents/tests/test_base_agents.py
================================================
import asyncio
from concurrent.futures import ThreadPoolExecutor

from datapizza.agents.agent import PLANNING_PROMT, Agent, StepResult
from datapizza.clients import MockClient
from datapizza.core.clients import ClientResponse
from datapizza.tools import tool


class TestBaseAgents:
    def test_agent_defaults(self):
        agent = Agent(name="datapizza_agent", client=MockClient())
        assert agent.name == "datapizza_agent"
        assert agent.system_prompt == "You are a helpful assistant."
        assert agent._planning_prompt == PLANNING_PROMT

    def test_init_agent(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            planning_prompt="test planning prompt",
        )
        assert agent.name == "test"
        assert agent._planning_prompt == "test planning prompt"

    def test_invoke_agent(self):
        agent = Agent(
            name="test", client=MockClient(), system_prompt="You are a test agent"
        )
        assert agent.run("Hello").text == "Hello"

    def test_a_invoke_agent(self):
        agent = Agent(
            name="test", client=MockClient(), system_prompt="You are a test agent"
        )
        res = asyncio.run(agent.a_run("Hello"))  # type: ignore
        assert res.text == "Hello"

    def test_stream_invoke_agent(self):
        agent = Agent(
            name="test", client=MockClient(), system_prompt="You are a test agent"
        )
        res = list(agent.stream_invoke("Hello"))
        assert isinstance(res[0], StepResult)
        assert res[0].index == 1
        assert res[0].text == "Hello"

    def test_can_call_agent(self):
        agent1 = Agent(
            name="test1", client=MockClient(), system_prompt="You are a test agent"
        )
        agent2 = Agent(
            name="test2", client=MockClient(), system_prompt="You are a test agent"
        )
        agent1.can_call(agent2)
        assert agent1._tools[0].name == agent2.as_tool().name
        assert agent1._tools[0].description == agent2.as_tool().description

        agent_aggregator = Agent(
            name="test_aggregator",
            client=MockClient(),
            system_prompt="You are a test agent",
            can_call=[agent1, agent2],
        )
        assert agent_aggregator._tools[0].name == agent1.as_tool().name
        assert agent_aggregator._tools[1].name == agent2.as_tool().name

    def test_params_as_class_attributes(self):
        class TestAgent(Agent):
            name = "test"
            system_prompt = "You are a test agent"

        client = MockClient()
        agent = TestAgent(client=client)
        assert agent.name == "test"
        assert agent._client == client
        assert agent.system_prompt == "You are a test agent"

    def test_tools_as_class_attributes(self):
        class TestAgent(Agent):
            name = "test"
            system_prompt = "You are a test agent"

            @tool
            def test_tool(self, x: str) -> str:
                return x

        agent = TestAgent(client=MockClient())
        assert agent._tools[0].name == "test_tool"

    def test_agent_stream_text(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stream=True,
        )
        res = list(agent.stream_invoke("Hello, how are you?"))
        assert isinstance(res[0], ClientResponse)
        assert res[0].text == "H"
        assert res[1].text == "He"
        assert res[2].text == "Hel"


class TestStatelessAgents:
    def test_stateless_agent_invoke(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stateless=True,
        )
        assert agent.run("Hello").text == "Hello"
        assert len(agent._memory) == 0

    def test_stateless_agent_stream_invoke(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stateless=True,
        )
        res = list(agent.stream_invoke("Hello, how are you?"))
        assert isinstance(res[0], StepResult)
        assert res[0].text == "Hello, how are you?"
        assert len(agent._memory) == 0

    def test_not_stateless_agent_invoke(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stateless=False,
        )
        assert agent.run("Hello").text == "Hello"
        assert len(agent._memory) == 2

        agent.run("Hello")
        assert len(agent._memory) == 4

    def test_non_stateless_lock_async(self):
        async def test_func():
            agent = Agent(
                name="test",
                client=MockClient(),
                system_prompt="You are a test agent",
                stateless=False,
            )

            tasks = []
            for x in range(3):
                tasks.append(asyncio.create_task(agent.a_run(str(x))))

            assert len(agent._memory) == 0

            res = await asyncio.gather(*tasks)

            assert len(agent._memory) == 6
            return res

        asyncio.run(test_func())

    def test_non_stateless_lock_thread(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stateless=False,
        )

        def test_func(x):
            agent.run(str(x))

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(test_func, x) for x in range(9)]
            [future.result() for future in futures]

        assert len(agent._memory) == 18

    def test_non_stateless_stream_invoke_lock(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stateless=False,
        )
        res = agent.stream_invoke("Hello, how are you?")
        res2 = agent.stream_invoke("Hello, how are you?")
        assert isinstance(next(res), StepResult)
        list(res)
        assert isinstance(next(res2), StepResult)
        list(res2)

        assert len(agent._memory) == 4

    def test_stateless_stream_invoke_lock(self):
        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            stateless=True,
        )
        res = agent.stream_invoke("Hello, how are you?")
        res2 = agent.stream_invoke("Hello, how are you?")

        next(res)
        next(res2)

        list(res)
        list(res2)

        assert len(agent._memory) == 0

    def test_tools_with_end_invoke(self):
        @tool(end=True)
        def test_tool(*args, **kwargs):
            return "tool called"

        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            tools=[test_tool],
        )

        res = agent.run("function call")
        assert len(res.tools_used)
        assert res.index == 1

    def test_tools_with_not_end_invoke(self):
        @tool(end=False)
        def test_tool(*args, **kwargs):
            return "tool called"

        agent = Agent(
            name="test",
            client=MockClient(),
            system_prompt="You are a test agent",
            tools=[test_tool],
        )

        res = agent.run("function call")
        assert res.index == 2



================================================
FILE: datapizza-ai-core/datapizza/cache/__init__.py
================================================
# Import MemoryCache from core implementation
from pkgutil import extend_path

__path__ = extend_path(__path__, __name__)

from datapizza.core.cache import MemoryCache

__all__ = ["MemoryCache"]



================================================
FILE: datapizza-ai-core/datapizza/clients/__init__.py
================================================
from pkgutil import extend_path

__path__ = extend_path(__path__, __name__)

from .factory import ClientFactory
from .mock_client import MockClient

__all__ = ["ClientFactory", "MockClient"]



================================================
FILE: datapizza-ai-core/datapizza/clients/factory.py
================================================
from enum import Enum

from datapizza.clients.mock_client import MockClient
from datapizza.core.clients.client import Client


class Provider(str, Enum):
    """Supported LLM providers"""

    OPENAI = "openai"
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    MISTRAL = "mistral"
    AZURE_OPENAI = "azure_openai"
    WATSONX = "watsonx"
    MOCK = "mock"


class ClientFactory:
    """Factory for creating LLM clients"""

    @staticmethod
    def create(
        provider: str | Provider,
        api_key: str,
        model: str,
        system_prompt: str = "",
        temperature: float = 0.7,
        **kwargs,
    ) -> "Client":
        """
        Create a client instance based on the specified provider.

        Args:
            provider: The LLM provider to use (openai, google, or anthropic)
            api_key: API key for the provider
            model: Model name to use (provider-specific)
            system_prompt: System prompt to use
            temperature: Temperature for generation (0-2)
            **kwargs: Additional provider-specific arguments

        Returns:
            An instance of the appropriate client

        Raises:
            ValueError: If the provider is not supported
        """
        if isinstance(provider, str):
            provider = Provider(provider.lower())

        match provider:
            case Provider.OPENAI:
                try:
                    from datapizza.clients.openai import OpenAIClient  # type: ignore
                except ImportError as e:
                    raise ImportError(
                        "OpenAI client is not installed. Please install it with `pip install datapizza-ai-clients-openai`"
                    ) from e

                return OpenAIClient(
                    api_key=api_key,
                    model=model,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    **kwargs,
                )

            case Provider.GOOGLE:
                try:
                    from datapizza.clients.google import GoogleClient  # type: ignore
                except ImportError as e:
                    raise ImportError(
                        "Google client is not installed. Please install it with `pip install datapizza-ai-clients-google`"
                    ) from e

                return GoogleClient(
                    api_key=api_key,
                    model=model,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    **kwargs,
                )

            case Provider.ANTHROPIC:
                try:
                    from datapizza.clients.anthropic import (  # type: ignore
                        AnthropicClient,
                    )
                except ImportError as e:
                    raise ImportError(
                        "Anthropic client is not installed. Please install it with `pip install datapizza-ai-clients-anthropic`"
                    ) from e

                return AnthropicClient(
                    api_key=api_key,
                    model=model,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    **kwargs,
                )

            case Provider.MISTRAL:
                try:
                    from datapizza.clients.mistral import MistralClient  # type: ignore
                except ImportError as e:
                    raise ImportError(
                        "Mistral client is not installed. Please install it with `pip install datapizza-ai-clients-mistral`"
                    ) from e

                return MistralClient(
                    api_key=api_key,
                    model=model,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    **kwargs,
                )

            case Provider.AZURE_OPENAI:
                try:
                    from datapizza.clients.azure_openai_client import (  # type: ignore
                        AzureOpenAIClient,
                    )
                except ImportError as e:
                    raise ImportError(
                        "Azure OpenAI client is not installed. Please install it with `pip install datapizza-ai-clients-azure-openai`"
                    ) from e

                return AzureOpenAIClient(
                    api_key=api_key,
                    model=model,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    **kwargs,
                )

            case Provider.WATSONX:
                try:
                    from datapizza.clients.watsonx import (  # type: ignore
                        WatsonXClient,
                    )
                except ImportError as e:
                    raise ImportError(
                        "IBM WatsonX client is not installed. Please install it with `pip install datapizza-ai-clients-watsonx`"
                    ) from e

                return WatsonXClient(
                    api_key=api_key,
                    model=model,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    **kwargs,
                )
            case Provider.MOCK:
                return MockClient(model_name=model, system_prompt=system_prompt)
            case _:
                raise ValueError(f"Unsupported provider: {provider}")



================================================
FILE: datapizza-ai-core/datapizza/clients/mock_client.py
================================================
import logging
from typing import Literal

from datapizza.core.clients import ClientResponse
from datapizza.core.clients.client import Client
from datapizza.core.clients.models import TokenUsage
from datapizza.memory.memory import Memory, Turn
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.tools.tools import Tool
from datapizza.type import (
    ROLE,
    Block,
    FunctionCallBlock,
    FunctionCallResultBlock,
    Model,
    StructuredBlock,
    TextBlock,
)

log = logging.getLogger(__name__)


class FakeMemoryAdapter(MemoryAdapter):
    def _text_to_message(self, text: str, role: ROLE) -> dict:
        return {"role": role.value, "content": text}

    def _turn_to_message(self, turn: Turn) -> dict:
        return {"role": turn.role.value, "blocks": turn.blocks}


class MockClient(Client):
    """A client for interacting with the Mock API.

    This class provides methods for invoking the Mock API to generate responses
    based on given input data. It extends the InferenceClient class.
    """

    def __init__(
        self,
        model_name: str | None = None,
        system_prompt: str = "",
        temperature: float = 0.6,
    ):
        super().__init__(
            model_name or "mock_client",
            system_prompt=system_prompt,
            temperature=temperature,
        )

        self.memory_adapter = FakeMemoryAdapter()

    def _invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ):
        if tools is None:
            tools = []

        input_text = ""
        if isinstance(input, list):
            for b in input:
                if isinstance(b, TextBlock):
                    input_text = b.content

        if memory and isinstance(memory[-1].blocks[-1], FunctionCallResultBlock):
            return ClientResponse(
                content=[TextBlock(content=memory[-1].blocks[-1].result)]
            )

        if not input_text:
            return ClientResponse(
                content=[
                    TextBlock(
                        content="hi i got this input: "
                        + " and a memory of length: "
                        + (str(len(memory)) if memory else "None")
                    )
                ]
            )

        if "function" in input_text and tools:
            arguments = {
                "text": "This is a test",
            }
            return ClientResponse(
                content=[
                    FunctionCallBlock(
                        id="1",
                        arguments=arguments,
                        name=tools[0].name,
                        tool=tools[0],
                    )
                ]
            )

        if "exception" in input_text:
            raise Exception("This is a test exception")

        if memory:
            text = ""
            for b in memory.iter_blocks():
                text += b.content

            text += input_text

            return ClientResponse(
                content=[TextBlock(content=text)],
                usage=TokenUsage(
                    prompt_tokens=len(input_text),
                    completion_tokens=len(text),
                    cached_tokens=0,
                ),
            )

        return ClientResponse(
            content=[TextBlock(content=input_text)],
            usage=TokenUsage(
                prompt_tokens=len(input_text),
                completion_tokens=len(input_text),
                cached_tokens=0,
            ),
        )

    async def _a_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ):
        if tools is None:
            tools = []
        return self._invoke(
            input=input,
            tools=tools,
            memory=memory,
            tool_choice=tool_choice,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt,
            **kwargs,
        )

    def _structured_response(
        self,
        input: list[Block],
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ):
        if isinstance(input[0], TextBlock):
            return ClientResponse(
                content=[
                    StructuredBlock(
                        content=output_cls.model_validate_json(input[0].content)
                    )
                ]
            )
        else:
            raise ValueError("input must be a list of TextBlock")

    def _stream_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ):
        if tools is None:
            tools = []
        given_response = ""

        if not isinstance(input[0], TextBlock):
            raise ValueError("input must be a list of TextBlock")

        for char in input[0].content:
            given_response += char
            yield ClientResponse(
                content=[TextBlock(content=given_response)], delta=char
            )

    async def _a_stream_invoke(  # type: ignore
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ):
        if tools is None:
            tools = []
        given_response = ""
        for char in input[0].content:  # type: ignore
            given_response += char
            yield ClientResponse(
                content=[TextBlock(content=given_response)], delta=char
            )

    def _convert_tool_choice(self, tool_choice: str | list[str]) -> dict:
        return {}

    def _a_structured_response(  # type: ignore
        self,
        input: list[Block],
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ):
        return self._structured_response(
            input=input,
            output_cls=output_cls,
            memory=memory,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt,
            tools=tools,
            tool_choice=tool_choice,
            **kwargs,
        )



================================================
FILE: datapizza-ai-core/datapizza/core/__init__.py
================================================
import logging

from datapizza.core.utils import _basic_config

# Setup base logging

# Create and configure the main package logger
log = logging.getLogger("datapizza")
_basic_config(log)

log.setLevel(logging.DEBUG)



================================================
FILE: datapizza-ai-core/datapizza/core/__version__.py
================================================
VERSION = (3, 0, 8)

__version__ = ".".join(map(str, VERSION))



================================================
FILE: datapizza-ai-core/datapizza/core/constants.py
================================================
CAPTIONER_DEFAULT_USER_PROMPT_FIGURES = "Describe the image in Italian with as much detail as possible. Focus on every element you see, including colors, shapes, objects, people, actions, emotions, and the setting. Be vivid and descriptive in your explanation."
CAPTIONER_DEFAULT_USER_PROMPT_TABLES = "Describe the table in Italian with as much detail as possible. Focus on every element you notice, including the structure, headings, rows, columns, values, patterns, and any notable trends or anomalies. Be precise and descriptive in your explanation."
DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant."



================================================
FILE: datapizza-ai-core/datapizza/core/models.py
================================================
from abc import ABC, abstractmethod
from typing import Any

from datapizza.tracing.tracing import tracer


class ChainableProducer(ABC):
    """
    A class that can produce a module.
    If a ChainableProducer is used as a node in a pipeline, it will produce a module.
    """

    def as_module_component(self):
        """
        Returns a module component that can be used in a pipeline.
        """
        return self._as_module_component()

    @abstractmethod
    def _as_module_component(self):
        raise NotImplementedError("Subclasses must implement _as_module_component")


class PipelineComponent(ABC):
    """
    Abstract base class for components that can be used in datapizza-ai pipelines.

    Provides a common __call__ interface for execution logging and delegates
    the core logic to the component's _run/_a_run methods.

    Supports both synchronous and asynchronous processing through _run and
    _a_run methods respectively.
    """

    def __call__(self, *args, **kwargs):
        """
        Synchronous execution entry point that delegates to run.

        This method is called when the component is called in a synchronous context.
        """
        return self.run(*args, **kwargs)

    def validate_input(self, args, kwargs):
        """
        Validate the input of the component.
        """
        assert 1 == 1

    def validate_output(self, data):
        """
        Validate the output of the component.
        """
        assert 1 == 1

    def run(self, *args, **kwargs) -> Any:
        """
        Synchronous execution wrapper around _run with tracing and validation.

        This method is called when the component is executed in a sync context.
        """
        with tracer.start_as_current_span(
            f"PipelineComponent.{self.__class__.__name__}"
        ):
            self.validate_input(args, kwargs)
            data = self._run(*args, **kwargs)
            self.validate_output(data)
            return data

    async def a_run(self, *args, **kwargs) -> Any:
        """
        Asynchronous execution wrapper around _a_run with tracing and validation.

        This method is called when the component is awaited in an async context.
        """
        with tracer.start_as_current_span(
            f"PipelineComponent.{self.__class__.__name__}"
        ):
            self.validate_input(args, kwargs)
            data = await self._a_run(*args, **kwargs)
            self.validate_output(data)
            return data

    @abstractmethod
    def _run(self, *args, **kwargs) -> Any:
        """
        The core processing logic of the component.

        Subclasses must implement this method to define their specific behavior.
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} must implement the _run method"
        )

    async def _a_run(self, *args, **kwargs) -> Any:
        """
        The asynchronous core processing logic of the component.

        Subclasses must implement this method to define their async-specific behavior.
        For simple cases, this can delegate to the synchronous _run method.
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} must implement the _a_run method"
        )



================================================
FILE: datapizza-ai-core/datapizza/core/utils.py
================================================
import base64
import io
import logging
import os
import sys
from typing import Any

from typing_extensions import override

from datapizza.core.clients.models import TokenUsage

# logger: logging.Logger = logging.getLogger(__name__)


SENSITIVE_HEADERS = {"api-key", "authorization"}


def is_dict(obj: object) -> bool:
    return isinstance(obj, dict)


def _basic_config(logger: logging.Logger) -> None:
    # Color codes for different log levels
    COLORS = {
        "DEBUG": "\033[36m",  # Cyan
        "INFO": "\033[32m",  # Green
        "WARNING": "\033[33m",  # Yellow
        "ERROR": "\033[31m",  # Red
        "CRITICAL": "\033[31;1m",  # Bright Red
    }
    RESET = "\033[0m"

    # Create a formatter with level at the start and color
    formatter = logging.Formatter(
        fmt="%(levelname_colored)s [%(asctime)s - %(name)s:%(lineno)d] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Add color to the log level
    old_format = formatter.format

    def format(record):
        record.levelname_colored = (
            f"{COLORS.get(record.levelname, '')}{record.levelname:<8}{RESET}"
        )
        return old_format(record)

    formatter.format = format

    # Create a handler (console handler) using stdout
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.addFilter(EnvLogLevelFilter())
    console_handler.addFilter(SensitiveHeadersFilter())

    logger.addFilter(EnvLogLevelFilter())
    logger.addFilter(SensitiveHeadersFilter())
    logger.setLevel(logging.DEBUG)
    logger.addHandler(console_handler)
    logger.propagate = False


def sum_token_usage(usages: list[TokenUsage]) -> TokenUsage:
    return sum(usages, TokenUsage())


class EnvLogLevelFilter(logging.Filter):
    """Filter that checks the environment variable for log level before each log record."""

    @override
    def filter(self, record: logging.LogRecord) -> bool:
        # Get current log level from environment
        env_level = os.getenv("DATAPIZZA_LOG_LEVEL", "INFO")
        # Convert string level to numeric level
        numeric_level = getattr(logging, env_level.upper(), logging.INFO)
        # Check if this record should pass based on environment level
        return record.levelno >= numeric_level


class SensitiveHeadersFilter(logging.Filter):
    @override
    def filter(self, record: logging.LogRecord) -> bool:
        if (
            record.args
            and isinstance(record.args, dict)
            and "headers" in record.args
            and isinstance(record.args["headers"], dict)
        ):
            headers = record.args["headers"] = {**record.args["headers"]}
            for header in headers:
                if str(header).lower() in SENSITIVE_HEADERS:
                    headers[header] = "<redacted>"
        return True


def extract_media(coordinates, file_path, page_number):
    try:
        import fitz
    except ImportError as e:
        raise ImportError(
            "PyMuPDF is not installed. Please install it using `pip install PyMuPDF`."
        ) from e

    try:
        from PIL import Image, ImageDraw
    except ImportError as e:
        raise ImportError(
            "PIL is not installed. Please install it using `pip install Pillow`."
        ) from e

    doc = fitz.open(file_path)

    current_page = doc[page_number - 1]

    polygon_coords_points = [coord * 72 for coord in coordinates]

    zoom = 2
    mat = fitz.Matrix(zoom, zoom)
    pix = current_page.get_pixmap(matrix=mat)

    img = Image.frombytes("RGB", (pix.width, pix.height), pix.samples)  # type: ignore

    x_scale = pix.width / current_page.rect.width
    y_scale = pix.height / current_page.rect.height

    pixel_coords = [
        (polygon_coords_points[i] * x_scale, polygon_coords_points[i + 1] * y_scale)
        for i in range(0, len(polygon_coords_points), 2)
    ]

    mask = Image.new("L", img.size, 0)
    mask_draw = ImageDraw.Draw(mask)
    mask_draw.polygon(pixel_coords, fill=255)

    img_masked = Image.new("RGBA", img.size, (255, 255, 255, 0))
    img_masked.paste(img, (0, 0), mask)

    bbox = mask.getbbox()
    img_cropped = img_masked.crop(bbox)

    # Convert image to base64
    img_buffer = io.BytesIO()
    img_cropped.save(img_buffer, format="PNG")  # Save as PNG
    img_base64 = base64.b64encode(img_buffer.getvalue()).decode(
        "utf-8"
    )  # Convert to base64 string

    return img_base64


# Helper function to replace environment variables
def replace_env_vars(
    value,
    constants: dict[str, str] | None = None,
    skip_unknown: bool = False,
) -> Any:
    """Replace ${VAR_NAME} placeholders with values from constants or environment variables.

    Args:
        value: The value to process (can be string, dict, list, or any other type)
        constants: Dictionary mapping variable names to their values
        skip_unknown: If True, leave unknown ${VAR_NAME} placeholders untouched instead of
                      raising an error. Useful when other placeholder types (like elements)
                      will be processed later.

    Returns:
        The value with placeholders replaced
    """
    if not constants:
        constants = {}

    if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
        var_name = value[2:-1]

        if var_name in constants:
            return constants[var_name]

        env_value = os.environ.get(var_name)
        if env_value:
            return env_value

        if skip_unknown:
            return value  # Leave the placeholder as-is for later processing

        raise ValueError(f"Environment variable {var_name} not found or empty")
    elif isinstance(value, dict):
        return {
            k: replace_env_vars(v, constants, skip_unknown) for k, v in value.items()
        }
    elif isinstance(value, list):
        return [replace_env_vars(item, constants, skip_unknown) for item in value]
    else:
        return value



================================================
FILE: datapizza-ai-core/datapizza/core/cache/__init__.py
================================================
from datapizza.core.cache.cache import Cache, MemoryCache, cacheable

__all__ = ["Cache", "MemoryCache", "cacheable"]



================================================
FILE: datapizza-ai-core/datapizza/core/cache/cache.py
================================================
import hashlib
import logging
from abc import ABC, abstractmethod
from collections.abc import Callable
from functools import wraps

log = logging.getLogger(__name__)


class Cache(ABC):
    """
    This is the abstract base class for all cache implementations.
    Concrete subclasses must provide implementations for the abstract methods that define how caching is handled.


    When a cache instance is attached to a client, it will automatically store the results of the client`s method calls.
    If the same method is invoked multiple times with identical arguments, the cache returns the stored result instead of re-executing the method.
    """

    @abstractmethod
    def get(self, key: str) -> object:
        """
        Retrieve an object from the cache.

        Args:
            key (str): The key to retrieve the object for.

        Returns:
            The object stored in the cache.
        """

    @abstractmethod
    def set(self, key: str, value: str):
        """
        Store an object in the cache.

        Args:
            key (str): The key to store the object for.
            value (str): The object to store in the cache.
        """


def cacheable(key_func: Callable):
    """
    Decorator that caches function results based on key functions.

    Args:
        *key_funcs: Functions that take the function's self and arguments and return a value for the cache key
    """

    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            if not self.cache:
                return func(self, *args, **kwargs)

            try:
                # Get all arguments as a dictionary
                bound_args = kwargs.copy()
                func_args = func.__code__.co_varnames[: func.__code__.co_argcount]
                for i, arg_name in enumerate(func_args[1:]):  # Skip 'self'
                    if i < len(args):
                        bound_args[arg_name] = args[i]

                # Generate cache key using the provided functions

                cache_key = key_func(self, bound_args)
                # hash the string
                cache_key = hashlib.sha256(cache_key.encode()).hexdigest()

                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    log.info(f"Cache hit for {cache_key}")
                    return cached_result

            except Exception as e:
                log.warning(f"Error generating cache key for {func.__name__}: {e}")
                return func(self, *args, **kwargs)

            # Execute function if not cached
            result = func(self, *args, **kwargs)
            try:
                self.cache.set(cache_key, result)
            except Exception as e:
                log.error(f"Error setting cache for {cache_key}: {e}")
            return result

        return wrapper

    return decorator


class MemoryCache(Cache):
    """
    A simple in-memory cache implementation.
    """

    def __init__(self):
        self.cache = {}

    def get(self, key: str) -> object:
        """Retrieve an object from the cache."""
        return self.cache.get(key)

    def set(self, key: str, value: str):
        """Set an object in the cache."""
        self.cache[key] = value



================================================
FILE: datapizza-ai-core/datapizza/core/clients/__init__.py
================================================
from datapizza.core.clients.client import Client
from datapizza.core.clients.models import ClientResponse

__all__ = ["Client", "ClientResponse"]



================================================
FILE: datapizza-ai-core/datapizza/core/clients/client.py
================================================
import json
import logging
import os
import time
from abc import abstractmethod
from collections.abc import AsyncIterator, Iterator
from typing import Any, Literal

from pydantic import BaseModel

from datapizza.core.cache import Cache, cacheable
from datapizza.core.clients.models import ClientResponse
from datapizza.core.models import ChainableProducer, PipelineComponent
from datapizza.memory import Memory
from datapizza.memory.memory_adapter import MemoryAdapter
from datapizza.tools import Tool
from datapizza.tracing.tracing import generation_span
from datapizza.type import Block, Model, TextBlock

# Use the module name to ensure proper hierarchical relationship with parent logger
log = logging.getLogger(__name__)


# Abstract class for all clients
class Client(ChainableProducer):
    """
    Represents the base class for all clients.
    Concrete implementations must implement the abstract methods to handle the actual inference.

    """

    def __init__(
        self,
        model_name: str,
        system_prompt: str,
        temperature: float | None = None,
        cache: Cache | None = None,
    ):
        log.debug(
            f"Initializing client with model_name: {model_name}, system_prompt: {system_prompt}, temperature: {temperature}"
        )
        self.model_name = model_name
        self.temperature = temperature
        self.system_prompt = system_prompt
        self.cache = cache
        self.client = None
        self.a_client = None
        self.memory_adapter: MemoryAdapter

    def _get_client(self) -> Any:
        if not self.client:
            self._set_client()
        return self.client

    def _set_client(self):
        raise NotImplementedError("This method should be implemented by the subclass")

    def _get_a_client(self) -> Any:
        if not self.a_client:
            self._set_a_client()
        return self.a_client

    def _set_a_client(self):
        raise NotImplementedError("This method should be implemented by the subclass")

    def _get_cache_key(self, args: dict) -> str:
        user_input = args.get("input", "")
        output_cls = args.get("output_cls")
        input_key: str = ""
        if isinstance(user_input, list):
            for b in user_input:
                input_key += str(hash(b))

        if isinstance(user_input, str):
            input_key = user_input

        key: str = (
            str(input_key)
            + str(self.model_name)
            + (args.get("system_prompt") or self.system_prompt or "")
            + (str(hash(args.get("memory"))) if args.get("memory") else "")
            + (str(output_cls.model_fields) if output_cls else "")
        )
        return key

    @cacheable(_get_cache_key)
    def invoke(
        self,
        input: str | list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> ClientResponse:
        """
        Performs a single inference request to the model.

        Args:
            input (str): The input text/prompt to send to the model
            tools (List[Tool], optional): List of tools available for the model to use. Defaults to [].
            memory (Memory, optional): Memory object containing conversation history. Defaults to None.
            tool_choice (str, optional): Controls which tool to use. Defaults to "auto".
            temperature (float, optional): Controls randomness in responses. Defaults to None.
            max_tokens (int, optional): Maximum number of tokens in the response. Defaults to None.
            system_prompt (str, optional): System-level instructions for the model. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the model's inference method

        Returns:
            A ClientResponse object containing the model's response
        """
        if tools is None:
            tools = []

        if isinstance(input, str) and input:
            input = [TextBlock(content=input)]
        elif input == "":
            input = []

        log.debug(
            f"Invoking model {self.__class__.__name__} with input: {str(input)[:100]}..."
        )

        with generation_span("client.invoke") as span:
            if tools is None:
                tools = []

            response = self._invoke(
                input=input,
                tools=tools,
                memory=memory,
                tool_choice=tool_choice,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt or self.system_prompt,
                **kwargs,
            )
            span.set_attribute("prompt_tokens_used", response.prompt_tokens_used)
            span.set_attribute(
                "completion_tokens_used", response.completion_tokens_used
            )
            span.set_attribute("cached_tokens_used", response.cached_tokens_used)
            span.set_attribute("model_name", self.model_name)
            if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                span.set_attribute(
                    "input", json.dumps([b.to_dict() for b in input]) if input else ""
                )
                span.set_attribute("output", response.text)
                span.set_attribute("memory", memory.json_dumps() if memory else "None")
            span.set_attribute("stop_reason", response.stop_reason or "None")

        assert isinstance(response, ClientResponse)
        return response

    @cacheable(_get_cache_key)
    async def a_invoke(
        self,
        input: str | list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> ClientResponse:
        """
        Performs a single inference request to the model.

        Args:
            input (str): The input text/prompt to send to the model
            tools (List[Tool], optional): List of tools available for the model to use. Defaults to [].
            memory (Memory, optional): Memory object containing conversation history. Defaults to None.
            tool_choice (str, optional): Controls which tool to use. Defaults to "auto".
            temperature (float, optional): Controls randomness in responses. Defaults to None.
            max_tokens (int, optional): Maximum number of tokens in the response. Defaults to None.
            system_prompt (str, optional): System-level instructions for the model. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the model's inference method

        Returns:
            A ClientResponse object containing the model's response
        """
        if isinstance(input, str) and input:
            input = [TextBlock(content=input)]
        elif input == "":
            input = []

        with generation_span("client.a_invoke") as span:
            if tools is None:
                tools = []
            log.debug(f"Invoking model {self.__class__.__name__} with input: {input}")

            if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                span.set_attribute(
                    "input", json.dumps([b.to_dict() for b in input]) if input else ""
                )

            response = await self._a_invoke(
                input=input,
                tools=tools,
                memory=memory,
                tool_choice=tool_choice,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt or self.system_prompt,
                **kwargs,
            )
            if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                span.set_attribute("output", response.text)
                span.set_attribute("memory", memory.json_dumps() if memory else "None")
            span.set_attribute("prompt_tokens_used", response.prompt_tokens_used)
            span.set_attribute(
                "completion_tokens_used", response.completion_tokens_used
            )
            span.set_attribute("cached_tokens_used", response.cached_tokens_used)
            span.set_attribute("model_name", self.model_name)
            span.set_attribute("stop_reason", response.stop_reason or "None")

        assert isinstance(response, ClientResponse)
        return response

    def stream_invoke(
        self,
        input: str | list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        """
        Streams the model's response token by token.

        Args:
            input (str): The input text/prompt to send to the model
            tools (List[Tool], optional): List of tools available for the model to use. Defaults to [].
            memory (Memory, optional): Memory object containing conversation history. Defaults to None.
            tool_choice (str, optional): Controls which tool to use. Defaults to "auto".
            temperature (float, optional): Controls randomness in responses. Defaults to None.
            max_tokens (int, optional): Maximum number of tokens in the response. Defaults to None.
            system_prompt (str, optional): System-level instructions for the model. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the model's inference method

        Returns:
            An iterator yielding ClientResponse objects containing the model's response
        """

        if isinstance(input, str) and input:
            input = [TextBlock(content=input)]
        elif input == "":
            input = []

        with generation_span("client.stream_invoke") as span:
            if tools is None:
                tools = []
            log.debug(f"Streaming invoke model {self.__class__.__name__}")
            first_response = True

            response_stream = self._stream_invoke(
                input,
                tools,
                memory,
                tool_choice,
                temperature or self.temperature,
                max_tokens,
                system_prompt or self.system_prompt,
                **kwargs,
            )
            last_response = None
            for r in response_stream:
                if first_response:
                    span.set_attribute("time_at_first_token", int(time.time() * 1000))
                    first_response = False

                last_response = r
                yield r

            if last_response:
                span.set_attribute(
                    "prompt_tokens_used", last_response.prompt_tokens_used
                )
                span.set_attribute(
                    "completion_tokens_used", last_response.completion_tokens_used
                )
                span.set_attribute(
                    "cached_tokens_used", last_response.cached_tokens_used
                )
                span.set_attribute("model_name", self.model_name)
                span.set_attribute("stop_reason", last_response.stop_reason or "None")
                if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                    span.set_attribute(
                        "input",
                        json.dumps([b.to_dict() for b in input]) if input else "",
                    )
                    span.set_attribute("output", last_response.text)
                    span.set_attribute(
                        "memory", memory.json_dumps() if memory else "None"
                    )

    async def a_stream_invoke(
        self,
        input: str | list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        """
        Streams the model's response token by token asynchronously.

        Args:
            input (str): The input text/prompt to send to the model
            tools (List[Tool], optional): List of tools available for the model to use. Defaults to [].
            memory (Memory, optional): Memory object containing conversation history. Defaults to None.
            tool_choice (str, optional): Controls which tool to use. Defaults to "auto".
            temperature (float, optional): Controls randomness in responses. Defaults to None.
            max_tokens (int, optional): Maximum number of tokens in the response. Defaults to None.
            system_prompt (str, optional): System-level instructions for the model. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the model's inference method

        Returns:
            An async iterator yielding ClientResponse objects containing the model's response
        """
        if isinstance(input, str) and input:
            input = [TextBlock(content=input)]
        elif input == "":
            input = []

        with generation_span("client.a_stream_invoke") as span:
            if tools is None:
                tools = []
            log.debug(f"Streaming invoke model {self.__class__.__name__}")
            first_response = True
            last_response = None

            if (
                input
                and os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true"
            ):
                span.set_attribute(
                    "input",
                    json.dumps([b.to_dict() for b in input]) if input else "",
                )

            response_stream = self._a_stream_invoke(
                input,
                tools,
                memory,
                tool_choice,
                temperature or self.temperature,
                max_tokens,
                system_prompt or self.system_prompt,
                **kwargs,
            )
            async for r in response_stream:  # type: ignore
                if first_response:
                    span.set_attribute("time_at_first_token", int(time.time() * 1000))
                    first_response = False

                last_response = r
                yield r

            if last_response:
                span.set_attribute(
                    "prompt_tokens_used", last_response.prompt_tokens_used
                )
                span.set_attribute(
                    "completion_tokens_used", last_response.completion_tokens_used
                )
                span.set_attribute(
                    "cached_tokens_used", last_response.cached_tokens_used
                )
                span.set_attribute("model_name", self.model_name)
                span.set_attribute("stop_reason", last_response.stop_reason or "None")

                if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                    span.set_attribute("output", last_response.text)
                    span.set_attribute(
                        "memory", memory.json_dumps() if memory else "None"
                    )

    @cacheable(_get_cache_key)
    def structured_response(
        self,
        *,
        input: str | list[Block],
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        """
        Structures the model's response according to a specified output class.

        Args:
            input (str): The input text/prompt to send to the model
            output_cls (Type[Model]): The class type to structure the response into
            memory (Memory, optional): Memory object containing conversation history. Defaults to None.
            temperature (float, optional): Controls randomness in responses. Defaults to None.
            max_tokens (int, optional): Maximum number of tokens in the response. Defaults to None.
            system_prompt (str, optional): System-level instructions for the model. Defaults to None.
            tools (List[Tool], optional): List of tools available for the model to use. Defaults to [].
            tool_choice (Literal["auto", "required", "none"] | list[str], optional): Controls which tool to use ("auto" by default). Defaults to "auto".
            **kwargs: Additional keyword arguments to pass to the model's inference method

        Returns:
            A ClientResponse object containing the structured response
        """
        if output_cls != {"type": "json_object"} and not issubclass(
            output_cls, BaseModel
        ):
            raise ValueError(
                "output_cls must be a subclass of Model or must be {'type': 'json_object'}"
            )

        if isinstance(input, str) and input:
            input = [TextBlock(content=input)]
        elif input == "":
            input = []

        with generation_span("client.structured_response") as span:
            log.debug(
                f"Structured response model {self.__class__.__name__} with input: {input}, memory: {memory}"
            )

            response = self._structured_response(
                input,
                output_cls,
                memory,
                temperature or self.temperature,
                max_tokens,
                system_prompt or self.system_prompt,
                tools,
                tool_choice,
                **kwargs,
            )

            span.set_attribute("prompt_tokens_used", response.prompt_tokens_used)
            span.set_attribute(
                "completion_tokens_used", response.completion_tokens_used
            )
            span.set_attribute("cached_tokens_used", response.cached_tokens_used)
            span.set_attribute("model_name", self.model_name)
            span.set_attribute("stop_reason", response.stop_reason or "None")
            if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                span.set_attribute(
                    "input", json.dumps([b.to_dict() for b in input]) if input else ""
                )
                span.set_attribute("output", str(response.structured_data[0]))
                span.set_attribute("memory", memory.json_dumps() if memory else "None")

        assert isinstance(response, ClientResponse)
        return response

    @cacheable(_get_cache_key)
    async def a_structured_response(
        self,
        *,
        input: str | list[Block],
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        """
        Structures the model's response according to a specified output class.

        Args:
            input (str): The input text/prompt to send to the model
            output_cls (Type[Model]): The class type to structure the response into
            memory (Memory, optional): Memory object containing conversation history. Defaults to None.
            temperature (float, optional): Controls randomness in responses. Defaults to None.
            max_tokens (int, optional): Maximum number of tokens in the response. Defaults to None.
            system_prompt (str, optional): System-level instructions for the model. Defaults to None.
            tools (List[Tool], optional): List of tools available for the model to use. Defaults to [].
            tool_choice (Literal["auto", "required", "none"] | list[str], optional): Controls which tool to use ("auto" by default). Defaults to "auto".
            **kwargs: Additional keyword arguments to pass to the model's inference method

        Returns:
            A ClientResponse object containing the structured response
        """
        if isinstance(input, str) and input:
            input = [TextBlock(content=input)]
        elif input == "":
            input = []

        with generation_span("client.a_structured_response") as span:
            if output_cls != {"type": "json_object"} and not issubclass(
                output_cls, BaseModel
            ):
                raise ValueError(
                    "output_cls must be a subclass of Model or must be {'type': 'json_object'}"
                )

            log.debug(
                f"Structured response model {self.__class__.__name__} with input: {input}, memory: {memory}"
            )

            response = await self._a_structured_response(
                input,
                output_cls,
                memory,
                temperature or self.temperature,
                max_tokens,
                system_prompt or self.system_prompt,
                tools,
                tool_choice,
                **kwargs,
            )
            span.set_attribute("prompt_tokens_used", response.prompt_tokens_used)
            span.set_attribute(
                "completion_tokens_used", response.completion_tokens_used
            )
            span.set_attribute("cached_tokens_used", response.cached_tokens_used)
            span.set_attribute("model_name", self.model_name)
            span.set_attribute("stop_reason", response.stop_reason or "None")
            if os.getenv("DATAPIZZA_TRACE_CLIENT_IO", "false").lower() == "true":
                span.set_attribute(
                    "input", json.dumps([b.to_dict() for b in input]) if input else ""
                )
                span.set_attribute("output", str(response.structured_data[0]))
                span.set_attribute("memory", memory.json_dumps() if memory else "None")

        assert isinstance(response, ClientResponse)
        return response

    @abstractmethod
    def _invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> ClientResponse:
        """
        Internal method to implement the actual inference call.
        Must be implemented by concrete classes.
        """

    @abstractmethod
    async def _a_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> ClientResponse:
        pass

    @abstractmethod
    async def _a_structured_response(
        self,
        input: list[Block] | None,
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        pass

    @abstractmethod
    def _stream_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> Iterator[ClientResponse]:
        """
        Internal method to implement the streaming inference call.
        Must be implemented by concrete classes.
        """
        pass

    @abstractmethod
    async def _a_stream_invoke(
        self,
        input: list[Block],
        tools: list[Tool] | None = None,
        memory: Memory | None = None,
        tool_choice: str = "auto",
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        **kwargs,
    ) -> AsyncIterator[ClientResponse]:
        """
        Internal method to implement the async streaming inference call.
        Must be implemented by concrete classes.
        """
        pass

    @abstractmethod
    def _structured_response(
        self,
        input: list[Block],
        output_cls: type[Model],
        memory: Memory | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        system_prompt: str | None = None,
        tools: list[Tool] | None = None,
        tool_choice: Literal["auto", "required", "none"] | list[str] = "auto",
        **kwargs,
    ) -> ClientResponse:
        """
        Internal method to implement the structured response call.
        Must be implemented by concrete classes.
        """
        pass

    def _memory_to_contents(
        self, system_prompt: str | None, input: str, memory: Memory | None = None
    ) -> list[dict]:
        """Convert memory and input into GenAI content format"""
        contents = []
        contents.extend(
            self.memory_adapter.memory_to_messages(memory, system_prompt, input)
        )

        return contents

    @cacheable(
        lambda self, args: "|".join(args.get("text"))
        + "|"
        + args.get("model_name", self.model_name)
        + "|embed"
    )
    def embed(
        self, text: str | list[str], model_name: str | None = None, **kwargs
    ) -> list[float]:
        """Embed a text using the model

        Args:
            text (str | list[str]): The text to embed
            model_name (str, optional): The name of the model to use. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the model's embedding method

        Returns:
            list[float]: The embedding vector for the text
        """
        return self._embed(text, model_name, **kwargs)

    async def a_embed(
        self, text: str | list[str], model_name: str | None = None, **kwargs
    ) -> list[float] | list[list[float]]:
        """Embed a text using the model

        Args:
            text (str | list[str]): The text to embed
            model_name (str, optional): The name of the model to use. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the model's embedding method

        Returns:
            list[float]: The embedding vector for the text
        """
        return await self._a_embed(text, model_name, **kwargs)

    def _embed(
        self, text: str | list[str], model_name: str | None = None, **kwargs
    ) -> list[float]:
        """Embed a text using the model"""
        raise NotImplementedError("Embedding is not implemented for this client")

    async def _a_embed(
        self, text: str | list[str], model_name: str | None = None, **kwargs
    ) -> list[float] | list[list[float]]:
        """Embed a text using the model"""
        raise NotImplementedError("async Embedding is not implemented for this client")

    @abstractmethod
    def _convert_tool_choice(self, tool_choice: str | list[str]) -> dict:
        """Convert tool choice to the client's format"""
        pass

    def _as_module_component(self):
        return InferenceClientModule(self)

    def as_inference_module_component(self):
        return self._as_module_component()

    def as_stream_module_component(self):
        return StreamInferenceClientModule(self)

    def as_structured_response_module_component(self):
        return StructuredResponseInferenceClientModule(self)


class InferenceClientModule(PipelineComponent):
    def __init__(self, client: Client):
        self.client = client

    def _run(self, **kwargs):
        return self.client.invoke(**kwargs)

    async def _a_run(self, **kwargs):
        return await self.client.a_invoke(**kwargs)


class StructuredResponseInferenceClientModule(PipelineComponent):
    def __init__(self, client: Client):
        self.client = client

    def _run(self, **kwargs):
        return self.client.structured_response(**kwargs)

    async def _a_run(self, **kwargs):
        return self.client.a_structured_response(**kwargs)


class StreamInferenceClientModule(PipelineComponent):
    def __init__(self, client: Client):
        self.client = client

    def _run(self, **kwargs):
        return self.client.stream_invoke(**kwargs)

    async def _a_run(self, **kwargs):
        return self.client.a_stream_invoke(**kwargs)



================================================
FILE: datapizza-ai-core/datapizza/core/clients/models.py
================================================
import warnings

from pydantic import BaseModel, Field

from datapizza.type import (
    Block,
    FunctionCallBlock,
    StructuredBlock,
    TextBlock,
    ThoughtBlock,
)


class TokenUsage(BaseModel):
    prompt_tokens: int = Field(default=0)
    completion_tokens: int = Field(default=0)
    cached_tokens: int = Field(default=0)
    thinking_tokens: int = Field(default=0)

    def __add__(self, other: "TokenUsage") -> "TokenUsage":
        return TokenUsage(
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            completion_tokens=self.completion_tokens + other.completion_tokens,
            cached_tokens=self.cached_tokens + other.cached_tokens,
            thinking_tokens=self.thinking_tokens + other.thinking_tokens,
        )


class ClientResponse:
    """
    A class for storing the response from a client.
    Contains a list of blocks that can be text, function calls, or structured data,
    maintaining the order in which they were generated.

    Args:
        content (List[Block]): A list of blocks.
        delta (str, optional): The delta of the response. Used for streaming responses.
        usage (TokenUsage, optional): Aggregated token usage.
        stop_reason (str, optional): Stop reason.

    """

    def __init__(
        self,
        *,
        content: list[Block],
        delta: str | None = None,
        stop_reason: str | None = None,
        usage: TokenUsage | None = None,
        # Deprecated per-field args for backward compatibility:
        prompt_tokens_used: int | None = None,
        completion_tokens_used: int | None = None,
        cached_tokens_used: int | None = None,
        thinking_tokens_used: int | None = None,
    ):
        self.content = content
        self.delta = delta
        self.stop_reason = stop_reason

        if any(
            v is not None
            for v in (
                prompt_tokens_used,
                completion_tokens_used,
                cached_tokens_used,
                thinking_tokens_used,
            )
        ):
            warnings.warn(
                "ClientResponse: per-field token args are deprecated and ignored when `usage` is provided. "
                "Pass a TokenUsage via `usage`.",
                DeprecationWarning,
                stacklevel=2,
            )

        self.usage = usage or TokenUsage(
            prompt_tokens=prompt_tokens_used or 0,
            completion_tokens=completion_tokens_used or 0,
            cached_tokens=cached_tokens_used or 0,
            thinking_tokens=thinking_tokens_used or 0,
        )

    def __eq__(self, other):
        return isinstance(other, ClientResponse) and self.content == other.content

    @property
    def prompt_tokens_used(self) -> int:
        return self.usage.prompt_tokens

    @property
    def completion_tokens_used(self) -> int:
        return self.usage.completion_tokens

    @property
    def cached_tokens_used(self) -> int:
        return self.usage.cached_tokens

    @property
    def thinking_tokens_used(self) -> int:
        return self.usage.thinking_tokens

    @property
    def text(self) -> str:
        """Returns concatenated text from all TextBlocks in order"""
        return "\n".join(
            block.content for block in self.content if isinstance(block, TextBlock)
        )

    @property
    def thoughts(self) -> str:
        """Returns all thoughts in order"""
        return "\n".join(
            block.content for block in self.content if isinstance(block, ThoughtBlock)
        )

    @property
    def first_text(self) -> str | None:
        """Returns the content of the first TextBlock or None"""
        text_block = next(
            (item for item in self.content if isinstance(item, TextBlock)), None
        )
        return text_block.content if text_block else None

    @property
    def function_calls(self) -> list[FunctionCallBlock]:
        """Returns all function calls in order"""
        return [item for item in self.content if isinstance(item, FunctionCallBlock)]

    @property
    def structured_data(self) -> list[BaseModel]:
        """Returns all structured data in order"""
        return [
            item.content for item in self.content if isinstance(item, StructuredBlock)
        ]

    def is_pure_text(self) -> bool:
        """Returns True if response contains only TextBlocks"""
        return all(isinstance(block, TextBlock) for block in self.content)

    def is_pure_function_call(self) -> bool:
        """Returns True if response contains only FunctionCallBlocks"""
        return all(isinstance(block, FunctionCallBlock) for block in self.content)

    def __str__(self) -> str:
        return f"ClientResponse(content={self.content}, delta={self.delta}, stop_reason={self.stop_reason})"



================================================
FILE: datapizza-ai-core/datapizza/core/clients/tests/test_mock_client.py
================================================
import pytest
from pydantic import BaseModel

from datapizza.clients.mock_client import MockClient
from datapizza.core.clients import Client, ClientResponse
from datapizza.memory.memory import Memory
from datapizza.type import TextBlock
from datapizza.type.type import ROLE


def test_cannot_instantiate_abstract_class():
    with pytest.raises(TypeError):
        Client("a", "b", 0.5)


def test_mock_client():
    llm = MockClient(model_name="mock_client")

    response = ClientResponse(content=[TextBlock(content="Mock response")])

    assert llm.invoke(input="Mock response") == response


def test_stream_response():
    llm = MockClient(model_name="mock_client")
    text = "Here a long text to return"
    response = llm.stream_invoke(input=text)

    final_response = ""
    for i, res in enumerate(response):
        final_response += res.delta
        assert res.delta == text[i]
        assert res.text == text[: i + 1]

    assert final_response == text
    assert res.text == text


def test_memory():
    memory = Memory()
    textBlock = TextBlock(content="Mock response")
    textBlock2 = TextBlock(content="Mock response 2")
    memory.add_turn(textBlock, role=ROLE.ASSISTANT)
    assert memory[0].blocks == [textBlock]

    memory.clear()
    assert len(memory) == 0

    memory.add_turn([textBlock], role=ROLE.ASSISTANT)
    assert memory[0].blocks == [textBlock]

    memory.clear()
    assert len(memory) == 0

    memory.new_turn()
    assert len(memory) == 1

    memory.add_to_last_turn(textBlock)
    assert memory[0].blocks == [textBlock]

    memory.add_to_last_turn(textBlock2)
    assert memory[0].blocks == [textBlock, textBlock2]

    assert len(memory) == 1


def test_client_with_memory():
    text1 = "Mock response"
    text2 = "secondresponse"

    # Creazione Memory
    memory = Memory()

    # Creazione client
    llm = MockClient(model_name="mock_client")
    response = llm.invoke(input=text1, memory=memory)

    assert response.content == [TextBlock(content=text1)]

    assert response.text == text1

    memory.add_turn(response.content, role=ROLE.ASSISTANT)

    response = llm.invoke(input=text2, memory=memory)
    assert response.text == text1 + text2


def test_response_format():
    class TestModel(BaseModel):
        test: int

    input = '{"test": 1}'

    llm = MockClient(model_name="mock_client")
    response = llm.structured_response(input=input, output_cls=TestModel)

    # assert isinstance(response, TestModel)
    # assert response == TestModel(test=1)

    assert response.structured_data == [TestModel(test=1)]


def test_response_first_text():
    response = ClientResponse(
        content=[TextBlock(content="string1"), TextBlock("text2")]
    )
    assert response.first_text == "string1"


def test_client_module():
    llm = MockClient(model_name="mock_client")
    llm_module = llm.as_module_component()
    response = llm_module._run(input="Mock response")
    assert isinstance(response, ClientResponse)
    assert response.content == [TextBlock(content="Mock response")]

    memory = Memory()
    memory.add_turn(TextBlock(content="old_message"), role=ROLE.ASSISTANT)
    response_with_memory = llm_module._run(input="Mock response", memory=memory)
    assert "old_message" in response_with_memory.text
    assert "Mock response" in response_with_memory.text



================================================
FILE: datapizza-ai-core/datapizza/core/clients/tests/test_token_usage.py
================================================
from datapizza.core.clients.models import TokenUsage


def test_token_usage_model():
    token_usage = TokenUsage(
        prompt_tokens=100,
        completion_tokens=200,
        cached_tokens=300,
        thinking_tokens=400,
    )
    assert token_usage.prompt_tokens == 100
    assert token_usage.completion_tokens == 200
    assert token_usage.cached_tokens == 300
    assert token_usage.thinking_tokens == 400


def test_sum_token_usage():
    token_usage1 = TokenUsage(
        prompt_tokens=100,
        completion_tokens=200,
        cached_tokens=300,
        thinking_tokens=400,
    )
    token_usage2 = TokenUsage(
        prompt_tokens=50,
        completion_tokens=100,
        cached_tokens=150,
        thinking_tokens=200,
    )
    sum = token_usage1 + token_usage2
    assert sum.prompt_tokens == 150
    assert sum.completion_tokens == 300
    assert sum.cached_tokens == 450
    assert sum.thinking_tokens == 600



================================================
FILE: datapizza-ai-core/datapizza/core/embedder/__init__.py
================================================
from .base import BaseEmbedder

__all__ = ["BaseEmbedder"]



================================================
FILE: datapizza-ai-core/datapizza/core/embedder/base.py
================================================
from abc import abstractmethod
from typing import Generic, TypeVar

from datapizza.core.models import PipelineComponent

T = TypeVar("T")


class BaseEmbedder(PipelineComponent, Generic[T]):
    client: object
    a_client: object

    def _get_client(self):
        if not self.client:
            self._set_client()
        return self.client

    def _get_a_client(self):
        if not self.a_client:
            self._set_a_client()
        return self.a_client

    def _set_client(self):
        raise NotImplementedError("This method should be implemented by the subclass")

    def _set_a_client(self):
        raise NotImplementedError("This method should be implemented by the subclass")

    def __init__(self, model_name: str):
        self.model_name = model_name

    def _run(self, text: str):
        return self.embed(text)

    async def _a_run(self, text: str):
        return await self.a_embed(text)

    @abstractmethod
    def embed(self, text: str | list[str], **kwargs) -> T | list[T]:
        pass

    async def a_embed(self, text: str | list[str], **kwargs):
        raise NotImplementedError



================================================
FILE: datapizza-ai-core/datapizza/core/executors/async_executor.py
================================================
import asyncio
import concurrent.futures
import threading
from collections.abc import Coroutine
from contextlib import suppress
from typing import Any


class AsyncExecutor:
    """Thread-safe event loop executor for running async code from sync contexts."""

    _singleton_instance: "AsyncExecutor | None" = None
    _singleton_lock = threading.Lock()

    @classmethod
    def get_instance(cls) -> "AsyncExecutor":
        """Get or create the global singleton executor instance."""
        with cls._singleton_lock:
            if cls._singleton_instance is None or cls._singleton_instance._closed:
                cls._singleton_instance = cls()
            return cls._singleton_instance

    def __init__(self):
        """Initialize a dedicated event loop."""
        self._runner: asyncio.Runner | None = None
        self._loop: asyncio.AbstractEventLoop | None = None
        self._thread = threading.Thread(target=self._run_loop, daemon=True)
        self._started = threading.Event()
        self._closed = False
        self._thread.start()
        if not self._started.wait(timeout=5):
            raise RuntimeError("AsyncExecutor failed to start background event loop")

    def _run_loop(self):
        """Run the event loop inside a Runner context."""
        try:
            with asyncio.Runner() as runner:
                self._runner = runner
                self._loop = runner.get_loop()
                self._started.set()
                self._loop.run_forever()
        finally:
            self._closed = True
            self._runner = None
            self._loop = None
            self._started.set()

    def run(self, coro: Coroutine[Any, Any, Any], timeout: float | None = None) -> Any:
        """
        Run a coroutine in the event loop.

        :param coro: Coroutine to execute
        :param timeout: Optional timeout in seconds
        :return: Result of the coroutine
        :raises TimeoutError: If execution exceeds timeout
        """
        loop = self._loop
        if loop is None or self._closed:
            raise RuntimeError("AsyncExecutor event loop is not running")

        future = asyncio.run_coroutine_threadsafe(coro, loop)
        try:
            return future.result(timeout)
        except concurrent.futures.TimeoutError as exc:
            future.cancel()
            with suppress(concurrent.futures.CancelledError):
                future.result(timeout=0)
            raise TimeoutError(f"Operation timed out after {timeout} seconds") from exc



================================================
FILE: datapizza-ai-core/datapizza/core/modules/captioner.py
================================================
from abc import abstractmethod

from datapizza.core.models import PipelineComponent
from datapizza.type import Node


class NodeCaptioner(PipelineComponent):
    """
    A captioner that can caption a node.
    """

    @abstractmethod
    def caption(self, node: Node):
        """
        Caption a node.
        """
        pass

    async def a_caption(self, node: Node):
        """
        async Caption a node.
        """
        raise NotImplementedError

    def _run(self, node: Node):
        return self.caption(node)

    async def _a_run(self, node: Node):
        return await self.a_caption(node)


class Captioner(PipelineComponent):
    """
    A captioner that can caption a node.
    """

    @abstractmethod
    def caption(self, node: Node):
        """
        Caption a node.

        Args:
            node: The node to caption.

        Returns:
            The same node with the caption.
        """
        pass

    async def a_caption(self, node: Node):
        raise NotImplementedError

    def _run(self, node: Node):
        return self.caption(node)

    async def _a_run(self, node: Node):
        return await self.a_caption(node)



================================================
FILE: datapizza-ai-core/datapizza/core/modules/metatagger.py
================================================
from abc import abstractmethod

from datapizza.core.clients import Client
from datapizza.core.models import PipelineComponent
from datapizza.type.type import Chunk


class Metatagger(PipelineComponent):
    """
    A meta tagger that can tag a node.
    """

    def __init__(self, client: Client):
        self.client = client

    @abstractmethod
    def tag(self, chunks: list[Chunk]):
        pass

    def a_tag(self, chunks: list[Chunk]):
        raise NotImplementedError

    def _run(self, chunks: list[Chunk]):
        return self.tag(chunks)

    async def _a_run(self, chunk: list[Chunk]):
        return await self.a_tag(chunk)



================================================
FILE: datapizza-ai-core/datapizza/core/modules/parser.py
================================================
from abc import abstractmethod

from datapizza.core.models import PipelineComponent
from datapizza.type import Node


class Parser(PipelineComponent):
    """
    A parser is a pipeline component that converts a document into a structured hierarchical Node representation.
    """

    def __init__(self):
        super().__init__()

    @abstractmethod
    def parse(self, text: str, metadata: dict | None = None) -> Node:
        pass

    async def a_parse(self, text: str, metadata: dict | None = None) -> Node:
        raise NotImplementedError

    def _run(self, text: str, metadata: dict | None = None) -> Node:
        return self.parse(text, metadata)

    async def _a_run(self, text: str, metadata: dict | None = None) -> Node:
        return await self.a_parse(text, metadata)



================================================
FILE: datapizza-ai-core/datapizza/core/modules/prompt.py
================================================
from abc import abstractmethod

from datapizza.core.models import PipelineComponent
from datapizza.memory import Memory


class Prompt(PipelineComponent):
    """
    A class for creating prompts for RAG systems.
    """

    @abstractmethod
    def format(self, **kwargs) -> Memory:
        pass

    def _run(self, **kwargs) -> Memory:
        return self.format(**kwargs)

    async def _a_run(self, **kwargs) -> Memory:
        return self.format(**kwargs)



================================================
FILE: datapizza-ai-core/datapizza/core/modules/reranker.py
================================================
from abc import abstractmethod

from datapizza.core.models import PipelineComponent
from datapizza.type import Chunk


class Reranker(PipelineComponent):
    """
    A class for reranking documents.
    """

    @abstractmethod
    def rerank(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        pass

    async def a_rerank(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        raise NotImplementedError

    def _run(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        return self.rerank(query=query, documents=documents)

    async def _a_run(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        return await self.a_rerank(query=query, documents=documents)



================================================
FILE: datapizza-ai-core/datapizza/core/modules/rewriter.py
================================================
from abc import abstractmethod

from datapizza.core.models import PipelineComponent
from datapizza.memory import Memory


class Rewriter(PipelineComponent):
    """
    A rewriter that uses language models to transform text content with specific instructions and tools.
    """

    @abstractmethod
    def rewrite(self, user_prompt: str, memory: Memory | None = None) -> str:
        pass

    async def a_rewrite(self, user_prompt: str, memory: Memory | None = None) -> str:
        raise NotImplementedError

    def _run(self, user_prompt: str, memory: Memory | None = None, **kwargs) -> str:
        return self.rewrite(user_prompt, memory, **kwargs)

    async def _a_run(
        self, user_prompt: str, memory: Memory | None = None, **kwargs
    ) -> str:
        return await self.a_rewrite(user_prompt, memory, **kwargs)



================================================
FILE: datapizza-ai-core/datapizza/core/modules/splitter.py
================================================
from abc import abstractmethod
from typing import overload

from datapizza.core.models import PipelineComponent
from datapizza.type.type import Chunk, Node


class Splitter(PipelineComponent):
    def _run(self, node: Node) -> list[Chunk]:
        return self.split(node)

    async def _a_run(self, node: Node) -> list[Chunk]:
        return await self.a_split(node)

    @abstractmethod
    @overload
    def split(self, text: str) -> list[Chunk]:
        pass

    @abstractmethod
    @overload
    def split(self, node: Node) -> list[Chunk]:
        pass

    async def a_split(self, text: str | Node) -> list[Chunk]:
        raise NotImplementedError(
            f"a_split is not implemented in {self.__class__.__name__} "
        )



================================================
FILE: datapizza-ai-core/datapizza/core/vectorstore/__init__.py
================================================
from .vectorstore import Distance, Retriever, VectorConfig, Vectorstore

__all__ = ["Distance", "Retriever", "VectorConfig", "Vectorstore"]



================================================
FILE: datapizza-ai-core/datapizza/core/vectorstore/vectorstore.py
================================================
import logging
from abc import abstractmethod
from enum import Enum

from pydantic import BaseModel

from datapizza.core.models import ChainableProducer, PipelineComponent
from datapizza.type import Chunk, EmbeddingFormat

log = logging.getLogger(__name__)


class Distance(Enum):
    COSINE = "Cosine"
    EUCLIDEAN = "Euclidean"


class VectorConfig(BaseModel):
    name: str
    format: EmbeddingFormat = EmbeddingFormat.DENSE
    dimensions: int | None = None
    distance: Distance = Distance.COSINE

    def model_post_init(self, __context) -> None:
        if self.format == EmbeddingFormat.DENSE and self.dimensions is None:
            raise ValueError("Dimensions must be specified for dense embeddings")


class Vectorstore(ChainableProducer):
    """
    A class that can produce a vectorstore.
    If a Vectorstore is used as a node in a pipeline, it will produce a retriever.
    """

    @abstractmethod
    def add(self, chunk: Chunk | list[Chunk], collection_name: str | None = None):
        pass

    @abstractmethod
    async def a_add(
        self, chunk: Chunk | list[Chunk], collection_name: str | None = None
    ):
        pass

    @abstractmethod
    def update(self, collection_name: str, payload: dict, points: list[int], **kwargs):
        pass

    @abstractmethod
    def remove(self, collection_name: str, ids: list[str], **kwargs):
        pass

    @abstractmethod
    def search(
        self,
        collection_name: str,
        query_vector: list[float],
        k: int = 10,
        vector_name: str | None = None,
        **kwargs,
    ) -> list[Chunk]:
        pass

    @abstractmethod
    async def a_search(
        self,
        collection_name: str,
        query_vector: list[float],
        k: int = 10,
        vector_name: str | None = None,
        **kwargs,
    ) -> list[Chunk]:
        pass

    @abstractmethod
    def retrieve(self, collection_name: str, ids: list[str], **kwargs) -> list[Chunk]:
        pass

    def as_retriever(self, **kwargs):
        return Retriever(self, **kwargs)

    def _as_module_component(self, **kwargs):
        return self.as_retriever(**kwargs)


class Retriever(PipelineComponent):
    def __init__(self, vectorstore: Vectorstore, **kwargs):
        self.vectorstore: Vectorstore = vectorstore
        self.kwargs = kwargs

    def _run(
        self, collection_name: str, query_vector: list[float], k: int = 10, **kwargs
    ):
        return self.vectorstore.search(collection_name, query_vector, k, **kwargs)

    async def _a_run(
        self, collection_name: str, query_vector: list[float], k: int = 10, **kwargs
    ):
        return await self.vectorstore.a_search(
            collection_name=collection_name, query_vector=query_vector, k=k, **kwargs
        )



================================================
FILE: datapizza-ai-core/datapizza/core/vectorstore/tests/test_vectorstore_models.py
================================================
import pytest

from datapizza.core.vectorstore import VectorConfig
from datapizza.type import EmbeddingFormat


def test_vector_config_model_post_init():
    with pytest.raises(ValueError):
        VectorConfig(name="dense_emb_name", format=EmbeddingFormat.DENSE)



================================================
FILE: datapizza-ai-core/datapizza/embedders/__init__.py
================================================
from pkgutil import extend_path

__path__ = extend_path(__path__, __name__)

from .embedders import ChunkEmbedder, ClientEmbedder

__all__ = ["ChunkEmbedder", "ClientEmbedder"]



================================================
FILE: datapizza-ai-core/datapizza/embedders/embedders.py
================================================
import logging
from collections.abc import Generator

from datapizza.core.clients import Client
from datapizza.core.embedder import BaseEmbedder
from datapizza.core.models import PipelineComponent
from datapizza.type import Chunk, DenseEmbedding, SparseEmbedding

log = logging.getLogger(__name__)


class ClientEmbedder(BaseEmbedder):
    """Client embedder using any compatible LLM client with embedding capabilities."""

    def __init__(
        self,
        client: Client,
        model_name: str | None = None,
        embedding_name: str | None = None,
    ):
        self.client = client
        self.model_name = model_name
        self.embedding_name = embedding_name or model_name or self.client.model_name

    def embed(self, text: str | list[str], **kwargs) -> list[float]:
        return self.client.embed(text, self.model_name, **kwargs)  # type: ignore

    async def a_embed(self, text: str | list[str], **kwargs) -> list[float]:
        return await self.client.a_embed(text, self.model_name, **kwargs)  # type: ignore


class ChunkEmbedder(PipelineComponent):
    """ChunkEmbedder is a module that given a list of chunks, it put a list of embeddings in each chunk."""

    def __init__(
        self,
        client: BaseEmbedder,
        model_name: str | None = None,
        embedding_name: str | None = None,
        batch_size: int = 2047,
    ):
        """
        Initialize the ChunkEmbedder.

        Args:
            client (BaseEmbedder): The client to use for embedding.
            model_name (str, optional): The model name to use for embedding. Defaults to None.
            embedding_name (str, optional): The name of the embedding to use. Defaults to None.
            batch_size (int, optional): The batch size to use for embedding. Defaults to 2047.
        """
        self.client = client
        self.model_name = model_name
        self.embedding_name = embedding_name
        self.batch_size = batch_size

    def _batch_nodes(
        self, nodes: list[Chunk], batch_size: int
    ) -> Generator[list[Chunk], None, None]:
        for i in range(0, len(nodes), batch_size):
            yield nodes[i : i + batch_size]

    def embed(self, nodes: list[Chunk]) -> list[Chunk]:
        """
        Embeds the given list of chunks.

        Args:
            nodes (list[Chunk]): The list of chunks to embed.

        Returns:
            list[Chunk]: The list of chunks with embeddings.
        """
        if not all(isinstance(n, Chunk) for n in nodes):
            raise ValueError("Nodes must be of type Chunk")

        for batch in self._batch_nodes(nodes, batch_size=self.batch_size):
            embeddings = self.client.embed([n.text for n in batch], self.model_name)

            for n, embedding in zip(batch, embeddings, strict=False):
                if isinstance(embedding, SparseEmbedding):
                    embedding.name = self.embedding_name
                    n.embeddings.append(embedding)
                else:
                    n.embeddings.append(
                        DenseEmbedding(name=self.embedding_name, vector=embedding)  # type: ignore
                    )

        return nodes

    async def a_embed(self, nodes: list[Chunk]) -> list[Chunk]:
        """
        Asynchronously embeds the given list of chunks.

        Args:
            nodes (list[Chunk]): The list of chunks to embed.

        Returns:
            list[Chunk]: The list of chunks with embeddings.
        """
        if not all(isinstance(n, Chunk) for n in nodes):
            raise ValueError("Nodes must be of type Chunk")

        for batch in self._batch_nodes(nodes, batch_size=self.batch_size):
            embeddings = await self.client.a_embed(
                [n.text for n in batch], self.model_name
            )

            for n, embedding in zip(batch, embeddings, strict=False):
                if isinstance(embedding, SparseEmbedding):
                    n.embeddings.append(embedding)
                else:
                    n.embeddings.append(
                        DenseEmbedding(name=self.embedding_name, vector=embedding)  # type: ignore
                    )

        return nodes

    def _run(self, nodes: list[Chunk]) -> list[Chunk]:
        return self.embed(nodes)

    async def _a_run(self, nodes: list[Chunk]) -> list[Chunk]:
        return await self.a_embed(nodes)



================================================
FILE: datapizza-ai-core/datapizza/memory/__init__.py
================================================
from .memory import Memory, Turn
from .memory_adapter import MemoryAdapter

__all__ = [
    "Memory",
    "MemoryAdapter",
    "Turn",
]



================================================
FILE: datapizza-ai-core/datapizza/memory/__version__.py
================================================
VERSION = (3, 0, 8)

__version__ = ".".join(map(str, VERSION))



================================================
FILE: datapizza-ai-core/datapizza/memory/memory.py
================================================
import hashlib
import json

from datapizza.type import ROLE, Block, FunctionCallBlock


class Turn:
    def __init__(
        self,
        blocks: list[Block],
        role: ROLE = ROLE.ASSISTANT,
    ):
        if not isinstance(blocks, list):
            raise ValueError("blocks must be a list")
        if not all(isinstance(block, Block) for block in blocks):
            raise ValueError("all items in blocks must be Block instances")

        self.blocks = blocks
        self.role = role

    def __iter__(self):
        return iter(self.blocks)

    def __len__(self):
        return len(self.blocks)

    def __getitem__(self, index):
        return self.blocks[index]

    def __setitem__(self, index, value):
        self.blocks[index] = value

    def __delitem__(self, index):
        del self.blocks[index]

    def append(self, block: Block):
        self.blocks.append(block)

    def extend(self, blocks: list[Block]):
        self.blocks.extend(blocks)

    def insert(self, index, block: Block):
        self.blocks.insert(index, block)

    def to_dict(self):
        return {
            "role": self.role.value,
            "blocks": [block.to_dict() for block in self.blocks],
        }

    def __str__(self):
        return str(self.blocks)

    def __repr__(self):
        return f"Turn(blocks={self.blocks}, role={self.role})"


class Memory:
    """
    A class for storing the memory of a chat, organized by conversation turns.
    Each turn can contain multiple blocks (text, function calls, or structured data).
    """

    def __init__(self):
        # List of turns, where each turn is a list of blocks
        self.memory = []

    def new_turn(self, role: ROLE = ROLE.ASSISTANT):
        """Add a new conversation turn.

        Args:
            role (ROLE, optional): The role of the new turn. Defaults to ROLE.ASSISTANT.
        """
        self.memory.append(Turn([], role))

    def add_turn(
        self, blocks: list[Block] | list[FunctionCallBlock] | Block, role: ROLE
    ):
        """Add a new conversation turn containing one or more blocks.

        Args:
            blocks (list[Block] | Block): The blocks to add to the new turn.
            role (ROLE): The role of the new turn.
        """

        turn = Turn(blocks, role) if isinstance(blocks, list) else Turn([blocks], role)  # type: ignore

        self.memory.append(turn)

    def add_to_last_turn(self, block: Block):
        """Add a block to the most recent turn. Creates a new turn if memory is empty.
        Args:
            block (Block): The block to add to the most recent turn.
        """
        if not self.memory:
            self.memory.append(Turn([block], ROLE.ASSISTANT))
        else:
            self.memory[-1].append(block)

    def clear(self):
        """Clear all memory."""
        self.memory = []

    def __iter__(self):
        """Iterate through all blocks in all turns."""
        yield from self.memory

    def iter_blocks(self):
        """
        Iterate through blocks.
        """
        for turn in self.memory:
            yield from turn

    def copy(self):
        """Deep copy the memory."""
        from copy import deepcopy

        memory = Memory()
        memory.memory = deepcopy(self.memory)
        return memory

    def __len__(self):
        """Return the total number of turns."""
        return len(self.memory)

    def __getitem__(self, index):
        """Get all blocks from a specific turn."""
        return self.memory[index]

    def __setitem__(self, index, value):
        """Set blocks for a specific turn."""
        if isinstance(value, list):
            self.memory[index] = value
        else:
            self.memory[index] = [value]

    def __delitem__(self, index):
        """Delete a specific turn."""
        del self.memory[index]

    def __str__(self):
        """Return a string representation of the memory."""
        return str(self.memory)

    def __repr__(self):
        """Return a detailed string representation of the memory."""
        return f"Memory(turns={len(self)})"

    def __bool__(self):
        """Return True if memory contains any turns, False otherwise."""
        return bool(self.memory)

    def __eq__(self, other):
        """
        Compare two Memory objects based on their content hash.
        This is more efficient than comparing the full content structure.
        """
        if not isinstance(other, Memory):
            return False
        return hash(self) == hash(other)

    def __hash__(self):
        """
        Creates a deterministic hash based on the content of memory turns.
        """
        hash_components = []
        for turn in self.memory:
            turn_components = []
            for block in turn.blocks:
                turn_components.append(str(hash(block)))
            hash_components.append("|".join(turn_components))

        content_string = "||".join(hash_components)
        return int(hashlib.sha256(content_string.encode("utf-8")).hexdigest(), 16)

    def json_dumps(self) -> str:
        """Serialize the memory to JSON.

        Returns:
            str: The JSON representation of the memory.
        """
        return json.dumps(self.to_dict())

    def json_loads(self, json_str: str):
        """Deserialize JSON to the memory.

        Args:
            json_str (str): The JSON string to deserialize.
        """
        obj = json.loads(json_str)
        for t in obj:
            self.add_turn(
                blocks=[Block.from_dict(block) for block in t["blocks"]],
                role=ROLE(t["role"]),
            )

    def to_dict(self) -> list[dict]:
        """Convert memory to a dictionary.

        Returns:
            list[dict]: The dictionary representation of the memory.
        """
        return [turn.to_dict() for turn in self.memory]



================================================
FILE: datapizza-ai-core/datapizza/memory/memory_adapter.py
================================================
from abc import ABC, abstractmethod

from datapizza.memory import Memory, Turn
from datapizza.type import (
    ROLE,
    Block,
    FunctionCallBlock,
)


class MemoryAdapter(ABC):
    """
    A class for storing the memory of a chat, organized by conversation turns.
    Each turn can contain multiple blocks (text, function calls, or structured data).
    """

    def _text_to_message(self, text: str, role: ROLE) -> dict:
        raise NotImplementedError("Subclasses must implement _text_to_message method")

    def memory_to_messages(
        self,
        memory: Memory | None = None,
        system_prompt: str | None = None,
        input: str | list[Block] | Block | None = None,
    ) -> list[dict]:
        messages = []
        if system_prompt:
            messages.append(self._text_to_message(system_prompt, ROLE.SYSTEM))

        if memory:
            for turn in memory:
                if all(isinstance(block, FunctionCallBlock) for block in turn):
                    for block in turn:
                        messages.append(
                            self._turn_to_message(Turn([block], role=turn.role))
                        )
                else:
                    messages.append(self._turn_to_message(turn))

        if input:
            if isinstance(input, str):
                messages.append(self._text_to_message(input, ROLE.USER))
            elif isinstance(input, list):
                turn = Turn(input, role=ROLE.USER)
                messages.append(self._turn_to_message(turn))
            elif isinstance(input, Block):
                turn = Turn([input], role=ROLE.USER)
                messages.append(self._turn_to_message(turn))
            else:
                raise ValueError(f"Unsupported input type: {type(input)}")

        return messages

    @abstractmethod
    def _turn_to_message(self, turn: Turn) -> dict:
        pass



================================================
FILE: datapizza-ai-core/datapizza/memory/tests/__init__.py
================================================
# Initialize tests package



================================================
FILE: datapizza-ai-core/datapizza/memory/tests/test_memory.py
================================================
from typing import TypeVar

from pydantic import BaseModel

from datapizza.memory.memory import Memory
from datapizza.tools import tool
from datapizza.type import (
    ROLE,
    FunctionCallBlock,
    FunctionCallResultBlock,
    Media,
    MediaBlock,
    StructuredBlock,
    TextBlock,
)

Model = TypeVar("Model", bound=BaseModel)


def test_memory_json_dumps():
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.ASSISTANT)

    json = memory.json_dumps()
    assert (
        json
        == '[{"role": "user", "blocks": [{"type": "text", "content": "Hello, world!"}]}, {"role": "assistant", "blocks": [{"type": "text", "content": "Hello, world!"}]}]'
    )


def test_memory_to_dict():
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.ASSISTANT)

    assert memory.to_dict() == [
        {"role": "user", "blocks": [{"type": "text", "content": "Hello, world!"}]},
        {"role": "assistant", "blocks": [{"type": "text", "content": "Hello, world!"}]},
    ]


def test_to_dict_with_media_block():
    memory = Memory()
    memory.add_turn(
        blocks=[
            MediaBlock(
                media=Media(
                    media_type="image",
                    source_type="url",
                    source="https://example.com/image.png",
                    extension="png",
                )
            )
        ],
        role=ROLE.USER,
    )

    assert memory.to_dict() == [
        {
            "role": "user",
            "blocks": [
                {
                    "type": "media",
                    "media": {
                        "source": "https://example.com/image.png",
                        "extension": "png",
                        "media_type": "image",
                        "source_type": "url",
                        "detail": "high",
                    },
                }
            ],
        },
    ]


def test_to_dict_with_media_block_base64():
    memory = Memory()
    memory.add_turn(
        blocks=[
            MediaBlock(
                media=Media(
                    media_type="image",
                    source_type="base64",
                    source="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQAQMAAAAlPW0iAAAABlBMVEX///+/v7+jQ3Y5AAAACklEQVQI12NgAAAAAgAB4iG8MwAAAABJRU5ErkJggg==",
                    extension="png",
                )
            )
        ],
        role=ROLE.USER,
    )

    assert memory.to_dict() == [
        {
            "role": "user",
            "blocks": [
                {
                    "type": "media",
                    "media": {
                        "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQAQMAAAAlPW0iAAAABlBMVEX///+/v7+jQ3Y5AAAACklEQVQI12NgAAAAAgAB4iG8MwAAAABJRU5ErkJggg==",
                        "extension": "png",
                        "media_type": "image",
                        "source_type": "base64",
                        "detail": "high",
                    },
                }
            ],
        },
    ]


def test_memory_json_dumps_with_function_call():
    @tool
    def get_weather(city: str) -> str:
        """Get the weather for a city"""
        return f"The weather in {city} is sunny"

    memory = Memory()
    memory.add_turn(
        blocks=[
            FunctionCallBlock(
                id="1",
                arguments="{}",
                name="get_weather",
                tool=get_weather,
            )
        ],
        role=ROLE.USER,
    )

    assert memory.to_dict() == [
        {
            "role": "user",
            "blocks": [
                {
                    "type": "function",
                    "id": "1",
                    "arguments": "{}",
                    "name": "get_weather",
                    "tool": {
                        "name": "get_weather",
                        "description": "Get the weather for a city",
                        "properties": {
                            "city": {"type": "string", "description": "Parameter city"}
                        },
                        "required": ["city"],
                        "end_invoke": False,
                    },
                }
            ],
        }
    ]


def test_memory_json_dumps_with_function_call_result():
    @tool
    def get_weather(city: str) -> str:
        """Get the weather for a city"""
        return f"The weather in {city} is sunny"

    memory = Memory()

    memory.add_turn(
        blocks=[
            FunctionCallResultBlock(
                id="1",
                result="The weather in Rome is sunny",
                tool=get_weather,
            )
        ],
        role=ROLE.ASSISTANT,
    )

    assert memory.to_dict() == [
        {
            "role": "assistant",
            "blocks": [
                {
                    "type": "function_call_result",
                    "id": "1",
                    "tool": {
                        "name": "get_weather",
                        "description": "Get the weather for a city",
                        "properties": {
                            "city": {"type": "string", "description": "Parameter city"}
                        },
                        "required": ["city"],
                        "end_invoke": False,
                    },
                    "result": "The weather in Rome is sunny",
                }
            ],
        }
    ]


def test_structured_block():
    class Person(BaseModel):
        name: str
        age: int

    memory = Memory()
    memory.add_turn(
        blocks=[StructuredBlock(content=Person(name="John", age=30))],
        role=ROLE.ASSISTANT,
    )

    mem_dict = memory.to_dict()
    assert mem_dict == [
        {
            "role": "assistant",
            "blocks": [
                {
                    "type": "structured",
                    "content": '{"name":"John","age":30}',
                }
            ],
        }
    ]


def test_structured_json_block():
    memory = Memory()
    memory.add_turn(
        blocks=[StructuredBlock(content={"name": "John", "age": 30})],
        role=ROLE.ASSISTANT,
    )

    mem_dict = memory.to_dict()
    assert mem_dict == [
        {
            "role": "assistant",
            "blocks": [
                {
                    "type": "structured",
                    "content": {"name": "John", "age": 30},
                }
            ],
        }
    ]


def test_memory_copy():
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.ASSISTANT)

    memory_copy = memory.copy()
    assert memory_copy == memory

    memory_copy.clear()
    assert memory_copy == Memory()


def test_memory_deep_copy():
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.ASSISTANT)

    memory_copy = memory.copy()
    assert memory_copy == memory

    memory[0].blocks[0].content = "Hello, world! 2"
    assert memory_copy[0].blocks[0].content == "Hello, world!"

    memory[0].role = ROLE.ASSISTANT
    assert memory_copy[0].role == ROLE.USER


def test_empty_memory_json_loads():
    memory = Memory()

    json_str = memory.json_dumps()
    memory_copy = Memory()
    memory_copy.json_loads(json_str)
    assert memory_copy == memory


def test_memory_json_loadsas():
    memory = Memory()
    memory.add_turn(blocks=[TextBlock(content="Hello, world!")], role=ROLE.USER)

    json_str = memory.json_dumps()

    memory_copy = Memory()
    memory_copy.json_loads(json_str)
    assert memory_copy == memory



================================================
FILE: datapizza-ai-core/datapizza/modules/captioners/__init__.py
================================================
from .llm_captioner import LLMCaptioner

__all__ = ["LLMCaptioner"]



================================================
FILE: datapizza-ai-core/datapizza/modules/captioners/llm_captioner.py
================================================
import asyncio
from concurrent.futures import Future, ThreadPoolExecutor

from datapizza.core.clients import Client
from datapizza.core.modules.captioner import NodeCaptioner
from datapizza.type import Media, MediaBlock, MediaNode, Node, NodeType


class LLMCaptioner(NodeCaptioner):
    """
    Captioner that uses an LLM client to caption a node.
    """

    def __init__(
        self,
        client: Client,
        max_workers: int = 3,
        system_prompt_table: str | None = "Generate concise captions for tables.",
        system_prompt_figure: str | None = "Generate descriptive captions for figures.",
    ):
        """
        Captioner that uses an LLM client to caption a node.
        Args:
            client: The LLM client to use.
            max_workers: The maximum number of workers to use. in sync mode is the number of threads spawned, in async mode is the number of batches.
            system_prompt_table: The system prompt to use for table captioning.
            system_prompt_figure: The system prompt to use for figure captioning.
        """
        self.client = client
        self.max_workers = max_workers
        self.system_prompt_figure = system_prompt_figure
        self.system_prompt_table = system_prompt_table

    def _get_all_media_nodes(self, node: Node) -> list[MediaNode]:
        if isinstance(node, MediaNode):
            return [node]

        media_nodes = []
        for child in node.children:
            media_nodes.extend(self._get_all_media_nodes(child))
        return media_nodes

    def _process_media(self, media_node: MediaNode) -> tuple[MediaNode, str]:
        if media_node.node_type == NodeType.FIGURE:
            system_prompt = self.system_prompt_figure
        elif media_node.node_type == NodeType.TABLE:
            system_prompt = self.system_prompt_table
        else:
            raise ValueError(f"Unsupported node type: {media_node.node_type}")

        caption = self.caption_media(
            media=media_node.media, system_prompt=system_prompt
        )

        # Create a new MediaNode with the caption nodes
        new_children = [
            Node(
                node_type=NodeType.PARAGRAPH,
                content=f" {media_node.content} <{media_node.node_type.value}> [{caption}]",
                metadata=media_node.metadata,
            ),
            *media_node.children,
            Node(
                node_type=NodeType.PARAGRAPH,
                content=f"</{media_node.node_type.value}>",
                metadata=media_node.metadata,
            ),
        ]

        new_media_node = MediaNode(
            node_type=media_node.node_type,
            media=media_node.media,
            children=new_children,
            metadata=media_node.metadata,
        )

        return new_media_node, caption

    def _replace_media_nodes(
        self, node: Node, processed_nodes: list[MediaNode]
    ) -> Node:
        if isinstance(node, MediaNode):
            # Find matching processed node
            for processed_node in processed_nodes:
                if processed_node.media == node.media:
                    return processed_node
            return node

        # Create new node with processed children
        new_children = [
            self._replace_media_nodes(child, processed_nodes) for child in node.children
        ]
        return Node(
            node_type=node.node_type,
            content=node.content,
            children=new_children,
            metadata=node.metadata,
        )

    def caption(self, node: Node) -> Node:
        """
        Caption a node.
        Args:
            node: The node to caption.

        Returns:
            The same node with the caption.
        """
        media_nodes = self._get_all_media_nodes(node)

        # Process media nodes concurrently
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures: list[Future] = [
                executor.submit(self._process_media, media) for media in media_nodes
            ]

            # Wait for all futures to complete and collect results
            processed_nodes = []
            for future in futures:
                processed_node, _ = future.result()
                processed_nodes.append(processed_node)

        return self._replace_media_nodes(node, processed_nodes)

    def caption_media(self, media: Media, system_prompt: str | None = None) -> str:
        """
        Caption an image.
        Args:
            media: The media to caption.
            system_prompt: Optional system prompt to guide the captioning.

        Returns:
            The string caption.
        """
        response = self.client.invoke(
            input=[MediaBlock(media)], system_prompt=system_prompt
        )
        return response.text

    async def a_caption(self, node: Node) -> Node:
        """
        async Caption a node.
        Args:
            node: The node to caption.

        Returns:
            The same node with the caption.
        """
        media_nodes = self._get_all_media_nodes(node)

        # Process media nodes in batches based on max_workers
        processed_nodes = []
        for i in range(0, len(media_nodes), self.max_workers):
            batch = media_nodes[i : i + self.max_workers]

            # Process batch concurrently
            results = await asyncio.gather(
                *[self._a_process_media(media) for media in batch]
            )

            # Unpack results from this batch
            processed_nodes.extend([node for node, _ in results])

        return self._replace_media_nodes(node, processed_nodes)

    async def a_caption_media(
        self, media: Media, system_prompt: str | None = None
    ) -> str:
        """
        async Caption image.
        Args:
            media: The media to caption.
            system_prompt: Optional system prompt to guide the captioning.

        Returns:
            The string caption.
        """
        response = await self.client.a_invoke(
            input=[MediaBlock(media)], system_prompt=system_prompt
        )
        return response.text

    async def _a_process_media(self, media_node: MediaNode) -> tuple[MediaNode, str]:
        if media_node.node_type == NodeType.FIGURE:
            system_prompt = self.system_prompt_figure
        elif media_node.node_type == NodeType.TABLE:
            system_prompt = self.system_prompt_table
        else:
            raise ValueError(f"Unsupported node type: {media_node.node_type}")

        caption = await self.a_caption_media(
            media=media_node.media, system_prompt=system_prompt
        )

        # Create a new MediaNode with the caption nodes
        new_children = [
            Node(
                node_type=NodeType.PARAGRAPH,
                content=f" {media_node.content} <{media_node.node_type.value}> [{caption}]",
                metadata=media_node.metadata,
            ),
            *media_node.children,
            Node(
                node_type=NodeType.PARAGRAPH,
                content=f"</{media_node.node_type.value}>",
                metadata=media_node.metadata,
            ),
        ]

        new_media_node = MediaNode(
            node_type=media_node.node_type,
            media=media_node.media,
            children=new_children,
            metadata=media_node.metadata,
        )

        return new_media_node, caption



================================================
FILE: datapizza-ai-core/datapizza/modules/metatagger/__init__.py
================================================
from .keyword_metatagger import KeywordMetatagger

__all__ = ["KeywordMetatagger"]



================================================
FILE: datapizza-ai-core/datapizza/modules/metatagger/keyword_metatagger.py
================================================
from concurrent.futures import ThreadPoolExecutor

from pydantic import BaseModel

from datapizza.core.clients import Client
from datapizza.core.modules.metatagger import Metatagger
from datapizza.memory.memory import Memory
from datapizza.type import ROLE, Chunk, TextBlock


class KeywordMetatagger(Metatagger):
    """
    Keyword metatagger that uses an LLM client to add metadata to a chunk.
    """

    def __init__(
        self,
        client: Client,
        max_workers: int = 3,
        system_prompt: str | None = None,
        user_prompt: str | None = None,
        keyword_name: str = "keywords",
    ):
        """
        Args:
            client: The LLM client to use.
            max_workers: The maximum number of workers to use.
            system_prompt: The system prompt to use.
            user_prompt: The user prompt to use.
            keyword_name: The name of the keyword field.
        """
        self.client = client
        self.max_workers = max_workers
        self.keyword_name = keyword_name
        self.system_prompt = system_prompt
        self.user_prompt = user_prompt

    def _process_chunk(self, chunk: Chunk) -> Chunk:
        # Process the text with the client and return a Chunk with metadata
        if self.user_prompt:
            memory = Memory()
            memory.add_turn(
                blocks=[
                    TextBlock(content=self.user_prompt),
                ],
                role=ROLE.USER,
            )
        else:
            memory = None

        class KeywordMetataggerOutput(BaseModel):
            keywords: list[str]

        response = self.client.structured_response(
            input=chunk.text,
            system_prompt=self.system_prompt,
            memory=memory,
            output_cls=KeywordMetataggerOutput,
        )

        updated_metadata = chunk.metadata
        updated_metadata[self.keyword_name] = response.structured_data[0].keywords  # type: ignore

        return Chunk(id=chunk.id, text=chunk.text, metadata=updated_metadata)

    def _process(self, chunks: list[Chunk]) -> list[Chunk]:
        # Process chunks concurrently
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self._process_chunk, chunk) for chunk in chunks]
            results = [future.result() for future in futures]
        return results

    def __call__(self, chunks: list[Chunk]) -> list[Chunk]:
        return self._process(chunks)

    def tag(self, chunks: list[Chunk]):
        """
        Add metadata to a chunk.
        """
        return self._process(chunks)

    async def a_tag(self, chunks: list[Chunk]) -> list[Chunk]:
        """
        async Add metadata to a chunk.
        """
        raise NotImplementedError



================================================
FILE: datapizza-ai-core/datapizza/modules/metatagger/tests/test_keyword_metagger.py
================================================
from unittest.mock import Mock

import pytest
from pydantic import BaseModel

from datapizza.core.clients import Client, ClientResponse
from datapizza.memory.memory import Memory
from datapizza.modules.metatagger.keyword_metatagger import KeywordMetatagger
from datapizza.type import Chunk, StructuredBlock


@pytest.fixture
def mock_client():
    client = Mock(spec=Client)
    return client


@pytest.fixture
def sample_chunks():
    return [
        Chunk(
            id="1",
            text="This is a test document about artificial intelligence.",
            metadata={},
        ),
        Chunk(
            id="2",
            text="Machine learning and deep learning are important topics.",
            metadata={},
        ),
    ]


@pytest.fixture
def keyword_metatagger(mock_client):
    return KeywordMetatagger(
        client=mock_client,
        max_workers=2,
        system_prompt="Extract keywords from the text.",
        user_prompt="Please identify key terms.",
        keyword_name="keywords",
    )


def test_keyword_metatagger_initialization(keyword_metatagger):
    assert keyword_metatagger.max_workers == 2
    assert keyword_metatagger.system_prompt == "Extract keywords from the text."
    assert keyword_metatagger.user_prompt == "Please identify key terms."
    assert keyword_metatagger.keyword_name == "keywords"


def test_process_chunk(keyword_metatagger, mock_client):
    class KeywordMetataggerOutput(BaseModel):
        keywords: list[str]

    # Setup mock response
    mock_response = ClientResponse(
        content=[
            StructuredBlock(
                content=KeywordMetataggerOutput(keywords=["AI", "machine learning"])
            )
        ]
    )
    mock_client.structured_response.return_value = mock_response

    # Create test chunk
    chunk = Chunk(id="1", text="Test text", metadata={})

    # Process chunk
    result = keyword_metatagger._process_chunk(chunk)

    # Verify results
    assert result.id == "1"
    assert result.text == "Test text"
    assert result.metadata["keywords"] == ["AI", "machine learning"]

    # Verify client was called correctly
    mock_client.structured_response.assert_called_once()
    call_args = mock_client.structured_response.call_args[1]
    assert call_args["input"] == "Test text"
    assert call_args["system_prompt"] == "Extract keywords from the text."
    assert isinstance(call_args["memory"], Memory)
    assert (
        call_args["memory"].memory[0].blocks[0].content == "Please identify key terms."
    )


def test_process_chunks_concurrent(keyword_metatagger, mock_client, sample_chunks):
    class KeywordMetataggerOutput(BaseModel):
        keywords: list[str]

    # Setup mock responses
    mock_response1 = ClientResponse(
        content=[
            StructuredBlock(content=KeywordMetataggerOutput(keywords=["AI", "test"]))
        ]
    )
    mock_response2 = ClientResponse(
        content=[
            StructuredBlock(
                content=KeywordMetataggerOutput(
                    keywords=["machine learning", "deep learning"]
                )
            )
        ]
    )
    mock_client.structured_response.side_effect = [mock_response1, mock_response2]

    # Process chunks
    results = keyword_metatagger._process(sample_chunks)

    # Verify results
    assert len(results) == 2
    assert results[0].metadata["keywords"] == ["AI", "test"]
    assert results[1].metadata["keywords"] == ["machine learning", "deep learning"]
    assert mock_client.structured_response.call_count == 2



================================================
FILE: datapizza-ai-core/datapizza/modules/parsers/__init__.py
================================================
from pkgutil import extend_path

__path__ = extend_path(__path__, __name__)

from .text_parser import TextParser

__all__ = ["TextParser"]



================================================
FILE: datapizza-ai-core/datapizza/modules/parsers/md_parser.py
================================================
import re

from datapizza.core.modules.parser import Parser
from datapizza.type import Node, NodeType


class MDParser(Parser):
    """
    Parser that creates a hierarchical tree structure from Markdown file.
    The hierarchy goes from document -> sections -> paragraphs -> sentences.
    """

    def __init__(self):
        """Initialize the MDParser."""
        super().__init__()
        # Regex pattern for splitting text into sentences (same as TextParser)
        self.sentence_pattern = re.compile(
            r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s"
        )
        # Regex to match markdown headers
        self.header_pattern = re.compile(r"^(#+)\s+(.*)")

    def parse(self, file_path: str, metadata: dict | None = None) -> Node:
        """
        Parse markdown file into a hierarchical tree structure.

        Args:
            file_path: The path to the markdown file to parse
            metadata: Optional metadata for the root node

        Returns:
            A Node representing the document with section, paragraph and sentence nodes
        """
        if metadata is None:
            metadata = {}
        metadata["file_path"] = file_path

        with open(file_path, encoding="utf-8") as f:
            text = f.read()

        document_node = Node(
            children=[], metadata=metadata.copy(), node_type=NodeType.DOCUMENT
        )

        # Stack of (level, node). Document is level 0.
        stack: list[tuple[int, Node]] = [(0, document_node)]

        lines = text.split("\n")
        current_paragraph_lines = []

        def flush_paragraph():
            if not current_paragraph_lines:
                return

            para_text = " ".join(current_paragraph_lines).strip()
            if not para_text:
                return

            # Create paragraph node
            paragraph_node = Node(
                children=[],
                metadata=metadata.copy(),  # Add metadata to paragraph
                node_type=NodeType.PARAGRAPH,
            )

            # Split into sentences
            sentences = self._split_sentences(para_text)
            for i, sentence_text in enumerate(sentences):
                sent_metadata = metadata.copy()
                sent_metadata.update({"index": i, "text": sentence_text})
                sentence_node = Node(
                    children=[],
                    metadata=sent_metadata,
                    node_type=NodeType.SENTENCE,
                    content=sentence_text,
                )
                paragraph_node.add_child(sentence_node)

            # Add paragraph to the current active section (top of stack)
            stack[-1][1].add_child(paragraph_node)
            current_paragraph_lines.clear()

        for line in lines:
            line = line.strip()
            if not line:
                continue

            header_match = self.header_pattern.match(line)
            if header_match:
                # Flush any pending paragraph text before starting new section
                flush_paragraph()

                level = len(header_match.group(1))
                title = header_match.group(2).strip()

                # Pop stack until we find a parent with lower level
                while stack and stack[-1][0] >= level:
                    stack.pop()

                sec_metadata = metadata.copy()
                sec_metadata.update({"title": title, "level": level})

                # Create new section node
                section_node = Node(
                    children=[], metadata=sec_metadata, node_type=NodeType.SECTION
                )

                # Add to parent
                if stack:
                    stack[-1][1].add_child(section_node)

                # Push to stack
                stack.append((level, section_node))
            else:
                # Accumulate text for paragraph
                current_paragraph_lines.append(line)

        # Flush any remaining paragraph text
        flush_paragraph()

        return document_node

    async def a_parse(self, file_path: str, metadata: dict | None = None) -> Node:
        return self.parse(file_path, metadata)

    def _split_sentences(self, text: str) -> list[str]:
        """Split text into sentences."""
        sentences = self.sentence_pattern.split(text)
        return [s.strip() for s in sentences if s.strip()]



================================================
FILE: datapizza-ai-core/datapizza/modules/parsers/text_parser.py
================================================
import re

from datapizza.core.modules.parser import Parser
from datapizza.type import Node, NodeType


class TextParser(Parser):
    """
    Parser that creates a hierarchical tree structure from text.
    The hierarchy goes from document -> paragraphs -> sentences.
    """

    def __init__(self):
        """Initialize the TextParser."""
        # Regex pattern for splitting text into sentences
        self.sentence_pattern = re.compile(
            r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s"
        )

    def parse(self, text: str, metadata: dict | None = None) -> Node:
        """
        Parse text into a hierarchical tree structure.

        Args:
            text: The text to parse
            metadata: Optional metadata for the root node

        Returns:
            A Node representing the document with paragraph and sentence nodes
        """
        # Create root document node
        document_node = Node(
            children=[], metadata=metadata, node_type=NodeType.DOCUMENT
        )

        # Split text into paragraphs (based on double newlines)
        paragraphs = self._split_paragraphs(text)

        # Process each paragraph
        for i, paragraph_text in enumerate(paragraphs):
            paragraph_node = Node(
                children=[], metadata={"index": i}, node_type=NodeType.PARAGRAPH
            )

            # Split paragraph into sentences
            sentences = self._split_sentences(paragraph_text)

            # Process each sentence
            for j, sentence_text in enumerate(sentences):
                # Create sentence node with the text content in its metadata
                sentence_node = Node(
                    children=[],
                    metadata={"index": j, "text": sentence_text.strip()},
                    node_type=NodeType.SENTENCE,
                    content=sentence_text.strip(),
                )

                # Add sentence node to paragraph
                paragraph_node.add_child(sentence_node)

            # Add paragraph node to document
            document_node.add_child(paragraph_node)

        return document_node

    def a_parse(self, text: str, metadata: dict | None = None) -> Node:
        return self.parse(text, metadata)

    def _split_paragraphs(self, text: str) -> list[str]:
        """Split text into paragraphs based on double newlines."""
        # Split by double newlines and filter out empty paragraphs
        paragraphs = text.strip().split("\n\n")
        return [p.strip() for p in paragraphs if p.strip()]

    def _split_sentences(self, paragraph: str) -> list[str]:
        """Split paragraph into sentences."""
        sentences = self.sentence_pattern.split(paragraph)
        return [s.strip() for s in sentences if s.strip()]


def parse_text(text: str, metadata: dict | None = None) -> Node:
    """
    Convenience function to parse text into a hierarchical structure.

    Args:
        text: The text to parse
        metadata: Optional metadata for the root node

    Returns:
        A Node representing the document with paragraph, sentence, and word nodes
    """
    parser = TextParser()
    return parser.parse(text, metadata)



================================================
FILE: datapizza-ai-core/datapizza/modules/parsers/tests/test_base_parser.py
================================================
from datapizza.modules.parsers.text_parser import TextParser
from datapizza.type import NodeType


def test_base_parser():
    parser = TextParser()
    assert parser is not None

    text = """Hello, world!


    This is a test.
    this is a sentence.
    """

    document = parser.parse(text)
    assert document is not None
    assert document.node_type == NodeType.DOCUMENT

    assert len(document.children) == 2

    assert document.children[0].node_type == NodeType.PARAGRAPH
    assert document.children[0].children[0].node_type == NodeType.SENTENCE

    assert document.children[0].content == "Hello, world!"
    assert document.children[1].content == "This is a test. this is a sentence."
    assert len(document.children[1].children) == 2



================================================
FILE: datapizza-ai-core/datapizza/modules/parsers/tests/test_md_parser.py
================================================
import os
import tempfile

import pytest

from datapizza.modules.parsers.md_parser import MDParser
from datapizza.type import NodeType


@pytest.fixture
def temp_md_file():
    content = """# Title

This is a paragraph.

## Section 1

This is section 1.

### Subsection 1.1

Content 1.1.

## Section 2

Content 2.
"""
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".md") as f:
        f.write(content)
        path = f.name
    yield path
    os.remove(path)


def test_md_parser_hierarchy(temp_md_file):
    parser = MDParser()
    root = parser.parse(temp_md_file, metadata={"source": "test"})

    assert root.node_type == NodeType.DOCUMENT
    assert root.metadata["file_path"] == temp_md_file
    assert root.metadata["source"] == "test"
    assert len(root.children) == 1  # # Title

    title_node = root.children[0]
    assert title_node.node_type == NodeType.SECTION
    assert title_node.metadata["title"] == "Title"
    assert title_node.metadata["level"] == 1
    assert title_node.metadata["file_path"] == temp_md_file

    # Title content
    assert len(title_node.children) == 3  # Paragraph, Section 1, Section 2
    # Wait, logic check:
    # H1 Title
    #   Para
    #   H2 Section 1
    #     Para
    #     H3 Subsection 1.1
    #       Para
    #   H2 Section 2
    #     Para

    # Let's check the structure created by the parser.
    # stack starts with [(0, doc)]
    # # Title (level 1). pop until stack[-1][0] < 1. 0 < 1. ok. stack: [(0, doc), (1, Title)]
    # Para "This is a paragraph." -> child of Title.
    # ## Section 1 (level 2). pop until stack[-1][0] < 2. 1 < 2. ok. stack: [(0, doc), (1, Title), (2, Section 1)]
    # Para "This is section 1." -> child of Section 1.
    # ### Subsection 1.1 (level 3). pop until stack[-1][0] < 3. 2 < 3. ok. stack: [(0, doc), (1, Title), (2, Sec 1), (3, Sub 1.1)]
    # Para "Content 1.1." -> child of Sub 1.1.
    # ## Section 2 (level 2). pop until stack[-1][0] < 2. (3, Sub 1.1) pop. (2, Sec 1) pop. (1, Title) < 2. ok. stack: [(0, doc), (1, Title), (2, Section 2)]
    # Para "Content 2." -> child of Section 2.

    # So Title should have 3 children?
    # Children of Title:
    # 1. Paragraph
    # 2. Section 1
    # 3. Section 2 (since it was popped back to Title)

    assert len(title_node.children) == 3

    para1 = title_node.children[0]
    assert para1.node_type == NodeType.PARAGRAPH
    assert para1.children[0].content == "This is a paragraph."
    assert para1.metadata["file_path"] == temp_md_file

    sec1 = title_node.children[1]
    assert sec1.node_type == NodeType.SECTION
    assert sec1.metadata["title"] == "Section 1"
    assert sec1.metadata["file_path"] == temp_md_file

    # Section 1 children: Paragraph and Subsection 1.1
    assert len(sec1.children) == 2
    assert sec1.children[0].node_type == NodeType.PARAGRAPH
    assert sec1.children[0].children[0].content == "This is section 1."

    subsec1 = sec1.children[1]
    assert subsec1.node_type == NodeType.SECTION
    assert subsec1.metadata["title"] == "Subsection 1.1"

    sec2 = title_node.children[2]
    assert sec2.node_type == NodeType.SECTION
    assert sec2.metadata["title"] == "Section 2"


def test_md_parser_no_headers():
    parser = MDParser()
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".md") as f:
        f.write("Just a paragraph.")
        path = f.name

    try:
        root = parser.parse(path)
        assert root.node_type == NodeType.DOCUMENT
        assert root.metadata["file_path"] == path
        assert len(root.children) == 1
        assert root.children[0].node_type == NodeType.PARAGRAPH
        assert root.children[0].children[0].content == "Just a paragraph."
        assert root.children[0].metadata["file_path"] == path
    finally:
        os.remove(path)


def test_md_parser_sentences():
    parser = MDParser()
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".md") as f:
        f.write("Sentence one. Sentence two.")
        path = f.name

    try:
        root = parser.parse(path)
        para = root.children[0]
        assert len(para.children) == 2
        assert para.children[0].content == "Sentence one."
        assert para.children[1].content == "Sentence two."
        assert para.children[0].metadata["file_path"] == path
    finally:
        os.remove(path)



================================================
FILE: datapizza-ai-core/datapizza/modules/prompt/__init__.py
================================================
from .image_rag import ImageRAGPrompt
from .prompt import ChatPromptTemplate

__all__ = ["ChatPromptTemplate", "ImageRAGPrompt"]



================================================
FILE: datapizza-ai-core/datapizza/modules/prompt/image_rag.py
================================================
from jinja2 import Template

from datapizza.core.modules.prompt import Prompt
from datapizza.core.utils import extract_media
from datapizza.memory import Memory
from datapizza.type import ROLE, Block, Chunk, Media, MediaBlock, TextBlock


class ImageRAGPrompt(Prompt):
    """
    Create a memory for a image RAG system.

    """

    def __init__(
        self,
        user_prompt_template: str,
        image_prompt_presentation: str,
        each_image_template: str,
    ):
        """
        Args:
            user_prompt_template: str # The user prompt jinja template
            image_prompt_presentation: str # The image prompt jinja template
            each_image_template: str # The each image jinja template
        """
        self.user_prompt_template = Template(user_prompt_template)
        self.image_prompt_presentation = image_prompt_presentation
        self.each_image_template = Template(each_image_template)

    def _extract_images_from_chunk(self, chunk: Chunk) -> list[MediaBlock]:
        metadata = chunk.metadata
        list_bboxes = metadata["boundingRegions"]
        path_pdf = metadata["document_name"]

        list_media_blocks: list[MediaBlock] = []

        for bbox in list_bboxes:
            page_number = bbox["pageNumber"]
            polygon = bbox["polygon"]

            # Extract the image from the PDF
            image_as_base64 = extract_media(
                coordinates=polygon,
                file_path=path_pdf,
                page_number=page_number,
            )

            media = Media(
                media_type="image",
                source=image_as_base64,
                source_type="base64",
                extension="png",
            )

            list_media_blocks.append(MediaBlock(media=media))

        return list_media_blocks

    def format(
        self,
        chunks: list[Chunk],
        user_query: str,
        retrieval_query: str,
        memory: Memory | None = None,
    ) -> Memory:
        """
        Creates a new memory object that includes:
        - Existing memory messages
        - User's query
        - Function call retrieval results
        - Chunks retrieval results

        Args:
            chunks: The chunks to add to the memory.
            user_query: The user's query.
            retrieval_query: The query to search the vectorstore for.
            memory: The memory object to add the new messages to.

        Returns:
            memory: A new memory object with the new messages.
        """
        new_memory = Memory()

        if memory:
            for turn in memory:
                new_memory.add_turn(turn.blocks, turn.role)

        all_blocks: list[Block] = [TextBlock(content=self.image_prompt_presentation)]

        for chunk in chunks:
            all_blocks.extend(
                [
                    TextBlock(
                        content=self.each_image_template.render(
                            path_pdf=chunk.metadata["document_name"].split("/")[-1],
                        )
                    )
                ]
            )
            all_blocks.extend(self._extract_images_from_chunk(chunk))

        formatted_user_prompt = self.user_prompt_template.render(
            user_prompt=user_query, retrieval_query=retrieval_query
        )

        all_blocks.append(TextBlock(content=formatted_user_prompt))

        new_memory.add_turn(blocks=all_blocks, role=ROLE.USER)

        return new_memory



================================================
FILE: datapizza-ai-core/datapizza/modules/prompt/prompt.py
================================================
import uuid

from jinja2.sandbox import SandboxedEnvironment

from datapizza.core.modules.prompt import Prompt
from datapizza.memory.memory import Memory
from datapizza.tools import Tool, tool
from datapizza.type import (
    ROLE,
    Chunk,
    FunctionCallBlock,
    FunctionCallResultBlock,
    TextBlock,
)


@tool
def _search_vectorstore(query: str):
    """
    Search the vectorstore for the most relevant chunks

    Args:
        query: The query to search the vectorstore for

    Returns:
        A list of Chunks that are most relevant to the query
    """
    pass


class ChatPromptTemplate(Prompt):
    """
    It takes as input a Memory, Chunks, Prompt and creates a Memory
    with all existing messages + the user's qry + function_call_retrieval +
    chunks retrieval.
    args:
        user_prompt_template: str # The user prompt jinja template
        retrieval_prompt_template: str # The retrieval prompt jinja template
    """

    def __init__(self, user_prompt_template, retrieval_prompt_template):
        env = SandboxedEnvironment()

        self.user_prompt_template = env.from_string(user_prompt_template)
        self.retrieval_prompt_template = env.from_string(retrieval_prompt_template)

    def format(
        self,
        memory: Memory | None = None,
        chunks: list[Chunk] | None = None,
        user_prompt: str = "",
        retrieval_query: str = "",
    ) -> Memory:
        """
        Creates a new memory object that includes:
        - Existing memory messages
        - User's query
        - Function call retrieval results
        - Chunks retrieval results

        Args:
            memory: The memory object to add the new messages to.
            chunks: The chunks to add to the memory.
            user_prompt: The user's query.
            retrieval_query: The query to search the vectorstore for.

        Returns:
            A new memory object with the new messages.
        """

        new_memory = Memory()

        # Add existing memory if any
        if memory:
            for turn in memory:
                new_memory.add_turn(turn.blocks, turn.role)

        # Add user's prompt
        formatted_user_prompt = self.user_prompt_template.render(
            user_prompt=user_prompt
        )
        new_memory.add_turn(
            blocks=[TextBlock(content=formatted_user_prompt)], role=ROLE.USER
        )

        tool_id = str(uuid.uuid4())

        if chunks is not None:
            new_memory.add_turn(
                blocks=FunctionCallBlock(
                    id=tool_id,
                    arguments={"query": retrieval_query},
                    name="search_vectorstore",
                    tool=Tool(func=_search_vectorstore),
                ),
                role=ROLE.ASSISTANT,
            )

            formatted_retrieval = self.retrieval_prompt_template.render(chunks=chunks)
            new_memory.add_turn(
                blocks=FunctionCallResultBlock(
                    id=tool_id,
                    tool=Tool(func=_search_vectorstore),
                    result=formatted_retrieval,
                ),
                role=ROLE.TOOL,
            )

        return new_memory



================================================
FILE: datapizza-ai-core/datapizza/modules/prompt/tests/test_chat_prompt_template.py
================================================
from jinja2 import Template

from datapizza.memory import Memory
from datapizza.modules.prompt import ChatPromptTemplate
from datapizza.type import ROLE, Chunk, TextBlock


def test_chat_prompt_template_with_no_memory():
    user_prompt_template = "this is a user prompt: {{ user_prompt }}"
    retrieval_prompt_template = (
        "{% for chunk in chunks %}CHUNK: {{ chunk.text }}{% endfor %}"
    )

    chat_prompt_template = ChatPromptTemplate(
        user_prompt_template, retrieval_prompt_template
    )

    chunks = [
        Chunk(
            id="1",
            text="This is a chunk of text",
            metadata={"source": "test"},
        )
    ]
    user_prompt = "What is the capital of France?"

    res = chat_prompt_template.format(chunks=chunks, user_prompt=user_prompt)
    assert res is not None

    assert res.memory[0].role == ROLE.USER
    assert res.memory[0].blocks[0].content == Template(user_prompt_template).render(
        user_prompt=user_prompt
    )

    assert res.memory[2].role == ROLE.TOOL
    assert res.memory[2].blocks[0].result == Template(retrieval_prompt_template).render(
        chunks=chunks
    )


def test_chat_prompt_template_with_memory():
    user_prompt_template = "this is a user prompt: {{ user_prompt }}"
    retrieval_prompt_template = (
        "{% for chunk in chunks %}CHUNK: {{ chunk.text }}{% endfor %}"
    )
    user_prompt = "What is the capital of France?"

    chat_prompt_template = ChatPromptTemplate(
        user_prompt_template, retrieval_prompt_template
    )

    memory = Memory()
    memory.add_turn(TextBlock(content="This is an old message"), ROLE.USER)
    memory.add_turn(TextBlock(content="This is an old message"), ROLE.ASSISTANT)

    chunks = [
        Chunk(id="1", text="This is a chunk of text", metadata={"source": "test"})
    ]

    res = chat_prompt_template.format(
        memory=memory,
        chunks=chunks,
        user_prompt=user_prompt,
    )
    assert res is not None

    assert res.memory[2].role == ROLE.USER
    assert res.memory[2].blocks[0].content == Template(user_prompt_template).render(
        user_prompt=user_prompt
    )

    assert res.memory[4].role == ROLE.TOOL
    assert res.memory[4].blocks[0].result == Template(retrieval_prompt_template).render(
        chunks=chunks
    )



================================================
FILE: datapizza-ai-core/datapizza/modules/rewriters/__init__.py
================================================
from .tool_rewriter import ToolRewriter

__all__ = ["ToolRewriter"]



================================================
FILE: datapizza-ai-core/datapizza/modules/rewriters/tool_rewriter.py
================================================
from datapizza.core.clients import Client
from datapizza.core.modules.rewriter import Rewriter
from datapizza.memory.memory import Memory
from datapizza.tools import Tool
from datapizza.tools.tools import tool
from datapizza.type.type import FunctionCallBlock


class ToolRewriter(Rewriter):
    """
    A tool-based query rewriter that uses LLMs to transform user queries through structured tool interactions.
    """

    def __init__(
        self,
        client: Client,
        system_prompt: str | None = None,
        tool: Tool | None = None,
        tool_choice: str = "required",
        tool_output_name: str = "query",
        **invoke_args,
    ):
        self.client = client
        self.system_prompt = system_prompt
        self.invoke_args = invoke_args or {}

        if tool is None:
            # using a default tool
            self.tool = Tool(
                name="query_database",
                description="Query a database",
                func=self._search_vectorstore,
            )
        else:
            self.tool = tool

        self.tool_choice = tool_choice
        self.tool_output_name = tool_output_name

    def rewrite(self, user_prompt: str, memory: Memory | None = None) -> str:
        """
        Args:
            user_prompt: The user query to rewrite.
            memory: The memory to use for the rewrite.
            kwargs: Additional arguments to pass to the client.

        Returns:
            The rewritten query.
        """
        response = self.client.invoke(
            input=user_prompt,
            system_prompt=self.system_prompt,
            memory=memory,
            tool_choice=self.tool_choice,
            tools=[self.tool],
            **self.invoke_args,
        )

        if len(response.content) != 1:
            raise ValueError(
                "ToolRewriter supposed to return only one response, something bad occured"
            )
        else:
            if not isinstance(response.content[0], FunctionCallBlock):
                raise ValueError(
                    "ToolRewriter supposed to return a FunctionCallBlock, something bad occured"
                )

            return response.content[0].arguments[self.tool_output_name]

    async def a_rewrite(self, user_prompt: str, memory: Memory | None = None) -> str:
        """
        Args:
            user_prompt: The user query to rewrite.
            memory: The memory to use for the rewrite.
            kwargs: Additional arguments to pass to the client.

        Returns:
            The rewritten query.
        """
        response = await self.client.a_invoke(
            input=user_prompt,
            system_prompt=self.system_prompt,
            memory=memory,
            tool_choice=self.tool_choice,
            tools=[self.tool],
            **self.invoke_args,
        )
        if len(response.content) != 1:
            raise ValueError(
                "ToolRewriter supposed to return only one response, something bad occured"
            )
        else:
            if not isinstance(response.content[0], FunctionCallBlock):
                raise ValueError(
                    "ToolRewriter supposed to return a FunctionCallBlock, something bad occured"
                )
            return response.content[0].arguments[self.tool_output_name]

    @tool
    def _search_vectorstore(self, query: str):
        """
        Search the vectorstore for the most relevant chunks

        Args:
            query: The query to search the vectorstore for

        Returns:
            A list of Chunks that are most relevant to the query
        """
        pass



================================================
FILE: datapizza-ai-core/datapizza/modules/rewriters/tests/test_tool_rewriter.py
================================================
from datapizza.clients import MockClient
from datapizza.modules.rewriters import ToolRewriter


def test_tool_rewriter():
    tool_rewriter = ToolRewriter(
        client=MockClient(model_name="test"),
        system_prompt="test",
    )
    assert tool_rewriter is not None



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/__init__.py
================================================
from .node_splitter import NodeSplitter
from .pdf_image_splitter import PDFImageSplitter
from .recursive_splitter import RecursiveSplitter
from .text_splitter import TextSplitter

__all__ = [
    "NodeSplitter",
    "PDFImageSplitter",
    "RecursiveSplitter",
    "TextSplitter",
]



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/bbox_merger.py
================================================
from datapizza.core.models import PipelineComponent
from datapizza.type import Chunk


class BboxMerger(PipelineComponent):
    def __init__(self):
        """
        Initialize the BboxMerger
        """
        pass

    @staticmethod
    def get_combined_bounding_boxes(coordinates):
        """
        Given a list of bounding boxes with their page numbers, returns either:
        - A single dictionary with 'pageNumber' and 'polygon' keys if all boxes are on the same page
        - A list of dictionaries with 'pageNumber' and 'polygon' keys if they span multiple pages

        Args:
            coordinates: List of dictionaries containing 'pageNumber' and 'polygon' keys
                Example: [{'pageNumber': 1, 'polygon': [x1, y1, x2, y2, ...]}, ...]

        Returns:
            If single page: {'pageNumber': page_num, 'polygon': [x1, y1, x2, y2, ...]}
            If multiple pages: List of {'pageNumber': page_num, 'polygon': [x1, y1, x2, y2, ...]} for each page
        """
        # Group coordinates by page number
        page_coords = {}
        for coord in coordinates:
            page_num = coord["pageNumber"]
            polygon = coord["polygon"]
            if page_num not in page_coords:
                page_coords[page_num] = []
            page_coords[page_num].append(polygon)

        # Calculate bounding box for each page
        page_bboxes = {}
        for page_num, coords_list in page_coords.items():
            # Initialize with first box
            first_coords = coords_list[0]
            min_x = min(first_coords[::2])
            min_y = min(first_coords[1::2])
            max_x = max(first_coords[::2])
            max_y = max(first_coords[1::2])

            # Update with remaining boxes
            for coords in coords_list[1:]:
                min_x = min(min_x, min(coords[::2]))
                min_y = min(min_y, min(coords[1::2]))
                max_x = max(max_x, max(coords[::2]))
                max_y = max(max_y, max(coords[1::2]))

            # Create polygon with 4 corners in clockwise order: top-left, top-right, bottom-right, bottom-left
            combined_polygon = [
                min_x,
                min_y,  # top-left
                max_x,
                min_y,  # top-right
                max_x,
                max_y,  # bottom-right
                min_x,
                max_y,  # bottom-left
            ]

            page_bboxes[page_num] = {
                "pageNumber": page_num,
                "polygon": combined_polygon,
            }

        # If all boxes are on the same page, return single bbox
        if len(page_bboxes) == 1:
            return next(iter(page_bboxes.values()))

        # Otherwise return list of bboxes ordered by page number
        return [page_bboxes[page] for page in sorted(page_bboxes.keys())]

    def merge_metadata(self, chunks: list[Chunk]) -> list[Chunk]:
        for chunk in chunks:
            if chunk.metadata.get("boundingRegions"):
                chunk.metadata["boundingRegions"] = self.get_combined_bounding_boxes(
                    chunk.metadata["boundingRegions"]
                )
        return chunks

    def __call__(self, chunks: list[Chunk]) -> list[Chunk]:
        return self.merge_metadata(chunks)

    async def _a_run(self, chunks: list[Chunk]) -> list[Chunk]:
        return self.merge_metadata(chunks)



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/node_splitter.py
================================================
from datapizza.core.modules.splitter import Splitter
from datapizza.type.type import Chunk, Node


class NodeSplitter(Splitter):
    """
    A splitter that traverses a document tree from the root node. If the root node's content is smaller than max_chars,
    it becomes a single chunk. Otherwise, it recursively processes the node's children, creating chunks from the first
    level of children that fit within max_chars, continuing deeper into the tree structure as needed.
    """

    def __init__(self, max_char: int = 5000):
        """
        Initialize the NodeSplitter.

        Args:
            max_char: The maximum number of characters per chunk
        """
        self.max_char = max_char

    def _node_to_chunks(self, nodes: list[Node]) -> list[Chunk]:
        return [
            Chunk(id=str(node.id), text=node.content, metadata=node.metadata)
            for node in nodes
            if node.content
        ]

    def split(self, node: Node) -> list[Chunk]:
        """
        Split the node into chunks.

        Args:
            node: The node to split

        Returns:
            A list of chunks
        """
        if len(node.content) <= self.max_char:
            return self._node_to_chunks([node])

        result = []

        for child in node.children:
            result.extend(self.split(node=child))

        if not result:
            return self._node_to_chunks([node])

        return result

    def __call__(self, node: Node) -> list[Chunk]:
        return self.split(node)

    async def a_split(self, node: Node) -> list[Chunk]:
        return self.split(node)



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/pdf_image_splitter.py
================================================
import logging
import uuid
from pathlib import Path
from typing import Literal

from datapizza.core.modules.splitter import Splitter
from datapizza.type.type import Chunk

log = logging.getLogger(__name__)


class PDFImageSplitter(Splitter):
    """Splits a PDF document into individual pages, saves each page as an image using fitz,
    and returns metadata about each page as a Chunk object.
    """

    def __init__(
        self,
        image_format: Literal["png", "jpeg"] = "png",
        output_base_dir: str | Path = "output_images",
        dpi: int = 300,  # Added DPI setting for fitz
    ):
        """Initializes the Splitter.

        Args:
            image_format: The format to save the images in ('png' or 'jpeg'). Defaults to 'png'.
            output_base_dir: The base directory where images for processed PDFs will be saved.
                              A subdirectory will be created for each PDF. Defaults to 'output_images'.
            dpi: Dots Per Inch for rendering the PDF page to an image. Higher values increase resolution and file size. Defaults to 300.
        """
        self.image_format = image_format.lower()
        if self.image_format not in [
            "png",
            "jpeg",
            "jpg",
        ]:  # Allow jpg as alias for jpeg
            # Fitz save uses extension, so jpg is fine. Allow it as alias.
            if self.image_format == "jpg":
                self.image_format = "jpeg"
            else:
                raise ValueError("image_format must be 'png' or 'jpeg'/'jpg'")
        self.output_base_dir = Path(output_base_dir)
        self.dpi = dpi

    def split(self, pdf_path: str | Path) -> list[Chunk]:
        """Processes the PDF using fitz: converts pages to images and returns Chunk objects.

        Args:
            pdf_path: The path to the input PDF file.

        Returns:
            A list of Chunk objects, one for each page of the PDF.
        """

        try:
            import fitz
        except ImportError as e:
            raise ImportError(
                "PyMuPDF is not installed. Please install it using `pip install PyMuPDF`."
            ) from e

        pdf_path = Path(pdf_path)
        if not pdf_path.is_file() or pdf_path.suffix.lower() != ".pdf":
            raise ValueError(f"Invalid PDF path: {pdf_path}")

        # Create a unique output directory for this PDF's images
        pdf_filename = pdf_path.stem
        pdf_output_dir = self.output_base_dir / pdf_filename
        pdf_output_dir.mkdir(parents=True, exist_ok=True)

        chunks = []
        doc = None  # Initialize doc to None
        try:
            doc = fitz.open(pdf_path)
            for i, page in enumerate(doc):  # type: ignore
                page_num = i + 1
                image_id = str(uuid.uuid4())
                # Use 'jpg' extension if format is 'jpeg' for compatibility with fitz save
                file_extension = (
                    "jpg" if self.image_format == "jpeg" else self.image_format
                )
                image_filename = f"page_{page_num}_{image_id}.{file_extension}"
                image_path = pdf_output_dir / image_filename

                try:
                    # Generate pixmap with specified DPI
                    pix = page.get_pixmap(dpi=self.dpi)
                    # Save the pixmap. Fitz determines format from the extension.
                    pix.save(str(image_path))
                except Exception as e:
                    # Handle potential errors during pixmap generation or saving
                    print(
                        f"Error processing page {page_num} of {pdf_path} to {image_path}: {e}"
                    )
                    # Decide on error handling: skip this page, raise an error, etc.
                    # For now, we'll skip this page and continue
                    continue  # Skip creating a chunk for this failed page

                chunk = Chunk(
                    id=image_id,
                    text="",  # As requested
                    embeddings=[],  # Default factory handles this, but being explicit
                    metadata={
                        "image_path": str(image_path.resolve())
                    },  # Store absolute path
                )
                chunks.append(chunk)

        except Exception as e:
            log.error(f"Error opening or processing PDF {pdf_path}: {e}")
            raise Exception(f"Failed to process PDF {pdf_path}.") from e
        finally:
            if doc:
                doc.close()  # Ensure the document is closed

        return chunks

    async def a_split(self, pdf_path: str | Path) -> list[Chunk]:
        return self.split(pdf_path)

    def __call__(self, pdf_path: str | Path) -> list[Chunk]:
        return self.split(pdf_path)



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/recursive_splitter.py
================================================
import uuid

from datapizza.core.modules.splitter import Splitter
from datapizza.type.type import Chunk, Node


class RecursiveSplitter(Splitter):
    """
    The RecursiveSplitter takes leaf nodes from a tree document structure and groups them into Chunk objects until reaching the maximum character limit. Each leaf Node represents the smallest unit of content that can be grouped.

    """

    def __init__(self, max_char: int = 5000, overlap: int = 0):
        """
        Initialize the RecursiveSplitter.

        Args:
            max_char: The maximum number of characters per chunk
            overlap: The number of characters to overlap between chunks
        """
        self.max_char = max_char
        self.overlap = overlap

    def _nodes_to_chunk(self, nodes: list[Node]) -> Chunk:
        chunk_id = str(uuid.uuid4())
        chunk_text = " ".join([node.content for node in nodes])
        # check if "boundingRegions" is in the metadata and if it is, merge them
        bounding_regions = [node.metadata.get("boundingRegions", []) for node in nodes]
        chunk_metadata = {
            "boundingRegions": [
                region for regions in bounding_regions for region in regions
            ]
        }
        return Chunk(id=chunk_id, text=chunk_text, metadata=chunk_metadata)

    def get_all_leaves(self, node: Node) -> list[Node]:
        if not node.children:
            return [node]
        leaves = []
        for child in node.children:
            leaves.extend(self.get_all_leaves(child))
        return leaves

    def split(self, node: Node) -> list[Chunk]:
        """
        Split the node into chunks.

        Args:
            node: The node to split

        Returns:
            A list of chunks
        """
        all_leaves = self.get_all_leaves(node)
        result = []
        list_nodes = []
        current_length = 0

        while all_leaves:
            current_node = all_leaves.pop(0)
            node_content_length = len(current_node.content)

            # If adding this node would exceed max_char, create a chunk from collected nodes
            if current_length + node_content_length > self.max_char and list_nodes:
                result.append(self._nodes_to_chunk(list_nodes))
                # Handle overlap if specified
                if self.overlap > 0:
                    # Keep some nodes for overlap in the next chunk
                    overlap_size = 0
                    overlap_nodes = []
                    for i in range(len(list_nodes) - 1, -1, -1):
                        node_len = len(list_nodes[i].content)
                        if overlap_size + node_len <= self.overlap:
                            overlap_nodes.insert(0, list_nodes[i])
                            overlap_size += node_len
                        else:
                            break
                    list_nodes = overlap_nodes
                    current_length = overlap_size
                else:
                    list_nodes = []
                    current_length = 0

            # If the node itself is too large, create a chunk just for it
            if node_content_length > self.max_char:
                if list_nodes:
                    result.append(self._nodes_to_chunk(list_nodes))
                    list_nodes = []
                    current_length = 0
                result.append(self._nodes_to_chunk([current_node]))
            else:
                list_nodes.append(current_node)
                current_length += node_content_length

        # Don't forget remaining nodes
        if list_nodes:
            result.append(self._nodes_to_chunk(list_nodes))

        return result

    async def a_split(self, node: Node) -> list[Chunk]:
        return self.split(node)



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/text_splitter.py
================================================
import uuid

from datapizza.core.modules.splitter import Splitter
from datapizza.type.type import Chunk


class TextSplitter(Splitter):
    """
    A basic text splitter that operates directly on strings rather than Node objects.
    Unlike other splitters that work with Node types, this splitter takes raw text input
    and splits it into chunks while maintaining configurable size and overlap parameters.

    """

    def __init__(self, max_char: int = 5000, overlap: int = 0):
        """
        Initialize the TextSplitter.

        Args:
            max_char: The maximum number of characters per chunk
            overlap: The number of characters to overlap between chunks
        """

        self.max_char = max_char
        self.overlap = overlap

    def split(self, text: str) -> list[Chunk]:
        """
        Split the text into chunks.

        Args:
            text: The text to split

        Returns:
            A list of chunks
        """
        if not isinstance(text, str):
            raise TypeError("TextSplitter expects a string input")

        text_length = len(text)
        if text_length == 0:
            return []

        if text_length <= self.max_char:
            return [Chunk(id=str(uuid.uuid4()), text=text, metadata={})]

        # Ensure progress even if overlap is large
        step = max(1, self.max_char - max(0, self.overlap))

        chunks: list[Chunk] = []
        start = 0
        while start < text_length:
            end = min(start + self.max_char, text_length)
            chunk_text = text[start:end]
            chunks.append(
                Chunk(
                    id=str(uuid.uuid4()),
                    text=chunk_text,
                    metadata={"start_char": start, "end_char": end},
                )
            )

            if end >= text_length:
                break
            start += step

        return chunks

    async def a_split(self, text: str) -> list[Chunk]:
        return self.split(text)



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/tests/test_node_splitter.py
================================================
from datapizza.modules.splitters.node_splitter import NodeSplitter
from datapizza.type.type import Node


def test_process_within_limit():
    """Test processing a node within character limit"""
    splitter = NodeSplitter(max_char=100)
    content = "Short content"
    node = Node(content=content)
    result = splitter.run(node)
    assert len(result) == 1
    assert result[0].text == content


def test_process_with_children():
    """Test processing a node with children"""
    splitter = NodeSplitter(max_char=10)
    parent = Node(content="Parent node")
    child1 = Node(content="Child 1")
    child2 = Node(content="Child 2")
    parent.children = [child1, child2]

    result = splitter.run(parent)
    assert len(result) == 2
    assert any(node.text == "Child 1" for node in result)
    assert any(node.text == "Child 2" for node in result)


def test_empy_node():
    splitter = NodeSplitter(max_char=10)
    node = Node()
    result = splitter.run(node)
    assert len(result) == 0


def test_process_large_content_no_children():
    """Test processing a node with content larger than max_char but no children"""
    splitter = NodeSplitter(max_char=10)
    content = "This is a very long content that exceeds the maximum character limit"
    node = Node(content=content)
    result = splitter.run(node)
    assert len(result) == 1  # Should return original node if no way to split
    assert result[0].text == content



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/tests/test_recursive_splitter.py
================================================
from datapizza.modules.splitters.recursive_splitter import RecursiveSplitter
from datapizza.type.type import Node


def test_recursive_splitter():
    recursive_splitter = RecursiveSplitter(max_char=10, overlap=0)
    chunks = recursive_splitter.split(Node(content="This is a test string"))
    assert len(chunks) == 1



================================================
FILE: datapizza-ai-core/datapizza/modules/splitters/tests/test_text_splitter.py
================================================
from datapizza.modules.splitters.text_splitter import TextSplitter


def test_text_splitter():
    text_splitter = TextSplitter(max_char=10, overlap=0)
    chunks = text_splitter.run("This is a test string")
    assert len(chunks) == 3
    assert chunks[0].text == "This is a "
    assert chunks[1].text == "test strin"
    assert chunks[2].text == "g"


def test_text_splitter_with_overlap():
    text_splitter = TextSplitter(max_char=10, overlap=2)
    chunks = text_splitter.run("This is a test string")
    assert len(chunks) == 3

    assert chunks[0].text == "This is a "
    assert chunks[0].metadata.get("start_char") == 0
    assert chunks[0].metadata.get("end_char") == 10
    assert chunks[1].text == "a test str"
    assert chunks[1].metadata.get("start_char") == 8
    assert chunks[1].metadata.get("end_char") == 18
    assert chunks[2].text == "tring"



================================================
FILE: datapizza-ai-core/datapizza/modules/treebuilder/__init__.py
================================================
from .llm_treebuilder import LLMTreeBuilder

__all__ = ["LLMTreeBuilder"]



================================================
FILE: datapizza-ai-core/datapizza/modules/treebuilder/llm_treebuilder.py
================================================
import logging
import re
from xml.etree import ElementTree as ET

from datapizza.core.clients.client import Client
from datapizza.type import Node, NodeType

log = logging.getLogger(__name__)
# Define the system prompt
SYSTEM_PROMPT = """You are an expert text structuring tool. Your task is to analyze the given text and structure it hierarchically into sections, paragraphs, and sentences.
Output the structured text using XML-like tags: `<document>`, `<section>`, `<paragraph>`, and `<sentence>`.
Ensure all original text content is preserved within the innermost tags (sentences). Do not add any explanations or introductory text outside the main <document> tag.

Example Input:
# My Title
This is the first paragraph. It has two sentences.
This is the second paragraph.

Example Output:
<document>
<section>
<paragraph>
<sentence># My Title</sentence>
<sentence>This is the first paragraph.</sentence>
<sentence>It has two sentences.</sentence>
</paragraph>
<paragraph>
<sentence>This is the second paragraph.</sentence>
</paragraph>
</section>
</document>
"""


class LLMTreeBuilder:
    """
    TreeBuilder that creates a hierarchical tree structure from text input using an LLM.
    The hierarchy goes from document -> sections -> paragraphs -> sentences.

    params:
        client: Client - An instance of an LLM client (e.g., GeminiClient)
    """

    def __init__(
        self,
        client: Client,
        system_prompt: str | None = None,
    ):
        if not isinstance(client, Client):
            raise TypeError(
                "client must be an instance of datapizza.clients.client.Client"
            )
        self.client = client
        self.system_prompt = system_prompt or SYSTEM_PROMPT

    def parse(self, text: str) -> Node:
        """
        Build a tree from the input text using an LLM.

        Args:
            text: Input text to process

        Returns:
            A Node representing the document with hierarchical structure
        """
        if not text.strip():
            # Return an empty document node if the input text is empty
            return Node(node_type=NodeType.DOCUMENT, metadata={"source": "text_input"})

        # Call the LLM to get the structured text
        try:
            structured_text = self.client.invoke(
                input=text, system_prompt=self.system_prompt
            ).text

            if not structured_text:
                raise ValueError("LLM returned an empty structure.")

        except Exception as e:
            # Fallback or error handling: Treat the whole text as a single paragraph/sentence document
            log.error(
                f"Error calling LLM or parsing response: {e}. Falling back to basic structure."
            )
            sentence_node = Node(node_type=NodeType.SENTENCE, content=text.strip())
            paragraph_node = Node(
                node_type=NodeType.PARAGRAPH, children=[sentence_node]
            )
            section_node = Node(node_type=NodeType.SECTION, children=[paragraph_node])
            return Node(
                node_type=NodeType.DOCUMENT,
                children=[section_node],
                metadata={"source": "text_input", "llm_fallback": True},
            )

        # Parse the XML-like structure from the LLM response
        try:
            # Attempt to clean potential LLM artifacts before parsing
            clean_xml = self._clean_llm_output(structured_text)
            root_element = ET.fromstring(clean_xml)

            document_node = self._parse_element(root_element)
            # Add source metadata to the root document node
            if not document_node.metadata:
                document_node.metadata = {}
            document_node.metadata["source"] = "text_input"
            document_node.metadata["llm_structured"] = True

            return document_node
        except ET.ParseError as e:
            # Handle cases where the LLM output is not valid XML even after cleaning
            log.error(
                f"XML parsing failed after cleaning: {e}. Falling back to basic structure."
            )
            # Fallback: Treat the whole original text as a single paragraph/sentence document
            sentence_node = Node(node_type=NodeType.SENTENCE, content=text.strip())
            paragraph_node = Node(
                node_type=NodeType.PARAGRAPH, children=[sentence_node]
            )
            section_node = Node(node_type=NodeType.SECTION, children=[paragraph_node])
            return Node(
                node_type=NodeType.DOCUMENT,
                children=[section_node],
                metadata={
                    "source": "text_input",
                    "llm_fallback": True,
                    "parse_error": str(e),
                },
            )
        except Exception as e:  # Catch other potential errors during node creation
            log.error(
                f"Error processing LLM structure: {e}. Falling back to basic structure."
            )
            # Fallback: Treat the whole original text as a single paragraph/sentence document
            sentence_node = Node(node_type=NodeType.SENTENCE, content=text.strip())
            paragraph_node = Node(
                node_type=NodeType.PARAGRAPH, children=[sentence_node]
            )
            section_node = Node(node_type=NodeType.SECTION, children=[paragraph_node])
            return Node(
                node_type=NodeType.DOCUMENT,
                children=[section_node],
                metadata={
                    "source": "text_input",
                    "llm_fallback": True,
                    "processing_error": str(e),
                },
            )

    def _parse_element(self, element: ET.Element) -> Node:
        """Recursively parse an XML element into a Node."""
        tag_map = {
            "document": NodeType.DOCUMENT,
            "section": NodeType.SECTION,
            "paragraph": NodeType.PARAGRAPH,
            "sentence": NodeType.SENTENCE,
        }
        node_type = tag_map.get(element.tag.lower())

        if node_type is None:
            # If the tag is unrecognized, treat it as a sentence containing its text
            # This handles potential unexpected tags from the LLM
            print(f"Warning: Unrecognized tag '{element.tag}'. Treating as sentence.")
            content = (element.text or "").strip()
            for child in (
                element
            ):  # Also capture text from child elements if any unexpected nesting occurs
                content += (child.text or "").strip() + (child.tail or "").strip()
            return Node(node_type=NodeType.SENTENCE, content=content)

        if node_type == NodeType.SENTENCE:
            # Leaf node: extract text content
            content = (element.text or "").strip()
            # Also capture text after potential nested tags if any (shouldn't happen with prompt)
            for child in element:
                content += (child.tail or "").strip()
            return Node(node_type=node_type, content=content)
        else:
            # Internal node: recursively parse children
            # Only process child elements, ignore text directly within this node or tails
            children = [
                self._parse_element(child) for child in element if child.tag in tag_map
            ]  # Filter out unrecognized tags if necessary
            # Filter out any None results if _parse_element potentially returns None (though current logic shouldn't)
            children = [child for child in children if child is not None]
            return Node(node_type=node_type, children=children)

    def _clean_llm_output(self, xml_string: str) -> str:
        """Basic cleaning of LLM output to improve XML parsing robustness."""
        # Remove potential markdown code blocks ```xml ... ```
        xml_string = re.sub(
            r"```xml\\s*(.*?)\\s*```",
            r"\\1",
            xml_string,
            flags=re.DOTALL | re.IGNORECASE,
        )
        # Remove potential leading/trailing whitespace or explanations outside the root tag
        match = re.search(
            r"<document>.*</document>", xml_string, re.DOTALL | re.IGNORECASE
        )
        cleaned_xml = (
            match.group(0).strip() if match else xml_string.strip()
        )  # Fallback if no document tag found

        # Escape special XML characters in the text content
        # Use a function to avoid replacing entities that might already be correct
        def escape_entities(text):
            text = text.replace("&", "&amp;")  # Must be first
            text = text.replace("<", "&lt;")
            text = text.replace(">", "&gt;")
            return text

        # Apply escaping only to text content, not tags
        # This is a simplified approach; a proper XML parser would handle this better,
        # but ET.fromstring needs valid input first.
        # We split by tags and escape content in between.
        parts = re.split(r"(<[^>]+>)", cleaned_xml)
        escaped_parts = []
        for i, part in enumerate(parts):
            if i % 2 == 0:  # Text content (even indices)
                escaped_parts.append(escape_entities(part))
            else:  # Tag (odd indices)
                escaped_parts.append(part)

        return "".join(escaped_parts)

    def invoke(self, file_path: str) -> Node:
        """
        Invoke the tree builder on the input text using an LLM.

        Args:
            file_path: Path to the file to process

        Returns:
            A Node representing the document with hierarchical structure
        """
        with open(file_path) as file:
            text = file.read()
        return self.parse(text)

    def __call__(self, file_path: str) -> Node:
        return self.invoke(file_path)

    def _run(self, file_path: str) -> Node:
        return self.invoke(file_path)

    async def _a_run(self, file_path: str) -> Node:
        raise NotImplementedError("LLMTreeBuilder does not support async operations")



================================================
FILE: datapizza-ai-core/datapizza/modules/treebuilder/test/test_llm_treebuilder.py
================================================
import re
from unittest.mock import MagicMock, PropertyMock

import pytest

from datapizza.core.clients import Client
from datapizza.modules.treebuilder.llm_treebuilder import LLMTreeBuilder

# Raw LLM output from the snippet file, including potential markdown fences
STRUCTURED_TEXT_FROM_FILE = """
```xml
<document>
<section>
<paragraph>
<sentence>ETF provider Betashares, which manages $30 billion in funds, reached an agreement to acquire Bendigo and Adelaide Bank's superannuation business, in its first venture into the superannuation sector.</sentence>
<sentence>Betashares said it was part of a longer-term strategy to expand the business into the broader financial sector.</sentence>
<sentence>Shares in Bendigo increased 0.6 per cent on the news.</sentence>
<sentence>REITS (up 0.4 per cent) was the strongest sector on the index as Goodman added 0.5 per cent and Dexus climbed 2.8 per cent.</sentence>
</paragraph>
<paragraph>
<sentence>The laggards Casino operator Star Entertainment Group's shares hit an all-time low of 60Â¢ after it raised $565 million.</sentence>
<sentence>They closed the session 16 per cent weaker at 63Â¢.</sentence>
<sentence>Star, which raised $800 million in February, has had to return to the market for fresh funding and is hoping to raise $750 million at a share price of 60Â¢ a share.</sentence>
</paragraph>
<paragraph>
<sentence>Meanwhile, healthcare heavyweight CSL shed 1.4 per cent, weighing down the healthcare sector and insurance companies IAG (down 2.6 per cent) and Suncorp (down 2 per cent) gave back some of their gains from Tuesday.</sentence>
<sentence>Gold miners Newcrest (down 2.1 per cent) and Evolution (down 3.5 per cent) were also among the biggest large-cap decliners after the spot gold price dropped 0.9 per cent overnight.</sentence>
<sentence>Information technology (down 1.1 per cent) was the weakest sector on the local bourse with WiseTech losing 1.4 per cent.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>The lowdown</sentence>
<sentence>Novus Capital senior client adviser Gary Glover said the Australian sharemarket was surprisingly resilient following a negative lead from Wall Street and the latest inflation data, with markets starting to wake up to the fact that interest rates could stay higher for longer.</sentence>
<sentence>"Considering the damage overnight in the US, Australian markets held on pretty well," he said.</sentence>
<sentence>"I thought it would be a bigger down day across the board."</sentence>
</paragraph>
<paragraph>
<sentence>Glover said the market was volatile but quite range-bound, similar to previous periods of high inflation in the 1940s and 1970s.</sentence>
<sentence>Elsewhere, Wall Street's ugly September got even worse on Tuesday, as a sharp drop for stocks brought them back to where they were in June.</sentence>
</paragraph>
<paragraph>
<sentence>The S&P 500 tumbled 1.5 per cent for its fifth loss in the last six days.</sentence>
<sentence>The Dow Jones dropped 1.1 per cent, and the Nasdaq composite lost 1.6 per cent.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>Loading September has brought a loss of 5.2 per cent so far for the S&P 500, putting it on track to be the worst month of the year by far, as the realisation sets in that the Federal Reserve will indeed keep interest rates high for a long time.</sentence>
<sentence>That growing understanding has sent yields in the bond market to their highest levels in more than a decade, which in turn has undercut prices for stocks and other investments.</sentence>
<sentence>Treasury yields rose again on Tuesday following a mixed batch of reports on the economy.</sentence>
<sentence>The yield on the 10-year Treasury edged up to 4.55 per cent from 4.54 per cent late on Monday and is near its highest level since 2007.</sentence>
<sentence>It's up sharply from about 3.5 per cent in May and from 0.5 per cent about three years ago.</sentence>
</paragraph>
<paragraph>
<sentence>One economic report on Tuesday showed confidence among consumers was weaker than economists expected.</sentence>
<sentence>That's concerning because strong spending by US households has been a bulwark keeping the economy out of a long-predicted recession.</sentence>
</paragraph>
<paragraph>
<sentence>Besides high interest rates, a long list of other worries is also tugging at Wall Street.</sentence>
<sentence>The most immediate is the threat of another US government shutdown as Capitol Hill threatens a stalemate that could shut off federal services across the country.</sentence>
</paragraph>
<paragraph>
<sentence>Loading Wall Street has dealt with such shutdowns in the past, and stocks have historically been turbulent in the run-up to them, according to Lori Calvasina, strategist at RBC Capital Markets.</sentence>
<sentence>After looking at the seven shutdowns that lasted 10 days or more since the 1970s, she found the S&P 500 dropped an average of roughly 10 per cent in the three months heading into them.</sentence>
<sentence>But stocks managed to hold up rather well during the shutdowns, falling an average of just 0.3 per cent, before rebounding meaningfully afterward.</sentence>
</paragraph>
<paragraph>
<sentence>Wall Street is also contending with higher oil prices, shaky economies around the world, a strike by US autoworkers that could put more upward pressure on inflation and a resumption of US student-loan repayments that could dent spending by households.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>On Wall Street, the vast majority of stocks fell under such pressures, including 90 per cent of those within the S&P 500.</sentence>
<sentence>Big Tech stocks tend to be among the hardest hit by high rates, and they were the heaviest weights on the index.</sentence>
<sentence>Apple fell 2.3 per cent and Microsoft lost 1.7 per cent.</sentence>
</paragraph>
<paragraph>
<sentence>Amazon tumbled 4 per cent after the Federal Trade Commission and 17 state attorneys general filed an antitrust lawsuit against it.</sentence>
<sentence>They accuse the e-commerce behemoth of using its dominant position to inflate prices on other platforms, overcharge sellers and stifle competition.</sentence>
</paragraph>
<paragraph>
<sentence>In China, concerns continued over heavily indebted real estate developer Evergrande.</sentence>
<sentence>The property market crisis there is dragging on China's economic growth and raising worries about financial instability.</sentence>
<sentence>France's CAC 40 fell 0.7 per cent, and Germany's DAX lost 1 per cent.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>Crude oil prices rose, adding to worries about inflation.</sentence>
<sentence>A barrel of benchmark US crude climbed 71Â¢ to $US90.39.</sentence>
<sentence>Brent crude, the international standard, added 67Â¢ to $US93.96 per barrel.</sentence>
</paragraph>
<paragraph>
<sentence>Tweet of the day</sentence>
<sentence>Quote of the day</sentence>
<sentence>"The Senate committees have the power to summons witnesses within Australia but have no enforceable powers for witnesses who are overseas," said Senator Bridget McKenzie as former Qantas boss Alan Joyce chose not to front the Senate select committee into the federal government's decision to reject extra flights from Qatar Airways due to "personal commitments".</sentence>
</paragraph>
</section>
</document>
```
"""

# Expected clean XML after _clean_llm_output
EXPECTED_CLEAN_XML = """<document>
<section>
<paragraph>
<sentence>ETF provider Betashares, which manages $30 billion in funds, reached an agreement to acquire Bendigo and Adelaide Bank's superannuation business, in its first venture into the superannuation sector.</sentence>
<sentence>Betashares said it was part of a longer-term strategy to expand the business into the broader financial sector.</sentence>
<sentence>Shares in Bendigo increased 0.6 per cent on the news.</sentence>
<sentence>REITS (up 0.4 per cent) was the strongest sector on the index as Goodman added 0.5 per cent and Dexus climbed 2.8 per cent.</sentence>
</paragraph>
<paragraph>
<sentence>The laggards Casino operator Star Entertainment Group's shares hit an all-time low of 60Â¢ after it raised $565 million.</sentence>
<sentence>They closed the session 16 per cent weaker at 63Â¢.</sentence>
<sentence>Star, which raised $800 million in February, has had to return to the market for fresh funding and is hoping to raise $750 million at a share price of 60Â¢ a share.</sentence>
</paragraph>
<paragraph>
<sentence>Meanwhile, healthcare heavyweight CSL shed 1.4 per cent, weighing down the healthcare sector and insurance companies IAG (down 2.6 per cent) and Suncorp (down 2 per cent) gave back some of their gains from Tuesday.</sentence>
<sentence>Gold miners Newcrest (down 2.1 per cent) and Evolution (down 3.5 per cent) were also among the biggest large-cap decliners after the spot gold price dropped 0.9 per cent overnight.</sentence>
<sentence>Information technology (down 1.1 per cent) was the weakest sector on the local bourse with WiseTech losing 1.4 per cent.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>The lowdown</sentence>
<sentence>Novus Capital senior client adviser Gary Glover said the Australian sharemarket was surprisingly resilient following a negative lead from Wall Street and the latest inflation data, with markets starting to wake up to the fact that interest rates could stay higher for longer.</sentence>
<sentence>"Considering the damage overnight in the US, Australian markets held on pretty well," he said.</sentence>
<sentence>"I thought it would be a bigger down day across the board."</sentence>
</paragraph>
<paragraph>
<sentence>Glover said the market was volatile but quite range-bound, similar to previous periods of high inflation in the 1940s and 1970s.</sentence>
<sentence>Elsewhere, Wall Street's ugly September got even worse on Tuesday, as a sharp drop for stocks brought them back to where they were in June.</sentence>
</paragraph>
<paragraph>
<sentence>The S&amp;P 500 tumbled 1.5 per cent for its fifth loss in the last six days.</sentence>
<sentence>The Dow Jones dropped 1.1 per cent, and the Nasdaq composite lost 1.6 per cent.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>Loading September has brought a loss of 5.2 per cent so far for the S&amp;P 500, putting it on track to be the worst month of the year by far, as the realisation sets in that the Federal Reserve will indeed keep interest rates high for a long time.</sentence>
<sentence>That growing understanding has sent yields in the bond market to their highest levels in more than a decade, which in turn has undercut prices for stocks and other investments.</sentence>
<sentence>Treasury yields rose again on Tuesday following a mixed batch of reports on the economy.</sentence>
<sentence>The yield on the 10-year Treasury edged up to 4.55 per cent from 4.54 per cent late on Monday and is near its highest level since 2007.</sentence>
<sentence>It's up sharply from about 3.5 per cent in May and from 0.5 per cent about three years ago.</sentence>
</paragraph>
<paragraph>
<sentence>One economic report on Tuesday showed confidence among consumers was weaker than economists expected.</sentence>
<sentence>That's concerning because strong spending by US households has been a bulwark keeping the economy out of a long-predicted recession.</sentence>
</paragraph>
<paragraph>
<sentence>Besides high interest rates, a long list of other worries is also tugging at Wall Street.</sentence>
<sentence>The most immediate is the threat of another US government shutdown as Capitol Hill threatens a stalemate that could shut off federal services across the country.</sentence>
</paragraph>
<paragraph>
<sentence>Loading Wall Street has dealt with such shutdowns in the past, and stocks have historically been turbulent in the run-up to them, according to Lori Calvasina, strategist at RBC Capital Markets.</sentence>
<sentence>After looking at the seven shutdowns that lasted 10 days or more since the 1970s, she found the S&amp;P 500 dropped an average of roughly 10 per cent in the three months heading into them.</sentence>
<sentence>But stocks managed to hold up rather well during the shutdowns, falling an average of just 0.3 per cent, before rebounding meaningfully afterward.</sentence>
</paragraph>
<paragraph>
<sentence>Wall Street is also contending with higher oil prices, shaky economies around the world, a strike by US autoworkers that could put more upward pressure on inflation and a resumption of US student-loan repayments that could dent spending by households.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>On Wall Street, the vast majority of stocks fell under such pressures, including 90 per cent of those within the S&amp;P 500.</sentence>
<sentence>Big Tech stocks tend to be among the hardest hit by high rates, and they were the heaviest weights on the index.</sentence>
<sentence>Apple fell 2.3 per cent and Microsoft lost 1.7 per cent.</sentence>
</paragraph>
<paragraph>
<sentence>Amazon tumbled 4 per cent after the Federal Trade Commission and 17 state attorneys general filed an antitrust lawsuit against it.</sentence>
<sentence>They accuse the e-commerce behemoth of using its dominant position to inflate prices on other platforms, overcharge sellers and stifle competition.</sentence>
</paragraph>
<paragraph>
<sentence>In China, concerns continued over heavily indebted real estate developer Evergrande.</sentence>
<sentence>The property market crisis there is dragging on China's economic growth and raising worries about financial instability.</sentence>
<sentence>France's CAC 40 fell 0.7 per cent, and Germany's DAX lost 1 per cent.</sentence>
</paragraph>
</section>
<section>
<paragraph>
<sentence>Crude oil prices rose, adding to worries about inflation.</sentence>
<sentence>A barrel of benchmark US crude climbed 71Â¢ to $US90.39.</sentence>
<sentence>Brent crude, the international standard, added 67Â¢ to $US93.96 per barrel.</sentence>
</paragraph>
<paragraph>
<sentence>Tweet of the day</sentence>
<sentence>Quote of the day</sentence>
<sentence>"The Senate committees have the power to summons witnesses within Australia but have no enforceable powers for witnesses who are overseas," said Senator Bridget McKenzie as former Qantas boss Alan Joyce chose not to front the Senate select committee into the federal government's decision to reject extra flights from Qatar Airways due to "personal commitments".</sentence>
</paragraph>
</section>
</document>"""


REJOINED_TEXT = """ETF provider Betashares, which manages $30 billion in funds, reached an agreement to acquire Bendigo and Adelaide Bank\u2019s superannuation business, in its first venture into the superannuation sector. Betashares said it was part of a longer-term strategy to expand the business into the broader financial sector. Shares in Bendigo increased 0.6 per cent on the news. REITS (up 0.4 per cent) was the strongest sector on the index as Goodman added 0.5 per cent and Dexus climbed 2.8 per cent. The laggards Casino operator Star Entertainment Group\u2019s shares hit an all-time low of 60\u00a2 after it raised $565 million. They closed the session 16 per cent weaker at 63\u00a2. Star, which raised $800 million in February, has had to return to the market for fresh funding and is hoping to raise $750 million at a share price of 60\u00a2 a share.\n\nMeanwhile, healthcare heavyweight CSL shed 1.4 per cent, weighing down the healthcare sector and insurance companies IAG (down 2.6 per cent) and Suncorp (down 2 per cent) gave back some of their gains from Tuesday. Gold miners Newcrest (down 2.1 per cent) and Evolution (down 3.5 per cent) were also among the biggest large-cap decliners after the spot gold price dropped 0.9 per cent overnight. Information technology (down 1.1 per cent) was the weakest sector on the local bourse with WiseTech losing 1.4 per cent. The lowdown\n\nNovus Capital senior client adviser Gary Glover said the Australian sharemarket was surprisingly resilient following a negative lead from Wall Street and the latest inflation data, with markets starting to wake up to the fact that interest rates could stay higher for longer. \u201cConsidering the damage overnight in the US, Australian markets held on pretty well,\u201d he said. \u201cI thought it would be a bigger down day across the board.\u201d Glover said the market was volatile but quite range-bound, similar to previous periods of high inflation in the 1940s and 1970s. Elsewhere, Wall Street\u2019s ugly September got even worse on Tuesday, as a sharp drop for stocks brought them back to where they were in June. The S&P 500 tumbled 1.5 per cent for its fifth loss in the last six days. The Dow Jones dropped 1.1 per cent, and the Nasdaq composite lost 1.6 per cent.\n\nLoading September has brought a loss of 5.2 per cent so far for the S&P 500, putting it on track to be the worst month of the year by far, as the realisation sets in that the Federal Reserve will indeed keep interest rates high for a long time. That growing understanding has sent yields in the bond market to their highest levels in more than a decade, which in turn has undercut prices for stocks and other investments. Treasury yields rose again on Tuesday following a mixed batch of reports on the economy. The yield on the 10-year Treasury edged up to 4.55 per cent from 4.54 per cent late on Monday and is near its highest level since 2007. It\u2019s up sharply from about 3.5 per cent in May and from 0.5 per cent about three years ago. One economic report on Tuesday showed confidence among consumers was weaker than economists expected. That\u2019s concerning because strong spending by US households has been a bulwark keeping the economy out of a long-predicted recession.\n\nBesides high interest rates, a long list of other worries is also tugging at Wall Street. The most immediate is the threat of another US government shutdown as Capitol Hill threatens a stalemate that could shut off federal services across the country. Loading Wall Street has dealt with such shutdowns in the past, and stocks have historically been turbulent in the run-up to them, according to Lori Calvasina, strategist at RBC Capital Markets. After looking at the seven shutdowns that lasted 10 days or more since the 1970s, she found the S&P 500 dropped an average of roughly 10 per cent in the three months heading into them. But stocks managed to hold up rather well during the shutdowns, falling an average of just 0.3 per cent, before rebounding meaningfully afterward. Wall Street is also contending with higher oil prices, shaky economies around the world, a strike by US autoworkers that could put more upward pressure on inflation and a resumption of US student-loan repayments that could dent spending by households.\n\nOn Wall Street, the vast majority of stocks fell under such pressures, including 90 per cent of those within the S&P 500. Big Tech stocks tend to be among the hardest hit by high rates, and they were the heaviest weights on the index. Apple fell 2.3 per cent and Microsoft lost 1.7 per cent. Amazon tumbled 4 per cent after the Federal Trade Commission and 17 state attorneys general filed an antitrust lawsuit against it. They accuse the e-commerce behemoth of using its dominant position to inflate prices on other platforms, overcharge sellers and stifle competition. In China, concerns continued over heavily indebted real estate developer Evergrande. The property market crisis there is dragging on China\u2019s economic growth and raising worries about financial instability. France\u2019s CAC 40 fell 0.7 per cent, and Germany\u2019s DAX lost 1 per cent.\n\nCrude oil prices rose, adding to worries about inflation. A barrel of benchmark US crude climbed 71\u00a2 to $US90.39. Brent crude, the international standard, added 67\u00a2 to $US93.96 per barrel. Tweet of the day Quote of the day \u201cThe Senate committees have the power to summons witnesses within Australia but have no enforceable powers for witnesses who are overseas,\u201d said Senator Bridget McKenzie as former Qantas boss Alan Joyce chose not to front the Senate select committee into the federal government\u2019s decision to reject extra flights from Qatar Airways due to \u201cpersonal commitments\u201d."""


@pytest.fixture
def mock_llm_client() -> MagicMock:
    """Fixture to create a mock LLM client."""
    client = MagicMock(spec=Client)
    # Mock the 'invoke' method to return a MagicMock
    mock_response = MagicMock()
    type(mock_response).text = PropertyMock(return_value=STRUCTURED_TEXT_FROM_FILE)
    client.invoke.return_value = mock_response
    return client


def are_equal_ignore_whitespace_punctuation_case(str1, str2):
    cleaned_str1 = re.sub(r"[^A-Za-z0-9]+", "", str1).lower()
    cleaned_str2 = re.sub(r"[^A-Za-z0-9]+", "", str2).lower()
    return cleaned_str1 == cleaned_str2


def test_clean_llm_output(mock_llm_client):
    """Test the _clean_llm_output method directly."""
    builder = LLMTreeBuilder(client=mock_llm_client)
    cleaned_output = builder._clean_llm_output(STRUCTURED_TEXT_FROM_FILE)

    # Check if markdown fences are removed
    assert "```xml" not in cleaned_output
    assert "```" not in cleaned_output

    # Check if leading/trailing whitespace is removed (relative to <document> tags)
    assert cleaned_output.startswith("<document>")
    assert cleaned_output.endswith("</document>")

    # Check if specific entity is correctly escaped (e.g., S&P 500)
    assert "<sentence>The S&amp;P 500 tumbled 1.5 per cent" in cleaned_output

    # Very basic check to ensure it still looks like XML
    assert "<section>" in cleaned_output
    assert "<paragraph>" in cleaned_output
    assert "<sentence>" in cleaned_output


def test_build_tree_with_mock_llm_output(mock_llm_client):
    """Test build_tree using the mocked LLM output."""
    builder = LLMTreeBuilder(client=mock_llm_client)
    input_text = "This is some dummy input text."  # Actual input doesn't matter here
    root_node = builder.parse(input_text)

    assert are_equal_ignore_whitespace_punctuation_case(
        root_node.content, REJOINED_TEXT
    )



================================================
FILE: datapizza-ai-core/datapizza/pipeline/__init__.py
================================================
from .dag_pipeline import DagPipeline
from .functional_pipeline import Dependency, FunctionalPipeline
from .pipeline import IngestionPipeline

__all__ = ["DagPipeline", "Dependency", "FunctionalPipeline", "IngestionPipeline"]



================================================
FILE: datapizza-ai-core/datapizza/pipeline/dag_pipeline.py
================================================
import logging
from copy import deepcopy
from dataclasses import dataclass

from datapizza.core.models import ChainableProducer, PipelineComponent
from datapizza.core.utils import replace_env_vars

log = logging.getLogger(__name__)


@dataclass
class Edge:
    from_node_name: str
    to_node_name: str
    src_key: str | None
    dst_key: str


class DagPipeline:
    """
    A pipeline that runs a graph of a dependency graph.
    """

    def __init__(self):
        self.nodes: dict[str, PipelineComponent] = {}
        self.edges = []

    # get the nodes that depend on the given node
    def _get_edges_from(self, node_name: str) -> list[Edge]:
        return [d for d in self.edges if d.from_node_name == node_name]

    # get the nodes that the given node depends on
    def _get_edges_to(self, node_name: str) -> list[Edge]:
        return [d for d in self.edges if d.to_node_name == node_name]

    def add_module(self, node_name: str, node: PipelineComponent):
        """
        Add a module to the pipeline.

        Args:
            node_name (str): The name of the module.
            node (PipelineComponent): The module to add.
        """
        # Nodes must be ChainableProducer or PipelineComponent or callable
        if isinstance(node, ChainableProducer):
            module_component = node.as_module_component()
            self.nodes[node_name] = module_component

        elif isinstance(node, PipelineComponent) or callable(node):
            self.nodes[node_name] = node
        else:
            raise ValueError(
                f"Node {node_name} must be a ChainableProducer, PipelineComponent, or callable."
            )

    def connect(
        self,
        source_node: str,
        target_node: str,
        target_key: str,
        source_key: str | None = None,
    ):
        """
        Connect two nodes in the pipeline.

        Args:
            source_node (str): The name of the source node.
            target_node (str): The name of the target node.
            target_key (str): The key to store the result of the target node in the source node.
            source_key (str, optional): The key to retrieve the result of the source node from the target node. Defaults to None.
        """
        self.edges.append(
            Edge(
                from_node_name=source_node,
                to_node_name=target_node,
                src_key=source_key,
                dst_key=target_key,
            )
        )

    def _get_nodes_ready_to_run(self, results: dict):
        ready_nodes = []
        for node_name in self.nodes:
            # continue if node is already processed
            if node_name in results:
                continue

            # If node has no dependencies, it's ready to be processed
            if not self._get_edges_to(node_name):
                ready_nodes.append(node_name)
                continue

            # Check if all dependencies are resolved
            dependencies = [d.from_node_name for d in self._get_edges_to(node_name)]
            if all(dep in results for dep in dependencies):
                ready_nodes.append(node_name)

        return ready_nodes

    def _get_args_for_node(self, node_name: str, _input: dict, results: dict) -> dict:
        # Get all the nodes that must be run before the given node
        # get the results from the previous nodes and return them as a dict

        previous_nodes = self._get_edges_to(node_name)
        # Start with the original input
        args = deepcopy(_input.get(node_name, {}))

        for edge in previous_nodes:
            from_node_result = results[edge.from_node_name]

            # Extract the correct value from source
            if edge.src_key:
                value = from_node_result.get(edge.src_key)
                if value is None and edge.src_key not in from_node_result:
                    log.warning(
                        f"Source key '{edge.src_key}' not found in result of node '{edge.from_node_name}'. Using None."
                    )

            else:
                value = from_node_result

            # Place it in the right location in args
            if edge.dst_key:
                args[edge.dst_key] = value  # TODO  deepcopy(value)
            else:
                raise ValueError(
                    f"No destination key provided for node '{node_name}' from '{edge.from_node_name}'."
                )

        return args

    def run(self, data: dict) -> dict:
        """
        Run the pipeline.

        Args:
            data (dict): The input data to the pipeline.

        Returns:
            dict: The results of the pipeline.
        """
        processed_nodes = set()
        pipeline_results = {}
        queue = self._get_nodes_ready_to_run(pipeline_results)

        while queue:
            node_name = queue.pop(0)

            if node_name in processed_nodes:
                continue

            node = self.nodes[node_name]
            try:
                arguments = self._get_args_for_node(node_name, data, pipeline_results)
                log.debug(f"Arguments for node {node_name}: {list(arguments.keys())}")
                node_result = node(**arguments)
                pipeline_results[node_name] = node_result
                processed_nodes.add(node_name)

                newly_ready = self._get_nodes_ready_to_run(pipeline_results)
                for ready_node in newly_ready:
                    if ready_node not in processed_nodes and ready_node not in queue:
                        queue.append(ready_node)

            except Exception as e:
                log.error(f"Error running node {node_name}: {e!s}")
                raise

        if len(processed_nodes) < len(self.nodes):
            unprocessed = set(self.nodes.keys()) - processed_nodes
            log.warning(
                f"Not all nodes were processed. Unprocessed: {unprocessed}. Possible cycle or error."
            )
            # This could indicate a cycle in the graph or an earlier error

        return pipeline_results

    async def a_run(self, data: dict):
        """
        Run the pipeline asynchronously.

        Args:
            data (dict): The input data to the pipeline.

        Returns:
            dict: The results of the pipeline.
        """
        processed_nodes = set()
        pipeline_results = {}
        queue = self._get_nodes_ready_to_run(pipeline_results)

        while queue:
            node_name = queue.pop(0)

            if node_name in processed_nodes:
                continue

            node = self.nodes[node_name]
            try:
                arguments = self._get_args_for_node(node_name, data, pipeline_results)
                log.debug(f"Arguments for node {node_name}: {list(arguments.keys())}")
                node_result = await node.a_run(**arguments)
                pipeline_results[node_name] = node_result
                processed_nodes.add(node_name)

                newly_ready = self._get_nodes_ready_to_run(pipeline_results)
                for ready_node in newly_ready:
                    if ready_node not in processed_nodes and ready_node not in queue:
                        queue.append(ready_node)

            except Exception as e:
                log.error(f"Error running node {node_name}: {e!s}")
                raise

        if len(processed_nodes) < len(self.nodes):
            unprocessed = set(self.nodes.keys()) - processed_nodes
            log.warning(
                f"Not all nodes were processed. Unprocessed: {unprocessed}. Possible cycle or error."
            )

        return pipeline_results

    def from_yaml(self, config_path: str) -> "DagPipeline":
        """
        Load the pipeline from a YAML configuration file.

        Args:
            config_path (str): Path to the YAML configuration file.

        Returns:
            DagPipeline: The pipeline instance.
        """
        import importlib

        import yaml

        from datapizza.clients import ClientFactory

        with open(config_path) as file:
            config = yaml.safe_load(file)

        constants = config.get("constants", [])
        config = replace_env_vars(config, constants)

        dag_pipeline = config["dag_pipeline"]
        clients = {}

        if "clients" in dag_pipeline:
            for client_name, client_config in dag_pipeline["clients"].items():
                provider = client_config.pop("provider")
                client = ClientFactory.create(provider, **client_config)
                clients[client_name] = client

        self.nodes = {}
        self.edges = []

        if "modules" in dag_pipeline:
            for module_config in dag_pipeline["modules"]:
                try:
                    module_name = module_config["name"]
                    module_path = module_config["module"]
                    module_import = importlib.import_module(module_path)
                    class_ = getattr(module_import, module_config["type"])

                    params = module_config.get("params", {})

                    if "client" in params:
                        client_name = params["client"]
                        if client_name not in clients:
                            raise ValueError(
                                f"Client '{client_name}' not found in clients configuration for node {module_name}"
                            )
                        params["client"] = clients[client_name]

                    component_instance = class_(**params)
                    self.add_module(module_name, component_instance)
                except (ImportError, AttributeError) as e:
                    raise ValueError(
                        f"Could not load module {module_config.get('type', 'N/A')} for node {module_config.get('name', 'N/A')}: {e!s}"
                    ) from e
                except KeyError as e:
                    raise ValueError(
                        f"Missing required key {e!s} in module configuration: {module_config}"
                    ) from e

        if "connections" in dag_pipeline:
            for connection in dag_pipeline["connections"]:
                from_node = connection["from"]
                to_node = connection["to"]
                if from_node not in self.nodes:
                    raise ValueError(
                        f"Source node '{from_node}' for connection not found in loaded modules."
                    )
                if to_node not in self.nodes:
                    raise ValueError(
                        f"Target node '{to_node}' for connection not found in loaded modules."
                    )

                source_key = connection.get("source_key")
                target_key = connection.get("target_key")
                self.connect(
                    from_node,
                    to_node,
                    target_key=target_key,
                    source_key=source_key,
                )

        return self



================================================
FILE: datapizza-ai-core/datapizza/pipeline/functional_pipeline.py
================================================
import copy
import importlib
import logging
from collections.abc import Callable
from dataclasses import dataclass
from typing import Any

import yaml
from opentelemetry import trace

from datapizza.core.models import PipelineComponent
from datapizza.core.utils import replace_env_vars

tracer = trace.get_tracer(__name__)
log = logging.getLogger(__name__)


@dataclass
class Dependency:
    """Dependency for a node."""

    node_name: str
    input_key: str | None = None
    target_key: str | None = None


class FunctionalPipeline:
    """Pipeline for executing a series of nodes with dependencies."""

    def __init__(self):
        self.nodes = []

    def run(
        self,
        name: str,
        node: PipelineComponent,
        dependencies: list[Dependency] | None = None,
        kwargs: dict[str, Any] | None = None,
    ) -> "FunctionalPipeline":
        """
        Add a node to the pipeline with optional dependencies.

        Args:
            name (str): The name of the node.
            node (PipelineComponent): The node to add.
            dependencies (list[Dependency], optional): List of dependencies for the node. Defaults to None.
            kwargs (dict[str, Any], optional): Additional keyword arguments to pass to the node. Defaults to None.

        Returns:
            FunctionalPipeline: The pipeline instance.
        """
        if dependencies is None:
            dependencies = []
        if kwargs is None:
            kwargs = {}

        self.nodes.append(
            {
                "name": name,
                "node": node,
                "dependencies": dependencies,
                "type": "node",
                "kwargs": kwargs,
            }
        )
        return self

    def then(
        self,
        name: str,
        node: PipelineComponent,
        target_key: str,
        dependencies: list[Dependency] | None = None,
        kwargs: dict[str, Any] | None = None,
    ) -> "FunctionalPipeline":
        """
        Add a node to execute after the previous node.

        Args:
            name (str): The name of the node.
            node (PipelineComponent): The node to add.
            target_key (str): The key to store the result of the node in the previous node.
            dependencies (list[Dependency], optional): List of dependencies for the node. Defaults to None.
            kwargs (dict[str, Any], optional): Additional keyword arguments to pass to the node. Defaults to None.

        Returns:
            FunctionalPipeline: The pipeline instance.
        """
        deps = []
        prev_node = self.nodes[-1]
        deps = [Dependency(node_name=prev_node["name"], target_key=target_key)]
        if dependencies:
            deps.extend(dependencies)

        return self.run(name, node, deps, kwargs)

    def foreach(
        self,
        name: str,
        do: "PipelineComponent | FunctionalPipeline",
        dependencies: list[Dependency] | None = None,
    ) -> "FunctionalPipeline":
        """
        Execute a sub-pipeline for each item in a collection.

        Args:
            name (str): The name of the node.
            do (PipelineComponent): The sub-pipeline to execute for each item.
            dependencies (list[Dependency], optional): List of dependencies for the node. Defaults to None.

        Returns:
            FunctionalPipeline: The pipeline instance.

        """
        if dependencies is None:
            dependencies = []

        if isinstance(dependencies, Dependency):
            dependencies = [dependencies]

        if not isinstance(do, PipelineComponent):
            raise TypeError("do must be a PipelineComponent")

        self.nodes.append(
            {
                "name": name,
                "do": do,
                "dependencies": dependencies,
                "type": "foreach",
            }
        )
        return self

    def branch(
        self,
        condition: Callable,
        if_true: "FunctionalPipeline",
        if_false: "FunctionalPipeline",
        dependencies: list[Dependency] | None = None,
    ) -> "FunctionalPipeline":
        """
        Branch execution based on a condition.

        Args:
            condition (Callable): The condition to evaluate.
            if_true (FunctionalPipeline): The pipeline to execute if the condition is True.
            if_false (FunctionalPipeline): The pipeline to execute if the condition is False.
            dependencies (list[Dependency], optional): List of dependencies for the node. Defaults to None.

        Returns:
            FunctionalPipeline: The pipeline instance.
        """
        if dependencies is None:
            dependencies = []

        self.nodes.append(
            {
                "condition": condition,
                "if_true": if_true,
                "if_false": if_false,
                "dependencies": dependencies,
                "type": "branch",
            }
        )
        return self

    def get(self, name: str) -> "FunctionalPipeline":
        """
        Get the result of a node.

        Args:
            name (str): The name of the node.

        Returns:
            FunctionalPipeline: The pipeline instance.
        """
        self.nodes.append({"get_name": name, "type": "get"})
        return self

    def _resolve_dependencies(self, node, context):
        """Resolve dependencies for a node."""
        inputs = {}
        dependencies = node.get("dependencies", [])
        for dep in dependencies:
            if dep.node_name in context:
                if dep.target_key:
                    inputs[dep.target_key] = context[dep.node_name]
                else:
                    if len(dependencies) > 1:
                        raise ValueError(
                            f"Target key is required for node {dep.node_name} because it has more than 1 dependencies"
                        )

                    return context[dep.node_name]

        return inputs

    @tracer.start_as_current_span("functional_pipeline.execute")
    def execute(
        self,
        initial_data: dict[str, Any] | None = None,
        context: dict | None = None,  # type: ignore
    ) -> dict[str, Any]:
        """Execute the pipeline and return the results.

        Args:
            initial_data: Dictionary where keys are node names and values are the data
                         to be passed to those nodes when they execute.
            context: Dictionary where keys are node names and values are the data
                     to be passed to those nodes when they execute.

        Returns:
            dict: The results of the pipeline.
        """
        context: dict = context or {}
        initial_data = initial_data or {}

        for node in self.nodes:
            node_type = node.get("type")

            if node_type == "node":
                # Get dependencies from context
                dep_inputs = self._resolve_dependencies(node, context)

                # Get initial data for this node if available
                node_input = initial_data.get(node["name"], {})

                # Merge dependency inputs with node-specific initial data
                # Node-specific initial data takes precedence
                inputs = (
                    {**dep_inputs, **node_input, **node["kwargs"]}
                    if isinstance(node_input, dict)
                    else node_input
                )

                # Execute the node
                result = node["node"].run(**inputs)

                # Store the result in context
                context[node["name"]] = result

            elif node_type == "foreach":
                node_name = node.get("name")
                to_do = node.get("do")

                # Get dependencies data
                dep_inputs = self._resolve_dependencies(
                    node, {**context, **initial_data}
                )

                # Get collection to iterate over
                collection = dep_inputs  # .get(node_name) or []

                if not isinstance(collection, list):
                    collection = [collection]

                results = []
                for item in collection:
                    log.debug(f"Executing {node_name} for item: {item}")
                    # Execute sub-pipeline with the prepared initial data

                    if isinstance(to_do, PipelineComponent):
                        item_result = to_do.run(item)

                    # item_result = node["do"].execute(
                    #     initial_data=initial_data,
                    #     context={**{node_name: item}},
                    # )
                    if isinstance(item_result, list):
                        results.extend(item_result)
                    else:
                        results.append(item_result)

                context[node["name"]] = results

            elif node_type == "branch":
                # Get dependencies data
                dep_inputs = self._resolve_dependencies(node, context)

                # Evaluate condition with context
                condition_result = node["condition"](context)

                # Pass initial data to the chosen branch
                branch_initial_data = initial_data.get("branch", context)

                if condition_result:
                    branch_result = node["if_true"].execute(branch_initial_data)
                else:
                    branch_result = node["if_false"].execute(branch_initial_data)

                # Merge branch results into context
                context.update(branch_result)

            elif node_type == "parallel":
                # Get dependencies data
                dep_inputs = self._resolve_dependencies(node, context)

                # Get initial data for parallel execution
                parallel_initial_data = initial_data.get("parallel", {})

                parallel_results = []
                for i, pipeline in enumerate(node["pipelines"]):
                    # Get pipeline-specific initial data
                    pipeline_initial_data = parallel_initial_data.get(str(i), {})

                    # Create a copy of the context for each parallel branch
                    branch_context = copy.deepcopy(pipeline_initial_data)
                    result = pipeline.execute(branch_context)
                    parallel_results.append(result)

                # Merge all results from parallel execution
                for i, result in enumerate(parallel_results):
                    key = f"parallel_{i}"
                    context[key] = result

            elif node_type == "get":
                context = context[node["get_name"]]

        return context

    @staticmethod
    def from_yaml(yaml_path: str) -> "FunctionalPipeline":
        """
        Constructs a FunctionalPipeline from a YAML configuration file.
        The YAML should contain 'modules' (optional) defining reusable components
        and 'pipeline' defining the sequence of steps.

        Args:
            yaml_path: Path to the YAML configuration file.

        Returns:
            A configured FunctionalPipeline instance.

        Raises:
            ValueError: If the YAML format is invalid, a module cannot be loaded,
                        or a referenced node/condition name is not found.
            KeyError: If a required key is missing in the YAML structure.
            FileNotFoundError: If the yaml_path does not exist.
            yaml.YAMLError: If the YAML file cannot be parsed.
            ImportError: If a specified module cannot be imported.
            AttributeError: If a specified class/function is not found in the module.
        """
        try:
            with open(yaml_path) as f:
                raw_config = yaml.safe_load(f)
        except FileNotFoundError:
            log.error(f"YAML file not found at path: {yaml_path}")
            raise
        except yaml.YAMLError as e:
            log.error(f"Error parsing YAML file {yaml_path}: {e}")
            raise ValueError(f"Invalid YAML format: {e}") from e

        # Process the entire config to replace environment variables
        config = replace_env_vars(raw_config)

        if not isinstance(config, dict):
            raise ValueError("YAML config must be a dictionary.")

        # --- Load Modules (Nodes and Callables) ---
        loaded_nodes: dict[str, PipelineComponent] = {}
        if "modules" in config:
            if not isinstance(config["modules"], list):
                raise ValueError("YAML 'modules' section must be a list.")

            for i, module_config in enumerate(config["modules"]):
                if not isinstance(module_config, dict):
                    raise ValueError(
                        f"Module definition {i + 1} is not a dictionary: {module_config}"
                    )
                try:
                    module_name = module_config["name"]
                    module_path = module_config["module"]
                    class_or_func_name = module_config["type"]
                    params = module_config.get("params", {})

                    imported_module = importlib.import_module(module_path)
                    class_or_func: PipelineComponent = getattr(
                        imported_module, class_or_func_name
                    )

                    # Process params - resolve node references
                    processed_params = FunctionalPipeline._process_params(
                        params, loaded_nodes
                    )

                    # Instantiate if it's a class, otherwise use the function directly
                    if isinstance(class_or_func, type):  # Check if it's a class type
                        # Ensure parameters are passed correctly
                        instance = class_or_func(**processed_params)
                        loaded_nodes[module_name] = instance
                    elif callable(class_or_func):  # Check if it's a callable (function)
                        # Functions typically don't take init params like classes
                        if processed_params:
                            log.warning(
                                f"Params provided for function '{class_or_func_name}' (module '{module_name}') but functions usually don't take init params. Ignoring params: {processed_params}"
                            )
                        loaded_nodes[module_name] = class_or_func
                    else:
                        raise TypeError(
                            f"Loaded object '{class_or_func_name}' from module '{module_path}' is neither a class nor a callable function."
                        )

                except KeyError as e:
                    raise ValueError(
                        f"Missing required key {e} in module definition {i + 1}: {module_config}"
                    ) from e
                except (ImportError, AttributeError) as e:
                    raise ValueError(
                        f"Could not load type '{module_config.get('type', 'N/A')}' from module '{module_config.get('module', 'N/A')}' for component '{module_config.get('name', 'N/A')}': {e}"
                    ) from e
                except Exception as e:
                    log.error(
                        f"Failed to load or instantiate module '{module_config.get('name', 'N/A')}': {e}"
                    )
                    raise  # Re-raise the exception

        # --- Build Pipeline ---
        if "pipeline" not in config:
            raise ValueError(
                "YAML config must have a top-level 'pipeline' key containing a list of steps."
            )

        pipeline_config = config["pipeline"]
        if not isinstance(pipeline_config, list):
            raise ValueError("The 'pipeline' key must contain a list of steps.")

        # Call the helper method to build the pipeline using loaded nodes
        try:
            pipeline = FunctionalPipeline._build_pipeline_from_config(
                pipeline_config, loaded_nodes
            )
            return pipeline
        except (ValueError, KeyError, TypeError) as e:
            log.error(f"Error building pipeline from YAML config: {e}")
            raise

    @staticmethod
    def _process_params(params: dict, loaded_nodes: dict) -> dict:
        """
        Process parameters to resolve node references.

        Args:
            params: The parameters to process
            loaded_nodes: Dictionary of loaded node instances

        Returns:
            Processed parameters with node references resolved
        """
        if not params:
            return {}

        processed_params = {}

        for key, value in params.items():
            # Check if this is a string with curly braces format {node_name}
            if isinstance(value, str) and value.startswith("{") and value.endswith("}"):
                node_name = value[1:-1]  # Remove the curly braces
                if node_name in loaded_nodes:
                    processed_params[key] = loaded_nodes[node_name]
                else:
                    raise ValueError(
                        f"Node reference '{node_name}' not found. Available nodes: {list(loaded_nodes.keys())}"
                    )
            # If it's a nested dictionary
            elif isinstance(value, dict):
                # First check for direct node references in the dictionary values
                processed_dict = {}
                for dict_key, dict_value in value.items():
                    if (
                        isinstance(dict_value, str)
                        and dict_value.startswith("{")
                        and dict_value.endswith("}")
                    ):
                        node_name = dict_value[1:-1]  # Remove the curly braces
                        if node_name in loaded_nodes:
                            processed_dict[dict_key] = loaded_nodes[node_name]
                        else:
                            raise ValueError(
                                f"Node reference '{node_name}' not found. Available nodes: {list(loaded_nodes.keys())}"
                            )
                    else:
                        processed_dict[dict_key] = dict_value

                # Then recursively process any remaining complex values
                processed_params[key] = FunctionalPipeline._process_params(
                    processed_dict, loaded_nodes
                )
            # If it's a list, check each item for node references
            elif isinstance(value, list):
                processed_list = []
                for item in value:
                    if (
                        isinstance(item, str)
                        and item.startswith("{")
                        and item.endswith("}")
                    ):
                        node_name = item[1:-1]  # Remove the curly braces
                        if node_name in loaded_nodes:
                            processed_list.append(loaded_nodes[node_name])
                        else:
                            raise ValueError(
                                f"Node reference '{node_name}' not found in list. Available nodes: {list(loaded_nodes.keys())}"
                            )
                    elif isinstance(item, dict):
                        processed_list.append(
                            FunctionalPipeline._process_params(item, loaded_nodes)
                        )
                    else:
                        processed_list.append(item)
                processed_params[key] = processed_list
            else:
                processed_params[key] = value

        return processed_params

    @staticmethod
    def _build_pipeline_from_config(
        pipeline_config: list[dict[str, Any]],
        nodes_map: dict[str, PipelineComponent],
    ) -> "FunctionalPipeline":
        """Helper method to recursively build a pipeline from a config list."""
        pipeline = FunctionalPipeline()

        for i, step in enumerate(pipeline_config):
            if not isinstance(step, dict):
                raise ValueError(
                    f"Step {i + 1} in pipeline config is not a dictionary: {step}"
                )

            step_type = step.get("type")
            if not step_type:
                raise ValueError(f"Step {i + 1} missing 'type': {step}")

            name = step.get("name")
            node_ref = step.get("node")
            dependencies_config = step.get("dependencies", [])
            kwargs = step.get("kwargs", {})
            target_key = step.get("target_key")
            get_name = step.get("get_name")

            # Validate and build dependencies
            if not isinstance(dependencies_config, list):
                raise ValueError(
                    f"Step {i + 1} ('{step_type}'): 'dependencies' must be a list, got: {dependencies_config}"
                )
            try:
                dependencies = [Dependency(**dep) for dep in dependencies_config]
            except TypeError as e:
                raise ValueError(
                    f"Step {i + 1} ('{step_type}'): Invalid format in 'dependencies': {e}. Each dependency must be a dict with keys matching Dependency fields (node_name, input_key, target_key). Config: {dependencies_config}"
                ) from e

            # Resolve node from map if specified
            node_instance: PipelineComponent
            if node_ref:
                if not isinstance(node_ref, str):
                    raise ValueError(
                        f"Step {i + 1} ('{step_type}'): 'node' reference must be a string name, got: {node_ref}"
                    )
                if node_ref not in nodes_map:
                    raise KeyError(
                        f"Step {i + 1} ('{step_type}'): Node '{node_ref}' not found in provided nodes_map. Available: {list(nodes_map.keys())}"
                    )
                node_instance = nodes_map[node_ref]

            # Process kwargs to resolve any node references
            processed_kwargs = FunctionalPipeline._process_params(kwargs, nodes_map)

            # --- Handle different step types ---
            if step_type == "run":
                if not name or not node_instance:
                    raise ValueError(
                        f"Step {i + 1}: 'run' step requires 'name' (string) and resolved 'node', got name={name}, node_ref={node_ref}. Step: {step}"
                    )
                if not isinstance(name, str):
                    raise ValueError(
                        f"Step {i + 1}: 'run' step 'name' must be a string, got {type(name)}. Step: {step}"
                    )
                pipeline.run(
                    name=name,
                    node=node_instance,
                    dependencies=dependencies,
                    kwargs=processed_kwargs,
                )

            elif step_type == "then":
                if not name or not node_instance or not target_key:
                    raise ValueError(
                        f"Step {i + 1}: 'then' step requires 'name' (string), resolved 'node', and 'target_key' (string), got name={name}, node_ref={node_ref}, target_key={target_key}. Step: {step}"
                    )
                if not isinstance(name, str) or not isinstance(target_key, str):
                    raise ValueError(
                        f"Step {i + 1}: 'then' step 'name' and 'target_key' must be strings, got name type {type(name)}, target_key type {type(target_key)}. Step: {step}"
                    )
                pipeline.then(
                    name=name,
                    node=node_instance,
                    target_key=target_key,
                    dependencies=dependencies,
                    kwargs=processed_kwargs,
                )

            elif step_type == "get":
                if not get_name:
                    raise ValueError(
                        f"Step {i + 1}: 'get' step requires 'get_name' (string), got {get_name}. Step: {step}"
                    )
                if not isinstance(get_name, str):
                    raise ValueError(
                        f"Step {i + 1}: 'get' step 'get_name' must be a string, got {type(get_name)}. Step: {step}"
                    )
                pipeline.get(name=get_name)

            elif step_type == "foreach":
                if not name:
                    raise ValueError(
                        f"Step {i + 1}: 'foreach' step requires 'name' (string), got {name}. Step: {step}"
                    )
                if not isinstance(name, str):
                    raise ValueError(
                        f"Step {i + 1}: 'foreach' step 'name' must be a string, got {type(name)}. Step: {step}"
                    )

                nested_pipeline_config = step.get("pipeline")
                if not nested_pipeline_config or not isinstance(
                    nested_pipeline_config, list
                ):
                    raise ValueError(
                        f"Step {i + 1}: 'foreach' step requires a nested 'pipeline' list, got: {nested_pipeline_config}. Step: {step}"
                    )

                sub_pipeline = FunctionalPipeline._build_pipeline_from_config(
                    nested_pipeline_config, nodes_map
                )
                pipeline.foreach(name=name, do=sub_pipeline, dependencies=dependencies)

            elif step_type == "branch":
                condition_ref = step.get("condition")
                if_true_config = step.get("if_true")
                if_false_config = step.get("if_false")

                if not condition_ref or not if_true_config or not if_false_config:
                    raise ValueError(
                        f"Step {i + 1}: 'branch' step requires 'condition' (string ref), 'if_true' (list), and 'if_false' (list). Step: {step}"
                    )
                if not isinstance(condition_ref, str):
                    raise ValueError(
                        f"Step {i + 1}: 'branch' step 'condition' reference must be a string name, got: {condition_ref}"
                    )
                if condition_ref not in nodes_map:
                    raise KeyError(
                        f"Step {i + 1}: 'branch' step condition '{condition_ref}' not found in provided nodes_map. Available: {list(nodes_map.keys())}"
                    )
                if not callable(nodes_map[condition_ref]):
                    raise ValueError(
                        f"Step {i + 1}: 'branch' step condition '{condition_ref}' resolved from nodes_map is not callable."
                    )
                if not isinstance(if_true_config, list) or not isinstance(
                    if_false_config, list
                ):
                    raise ValueError(
                        f"Step {i + 1}: 'branch' step 'if_true' and 'if_false' must be lists of steps. Step: {step}"
                    )

                condition_func = nodes_map[condition_ref]
                if_true_pipeline = FunctionalPipeline._build_pipeline_from_config(
                    if_true_config, nodes_map
                )
                if_false_pipeline = FunctionalPipeline._build_pipeline_from_config(
                    if_false_config, nodes_map
                )

                pipeline.branch(
                    condition=condition_func,
                    if_true=if_true_pipeline,
                    if_false=if_false_pipeline,
                    dependencies=dependencies,
                )

            else:
                raise ValueError(
                    f"Step {i + 1}: Unsupported step type '{step_type}'. Step: {step}"
                )

        return pipeline



================================================
FILE: datapizza-ai-core/datapizza/pipeline/pipeline.py
================================================
import importlib
import logging
from typing import Any

import yaml

from datapizza.clients import ClientFactory
from datapizza.core.models import PipelineComponent
from datapizza.core.utils import replace_env_vars
from datapizza.core.vectorstore import Vectorstore
from datapizza.type import Chunk

log = logging.getLogger(__name__)


def _replace_element_refs(value: Any, elements: dict[str, Any]) -> Any:
    """
    Replace element references (${element_name}) with actual element instances.

    Args:
        value: The value to process (can be string, dict, list, or any other type)
        elements: Dictionary mapping element names to their instances

    Returns:
        The value with element references replaced by actual instances
    """
    if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
        element_name = value[2:-1]
        if element_name in elements:
            return elements[element_name]
        # If not found in elements, return as-is (might be a constant or env var)
        return value
    elif isinstance(value, dict):
        return {k: _replace_element_refs(v, elements) for k, v in value.items()}
    elif isinstance(value, list):
        return [_replace_element_refs(item, elements) for item in value]
    else:
        return value


def _instantiate_element(element_config: dict[str, Any]) -> Any:
    """
    Instantiate an element from its configuration.

    Args:
        element_config: Dictionary with 'type', 'module', and optional 'params' keys

    Returns:
        The instantiated element
    """
    module_path = element_config["module"]
    class_name = element_config["type"]
    params = element_config.get("params", {})

    module = importlib.import_module(module_path)
    class_ = getattr(module, class_name)
    return class_(**params)


class Pipeline:
    def __init__(self, components: list[PipelineComponent] | None = None):
        self.components = components or []

    def run(self, input_data=None):
        data = input_data
        for component in self.components:
            log.info(f"Running component {component.__class__.__name__}")
            data = component(data)
        return data

    async def a_run(self, input_data=None):
        data = input_data
        for component in self.components:
            log.info(f"Running component {component.__class__.__name__}")
            data = await component.a_run(data)
        return data


class IngestionPipeline:
    """
    A pipeline for ingesting data into a vector store.
    """

    def __init__(
        self,
        modules: list[PipelineComponent] | None = None,
        vector_store: Vectorstore | None = None,
        collection_name: str | None = None,
    ):
        """
        Initialize the ingestion pipeline.

        Args:
            modules (list[PipelineComponent], optional): List of pipeline components. Defaults to None.
            vector_store (Vectorstore, optional): Vector store to store the ingested data. Defaults to None.
            collection_name (str, optional): Name of the vector store collection to store the ingested data. Defaults to None.
        """
        self.pipeline = Pipeline(modules)
        self.vector_store = vector_store
        self.collection_name = collection_name
        self.components = modules

        if self.vector_store and not self.collection_name:
            raise ValueError("Collection name must be set if vector store is provided")

    def run(
        self, file_path: str | list[str], metadata: dict | None = None
    ) -> list[Chunk] | None:
        """Run the ingestion pipeline.

        Args:
            file_path (str | list[str]): The file path or list of file paths to ingest.
            metadata (dict, optional): Metadata to add to the ingested chunks. Defaults to None.

        Returns:
            list[Chunk] | None: If vector_store is not set, returns all accumulated chunks from all files.
                                If vector_store is set, returns None after storing all chunks.
        """
        # Normalize to list for uniform processing
        if isinstance(file_path, str):
            file_paths = [file_path]
        elif isinstance(file_path, list):
            # Validate that all elements are strings
            if not all(isinstance(fp, str) for fp in file_path):
                raise ValueError("All elements in file_path list must be strings")
            file_paths = file_path
        else:
            raise ValueError("file_path must be a string or a list of strings")

        all_chunks = []

        # Process each file path
        for fp in file_paths:
            data = self.pipeline.run(fp)

            if not self.vector_store:
                # If no vector store, accumulate results
                if isinstance(data, list):
                    all_chunks.extend(data)
                else:
                    all_chunks.append(data)
            else:
                # Validate chunk data immediately
                if not isinstance(data, list) or not all(
                    isinstance(item, Chunk) for item in data
                ):
                    raise ValueError(
                        f"Data returned from pipeline for '{fp}' must be a list of Chunk objects"
                    )
                all_chunks.extend(data)

        if not self.vector_store:
            return all_chunks

        # Adding metadata to all accumulated chunks
        if metadata:
            for chunk in all_chunks:
                chunk.metadata.update(metadata)

        # Add all chunks to vector store at once (only if we have chunks)
        if all_chunks:
            self.vector_store.add(all_chunks, self.collection_name)

    async def a_run(
        self, file_path: str | list[str], metadata: dict | None = None
    ) -> list[Chunk] | None:
        """Run the ingestion pipeline asynchronously.

        Args:
            file_path (str | list[str]): The file path or list of file paths to ingest.
            metadata (dict, optional): Metadata to add to the ingested chunks. Defaults to None.

        Returns:
            list[Chunk] | None: If vector_store is not set, returns all accumulated chunks from all files.
                                If vector_store is set, returns None after storing all chunks.
        """
        # Normalize to list for uniform processing
        if isinstance(file_path, str):
            file_paths = [file_path]
        elif isinstance(file_path, list):
            # Validate that all elements are strings
            if not all(isinstance(fp, str) for fp in file_path):
                raise ValueError("All elements in file_path list must be strings")
            file_paths = file_path
        else:
            raise ValueError("file_path must be a string or a list of strings")

        all_chunks = []

        # Process each file path
        for fp in file_paths:
            data = await self.pipeline.a_run(fp)

            if not self.vector_store:
                # If no vector store, accumulate results
                if isinstance(data, list):
                    all_chunks.extend(data)
                else:
                    all_chunks.append(data)
            else:
                # Validate chunk data immediately
                if not isinstance(data, list) or not all(
                    isinstance(item, Chunk) for item in data
                ):
                    raise ValueError(
                        f"Data returned from pipeline for '{fp}' must be a list of Chunk objects"
                    )
                all_chunks.extend(data)

        if not self.vector_store:
            return all_chunks

        # Adding metadata to all accumulated chunks
        if metadata:
            for chunk in all_chunks:
                chunk.metadata.update(metadata)

        # Add all chunks to vector store at once (only if we have chunks)
        if all_chunks:
            await self.vector_store.a_add(all_chunks, self.collection_name)

    def from_yaml(self, config_path: str) -> "IngestionPipeline":
        """
        Load the ingestion pipeline from a YAML configuration file.

        The YAML configuration supports the following sections:
        - constants: Key-value pairs for string substitution using ${VAR_NAME} syntax
        - elements: Reusable component definitions that can be referenced in modules
        - ingestion_pipeline: The main pipeline configuration with clients, modules, vector_store, and collection_name

        Example elements section:
            elements:
                my_embedder:
                    type: GoogleEmbedder
                    module: datapizza.embedders.google
                    params:
                        max_char: 2000

        Elements can be referenced in module params using ${element_name} syntax:
            modules:
                - name: embedder
                  type: ChunkEmbedder
                  module: datapizza.embedders
                  params:
                      client: "${my_embedder}"

        Args:
            config_path (str): Path to the YAML configuration file.

        Returns:
            IngestionPipeline: The ingestion pipeline instance.
        """
        with open(config_path) as file:
            config = yaml.safe_load(file)

        constants = config.get("constants", {})
        # Use skip_unknown=True to allow element references (like ${my_embedder})
        # to pass through without being treated as missing env vars
        config = replace_env_vars(config, constants, skip_unknown=True)

        # Parse and instantiate elements
        elements = {}
        if "elements" in config:
            for element_name, element_config in config["elements"].items():
                try:
                    elements[element_name] = _instantiate_element(element_config)
                except (ImportError, AttributeError) as e:
                    raise ValueError(
                        f"Could not load element '{element_name}' "
                        f"({element_config.get('type', 'N/A')}): {e!s}"
                    ) from e
                except KeyError as e:
                    raise ValueError(
                        f"Missing required key {e!s} in element configuration: {element_config}"
                    ) from e

        clients = {}
        ingestion_pipeline = config["ingestion_pipeline"]
        if "clients" in ingestion_pipeline:
            for client_name, client_config in ingestion_pipeline["clients"].items():
                provider = client_config.pop("provider")
                client = ClientFactory.create(
                    provider, client_config.get("api_key"), client_config.get("model")
                )
                clients[client_name] = client

        components = []
        if "modules" in ingestion_pipeline:
            for component_config in ingestion_pipeline["modules"]:
                try:
                    module_path = component_config["module"]
                    module = importlib.import_module(module_path)
                    class_ = getattr(module, component_config["type"])

                    params = component_config.get("params", {})

                    # Replace element references in params
                    params = _replace_element_refs(params, elements)

                    if "client" in params:
                        client_name = params["client"]
                        # Only do client lookup if it's still a string (not already replaced by element)
                        if isinstance(client_name, str):
                            if client_name not in clients:
                                raise ValueError(
                                    f"Client '{client_name}' not found in clients configuration"
                                )
                            params["client"] = clients[client_name]

                    component_instance = class_(**params)
                    components.append(component_instance)
                except (ImportError, AttributeError) as e:
                    raise ValueError(
                        f"Could not load component {component_config.get('type', 'N/A')}: {e!s}"
                    ) from e
                except KeyError as e:
                    raise ValueError(
                        f"Missing required key {e!s} in module configuration: {component_config}"
                    ) from e

        vector_store = None
        if "vector_store" in ingestion_pipeline:
            vector_store_config = ingestion_pipeline["vector_store"]
            vector_store_type = vector_store_config["type"]
            vector_store_module = importlib.import_module(vector_store_config["module"])
            vector_store_class = getattr(vector_store_module, vector_store_type)
            vector_store_params = vector_store_config.get("params", {})
            vector_store = vector_store_class(**vector_store_params)
            self.vector_store = vector_store

        collection_name = None
        if "collection_name" in ingestion_pipeline:
            collection_name = ingestion_pipeline["collection_name"]
            self.collection_name = collection_name

        self.components = components
        self.pipeline = Pipeline(components)
        return self



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/config.yaml
================================================
constants:
  PROMPT : "You are a helpful assistant that captions tables."
  PROMPT_TABLE : "You are a helpful assistant that captions tables."
  PROMPT_FIGURE : "You are a helpful assistant that captions figures."


ingestion_pipeline:
  clients:
    mock_client:
      provider: mock
      model: "mock-model"
      api_key: "abc"
    mock_embedder:
      provider: mock
      model: "mock-embedder"
      api_key: "abc"


  modules:
    - name: parser
      type: TextParser
      module: datapizza.modules.parsers
    - name: captioner
      type: LLMCaptioner
      module: datapizza.modules.captioners
      params:
        client: mock_client
        system_prompt_table: "${PROMPT_TABLE}"
        system_prompt_figure: "${PROMPT_FIGURE}"
    - name: node_splitter
      type: NodeSplitter
      module: datapizza.modules.splitters
      params:
        max_char: 5000
    - name: embedder1
      type: ClientEmbedder
      module: datapizza.embedders
      params:
        client: mock_embedder

  collection_name: "test"



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/config_with_elements.yaml
================================================
constants:
  PROMPT_TABLE: "You are a helpful assistant that captions tables."
  PROMPT_FIGURE: "You are a helpful assistant that captions figures."

elements:
  my_text_splitter:
    type: TextSplitter
    module: datapizza.modules.splitters
    params:
      max_char: 2000
  my_node_splitter:
    type: NodeSplitter
    module: datapizza.modules.splitters
    params:
      max_char: 3000

ingestion_pipeline:
  clients:
    mock_client:
      provider: mock
      model: "mock-model"
    mock_embedder:
      provider: mock
      model: "mock-embedder"

  modules:
    - name: parser
      type: TextParser
      module: datapizza.modules.parsers
    - name: splitter_from_element
      type: SplitterWrapper
      module: datapizza.pipeline.tests.test_pipeline
      params:
        splitter: "${my_text_splitter}"
    - name: captioner
      type: LLMCaptioner
      module: datapizza.modules.captioners
      params:
        client: mock_client
        system_prompt_table: "${PROMPT_TABLE}"
        system_prompt_figure: "${PROMPT_FIGURE}"

  collection_name: "test_elements"



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/dag_config.yaml
================================================
constants:
  PROMPT : "You are an assistant that answers questions.."

dag_pipeline:
  clients:
    mock_client:
      provider: mock
      model: "mock-model"
      api_key: "abc"
    mock_embedder:
      provider: mock
      model: "mock-embedder"
      api_key: "abc"

  modules:
    - name: rewriter
      type: ToolRewriter
      module: datapizza.modules.rewriters
      params:
        client: mock_client
        system_prompt: ${PROMPT}
    - name: embedder
      type: ClientEmbedder
      module: datapizza.embedders
      params:
        client: mock_embedder

  connections:
    - from: rewriter
      to: embedder
      target_key: input_data



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/functional_pipeline_config.yaml
================================================
modules:
  - name: mock_client
    module: datapizza.clients.mock_client
    type: MockClient
    params:
      model_name: mock-model

  - name: mock_client_2
    module: datapizza.clients.mock_client
    type: MockClient
    params:
      model_name: mock-model-2
      system_prompt: You are a helpful assistant that should answers my quesiont like if you are me. All context provided is from my knowledge base. If you don't know the answer, just say you don't know.

  - name: rewriter
    module: datapizza.modules.rewriters
    type: ToolRewriter
    params:
      client: "{mock_client}"
      system_prompt: You should use the tool to get the most relevant chunks from the vector store.

pipeline:
  - type: run
    name: rewriter
    node: rewriter



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/test_functional_pipeline.py
================================================
from pathlib import Path

from datapizza.core.models import PipelineComponent
from datapizza.pipeline import Dependency, FunctionalPipeline

# Get the directory containing this test file for relative path resolution
TEST_DIR = Path(__file__).parent


def test_functional_pipeline():
    class A(PipelineComponent):
        def _run(self):
            return "A"

        async def _a_run(self):
            return "A"

    class B(PipelineComponent):
        def _run(self):
            return "B"

        async def _a_run(self):
            return "B"

    pipeline = FunctionalPipeline()
    pipeline.run("start", A())
    pipeline.run("then", B())
    assert pipeline.execute() == {"start": "A", "then": "B"}


def test_functional_pipeline_with_dependencies():
    class A(PipelineComponent):
        def _run(self):
            return "A"

        async def _a_run(self):
            return "A"

    class B(PipelineComponent):
        def _run(self, data):
            return data + "B"

        async def _a_run(self, data):
            return data + "B"

    pipeline = FunctionalPipeline()
    pipeline.run("start", A())
    pipeline.run(
        "then", B(), dependencies=[Dependency(node_name="start", target_key="data")]
    )
    assert pipeline.execute() == {"start": "A", "then": "AB"}


def test_pipeline_with_then():
    class A(PipelineComponent):
        def _run(self):
            return "A"

        async def _a_run(self):
            return "A"

    class B(PipelineComponent):
        def _run(self, data):
            return "B"

        async def _a_run(self, data):
            return "B"

    pipeline = FunctionalPipeline()
    pipeline.run("start", A())
    pipeline.then("then", B(), target_key="data")
    assert pipeline.execute() == {"start": "A", "then": "B"}


def test_pipeline_with_then_and_dependencies():
    class A(PipelineComponent):
        def _run(self):
            return "A"

        async def _a_run(self):
            return "A"

    class C(PipelineComponent):
        def _run(self):
            return "C"

        async def _a_run(self):
            return "C"

    class B(PipelineComponent):
        def _run(self, data, c):
            return data + "B" + c

        async def _a_run(self, data, c):
            return data + "B" + c

    pipeline = FunctionalPipeline()
    pipeline.run("C", C())  # NOW C MUST BE RUN FIRST of B. In the future, we can
    # make this more flexible
    pipeline.run("start", A())
    pipeline.then(
        "then",
        B(),
        target_key="data",
        dependencies=[Dependency(node_name="C", target_key="c")],
    )
    assert pipeline.execute() == {"start": "A", "then": "ABC", "C": "C"}


def test_pipeline_with_get():
    class A(PipelineComponent):
        def _run(self):
            return "A"

        async def _a_run(self):
            return "A"

    pipeline = FunctionalPipeline().run("start", A()).get("start")
    res = pipeline.execute()

    assert res == "A"


def test_pipeline_from_yaml():
    from datapizza.clients.mock_client import MockClient
    from datapizza.modules.rewriters import ToolRewriter

    pipeline = FunctionalPipeline.from_yaml(
        str(TEST_DIR / "functional_pipeline_config.yaml")
    )
    assert pipeline is not None
    assert pipeline.nodes[0].get("name") == "rewriter"
    assert pipeline.nodes[0].get("node").__class__ == ToolRewriter
    assert isinstance(pipeline.nodes[0].get("node").client, MockClient)
    assert pipeline.nodes[0].get("node").client.model_name == "mock-model"


def test_pipeline_with_foreach():
    class A(PipelineComponent):
        def _run(self):
            return [1, 2, 3]

        async def _a_run(self):
            return [1, 2, 3]

    class B(PipelineComponent):
        def _run(self, data):
            return str(data) + "B"

        async def _a_run(self, data):
            return str(data) + "B"

    pipeline = (
        FunctionalPipeline()
        .run("start", A())
        .foreach(
            "foreach",
            dependencies=[Dependency(node_name="start")],
            do=B(),
        )
    )

    assert pipeline.execute() == {
        "start": [1, 2, 3],
        "foreach": ["1B", "2B", "3B"],
    }


def test_pipeline_with_branch():
    class A(PipelineComponent):
        def _run(self):
            return "A"

        async def _a_run(self):
            return "A"

    class B(PipelineComponent):
        def _run(self):
            return "B"

        async def _a_run(self):
            return "B"

    class C(PipelineComponent):
        def _run(self):
            return "C"

        async def _a_run(self):
            return "C"

    # Test branch with TRUE condition
    pipeline = (
        FunctionalPipeline()
        .run("start", A())
        .branch(
            condition=lambda context: context["start"] == "A",
            if_true=FunctionalPipeline().run("B", B()),
            if_false=FunctionalPipeline().run("C", C()),
        )
    )
    assert pipeline.execute() == {"start": "A", "B": "B"}

    # Test branch with FALSE condition
    pipeline = (
        FunctionalPipeline()
        .run("start", A())
        .branch(
            condition=lambda context: context["start"] == "B",
            if_true=FunctionalPipeline().run("B", B()),
            if_false=FunctionalPipeline().run("C", C()),
        )
    )
    assert pipeline.execute() == {"start": "A", "C": "C"}



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/test_graph_pipeline.py
================================================
from pathlib import Path

from datapizza.clients.mock_client import MockClient
from datapizza.core.clients.client import ClientResponse, StreamInferenceClientModule
from datapizza.core.models import PipelineComponent
from datapizza.embedders.embedders import ClientEmbedder
from datapizza.modules.rewriters import ToolRewriter
from datapizza.pipeline.dag_pipeline import DagPipeline

# Get the directory containing this test file for relative path resolution
TEST_DIR = Path(__file__).parent


class A(PipelineComponent):
    def _run(self, input_data):
        return "A"

    async def _a_run(self):
        return "A"


class B(PipelineComponent):
    def _run(self, input_data):
        return "B"

    async def _a_run(self):
        return "B"


def test_graph_pipeline():
    a = A()
    b = B()

    pipeline = DagPipeline()
    pipeline.add_module("A", a)
    pipeline.add_module("B", b)
    pipeline.connect("A", "B", target_key="input_data")

    assert pipeline.edges[0].from_node_name == "A"
    assert pipeline.edges[0].to_node_name == "B"
    assert pipeline.edges[0].src_key is None
    assert pipeline.edges[0].dst_key == "input_data"

    assert pipeline.nodes == {"A": a, "B": b}


def test_graph_pipeline_run_without_keys():
    a = A()
    b = B()

    pipeline = DagPipeline()
    pipeline.add_module("A", a)
    pipeline.add_module("B", b)
    pipeline.connect("A", "B", target_key="input_data")

    result = pipeline.run({"A": {"input_data": "A"}})

    assert result == {"A": "A", "B": "B"}


def test_graph_pipeline_run_with_target_keys():
    a = A()
    b = B()

    pipeline = DagPipeline()
    pipeline.add_module("A", a)
    pipeline.add_module("B", b)
    pipeline.connect("A", "B", target_key="input_data")

    result = pipeline.run({"A": {"input_data": "A"}})

    assert result == {"A": "A", "B": "B"}


def test_graph_pipeline_run_with_multiple_keys():
    class C(PipelineComponent):
        def _run(self, input_data):
            return {"output_data": "C"}

        async def _a_run(self):
            return {"output_data": "C"}

    a = C()
    b = B()

    pipeline = DagPipeline()
    pipeline.add_module("A", a)
    pipeline.add_module("B", b)
    pipeline.connect("A", "B", source_key="output_data", target_key="input_data")

    result = pipeline.run({"A": {"input_data": "A"}})

    assert result == {"A": {"output_data": "C"}, "B": "B"}


def test_graph_pipeline_from_yaml():
    pipeline = DagPipeline()
    pipeline.from_yaml(str(TEST_DIR / "dag_config.yaml"))

    assert pipeline.nodes["rewriter"].__class__ == ToolRewriter
    assert pipeline.nodes["embedder"].__class__ == ClientEmbedder

    assert pipeline.edges[0].from_node_name == "rewriter"
    assert pipeline.edges[0].to_node_name == "embedder"
    assert pipeline.edges[0].dst_key == "input_data"


def test_graph_pipeline_from_yaml_with_constants():
    pipeline = DagPipeline()
    pipeline.from_yaml(str(TEST_DIR / "dag_config.yaml"))

    assert (
        pipeline.nodes["rewriter"].system_prompt
        == "You are an assistant that answers questions.."
    )


def test_graph_pipeline_with_stream():
    pipeline = DagPipeline()

    client = MockClient(model_name="mock_client")
    generator = StreamInferenceClientModule(client=client)
    pipeline.add_module("generator", generator)

    result = pipeline.run({"generator": {"input": "Hello"}})

    assert isinstance(next(result.get("generator")), ClientResponse)


# def test_graph_pipeline_with_a_stream():
#     pipeline = DagPipeline()
#
#     client = MockClient(model_name="mock_client")
#     generator = StreamInferenceClientModule(client=client)
#     pipeline.add_module("generator", generator)
#     import asyncio
#
#     result = asyncio.run(pipeline.a_run({"generator": {"input": "Hello"}}))
#
#     assert isinstance(result.get("generator"), async_generator)



================================================
FILE: datapizza-ai-core/datapizza/pipeline/tests/test_pipeline.py
================================================
from pathlib import Path

from datapizza.core.models import PipelineComponent
from datapizza.pipeline import IngestionPipeline
from datapizza.type import Chunk, DenseEmbedding, Node

# Get the directory containing this test file for relative path resolution
TEST_DIR = Path(__file__).parent


class CustomSplitter(PipelineComponent):
    def __init__(self):
        pass

    def _run(self, nodes: list[Node]):
        """Takes a list of Nodes and returns them (dummy implementation)."""
        if not isinstance(nodes, list):
            raise TypeError(f"Expected input to be a list of Nodes, got {type(nodes)}")
        return nodes

    async def _a_run(self, nodes: list[Node]):
        """Takes a list of Nodes and returns them (dummy implementation)."""
        if not isinstance(nodes, list):
            raise TypeError(f"Expected input to be a list of Nodes, got {type(nodes)}")
        return nodes


class SplitterWrapper(PipelineComponent):
    """A wrapper component that accepts a splitter element for testing elements feature."""

    def __init__(self, splitter: PipelineComponent):
        self.splitter = splitter

    def _run(self, data):
        """Delegates to the wrapped splitter."""
        return self.splitter(data)

    async def _a_run(self, data):
        """Delegates to the wrapped splitter asynchronously."""
        return await self.splitter.a_run(data)


def test_pipeline():
    pipeline = IngestionPipeline(
        modules=[
            CustomSplitter(),
        ]
    )
    assert len(pipeline.pipeline.components) == 1
    assert isinstance(pipeline.pipeline.components[0], CustomSplitter)


def test_pipeline_from_yaml():
    from datapizza.modules.captioners.llm_captioner import LLMCaptioner
    from datapizza.modules.splitters.node_splitter import NodeSplitter

    pipeline = IngestionPipeline().from_yaml(str(TEST_DIR / "config.yaml"))
    assert isinstance(pipeline.pipeline.components[1], LLMCaptioner)
    assert isinstance(pipeline.pipeline.components[2], NodeSplitter)
    assert pipeline.collection_name == "test"


def test_pipeline_from_yaml_with_constants():
    pipeline = IngestionPipeline().from_yaml(str(TEST_DIR / "config.yaml"))
    assert (
        pipeline.pipeline.components[1].system_prompt_table
        == "You are a helpful assistant that captions tables."
    )
    assert (
        pipeline.pipeline.components[1].system_prompt_figure
        == "You are a helpful assistant that captions figures."
    )


def test_ingestion_pipeline():
    from datapizza.modules.splitters.text_splitter import TextSplitter

    class FakeEmbedder(PipelineComponent):
        def _run(self, nodes: list[Chunk]):
            for node in nodes:
                node.embeddings = [
                    DenseEmbedding(name="embedding", vector=[0.0] * 1536)
                ]
            return nodes

        async def _a_run(self, nodes: list[Chunk]):
            return self._run(nodes)

    pipeline = IngestionPipeline(
        modules=[
            TextSplitter(max_char=300),
            FakeEmbedder(),
        ],
        collection_name="test",
    )

    chunks = pipeline.run("Ciao, questo Ã¨ del testo da ingestionare")

    assert len(chunks) > 0


def test_ingestion_pipeline_with_multiple_files():
    """Test issue #63: IngestionPipeline should handle list of file paths."""
    from datapizza.modules.splitters.text_splitter import TextSplitter

    class FakeEmbedder(PipelineComponent):
        def _run(self, nodes: list[Chunk]):
            for node in nodes:
                node.embeddings = [
                    DenseEmbedding(name="embedding", vector=[0.0] * 1536)
                ]
            return nodes

    pipeline = IngestionPipeline(
        modules=[
            TextSplitter(max_char=300),
            FakeEmbedder(),
        ],
        collection_name="test_multi",
    )

    # Test with list of strings
    chunks = pipeline.run(
        ["Primo documento", "Secondo documento", "Terzo documento"],
        metadata={"batch": "test"},
    )

    assert len(chunks) == 3, f"Expected 3 chunks, got {len(chunks)}"


def test_ingestion_pipeline_list_validation():
    """Test that invalid list elements are rejected."""
    from datapizza.modules.splitters.text_splitter import TextSplitter

    pipeline = IngestionPipeline(
        modules=[TextSplitter(max_char=300)],
    )

    # Should raise ValueError for non-string elements
    try:
        pipeline.run([123, "valid_string", None])
        raise AssertionError("Should have raised ValueError")
    except ValueError as e:
        assert "must be strings" in str(e).lower()


def test_ingestion_pipeline_empty_list():
    """Test that empty list is handled gracefully."""
    from datapizza.modules.splitters.text_splitter import TextSplitter

    class FakeEmbedder(PipelineComponent):
        def _run(self, nodes: list[Chunk]):
            for node in nodes:
                node.embeddings = [
                    DenseEmbedding(name="embedding", vector=[0.0] * 1536)
                ]
            return nodes

    pipeline = IngestionPipeline(
        modules=[
            TextSplitter(max_char=300),
            FakeEmbedder(),
        ],
        collection_name="test_empty",
    )

    # Should handle empty list without errors
    chunks = pipeline.run([])

    assert len(chunks) == 0, "Should have 0 chunks for empty list"


def test_pipeline_from_yaml_with_elements():
    """Test that elements section in YAML config correctly instantiates and injects components."""
    pipeline = IngestionPipeline().from_yaml(
        str(TEST_DIR / "config_with_elements.yaml")
    )

    # Verify the pipeline has the expected components
    assert len(pipeline.pipeline.components) == 3

    # The second component should be SplitterWrapper with a TextSplitter element
    splitter_wrapper = pipeline.pipeline.components[1]
    # Use class name check because importlib loads from a different module context
    assert splitter_wrapper.__class__.__name__ == "SplitterWrapper"
    assert splitter_wrapper.splitter.__class__.__name__ == "TextSplitter"
    # Verify the element was instantiated with the correct params
    assert splitter_wrapper.splitter.max_char == 2000

    # Verify collection name
    assert pipeline.collection_name == "test_elements"


def test_pipeline_from_yaml_elements_with_constants():
    """Test that elements and constants can coexist and both work correctly."""
    pipeline = IngestionPipeline().from_yaml(
        str(TEST_DIR / "config_with_elements.yaml")
    )

    # Verify constants still work (from the third component - LLMCaptioner)
    captioner = pipeline.pipeline.components[2]
    assert (
        captioner.system_prompt_table
        == "You are a helpful assistant that captions tables."
    )
    assert (
        captioner.system_prompt_figure
        == "You are a helpful assistant that captions figures."
    )


def test_pipeline_from_yaml_backward_compatibility():
    """Test that existing YAML configs without elements section still work."""
    # This uses the original config.yaml which doesn't have elements section
    pipeline = IngestionPipeline().from_yaml(str(TEST_DIR / "config.yaml"))

    # Should work exactly as before
    assert len(pipeline.pipeline.components) == 4
    assert pipeline.collection_name == "test"



================================================
FILE: datapizza-ai-core/datapizza/tools/__init__.py
================================================
from pkgutil import extend_path

__path__ = extend_path(__path__, __name__)


from .tools import Tool, tool

__all__ = ["Tool", "tool"]



================================================
FILE: datapizza-ai-core/datapizza/tools/google.py
================================================
from datapizza.tools.tools import Tool


class GoogleSearch(Tool):
    """
    Represents the Google Search tool for grounding in Gemini models.
    This is a special tool that doesn't represent a function to be called by the client,
    but instead enables a feature in the Gemini model.
    """

    def __init__(self):
        """Initializes the GoogleSearch tool."""
        super().__init__(
            name="google_search",
            description="Enables Google Search for grounding model responses.",
            func=None,  # No function is associated with this tool
            properties={},
            required=[],
        )


google_search_tool = GoogleSearch()



================================================
FILE: datapizza-ai-core/datapizza/tools/mcp_client.py
================================================
from __future__ import annotations

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from datetime import timedelta
from typing import Any

import mcp.types as types
from mcp.client.session import ClientSession, SamplingFnT
from mcp.client.stdio import StdioServerParameters, stdio_client
from mcp.client.streamable_http import streamablehttp_client
from mcp.shared.session import ProgressFnT
from pydantic import AnyUrl

from datapizza.core.executors.async_executor import AsyncExecutor
from datapizza.tools import Tool


class MCPClient:
    """
    Helper for interacting with Model Context Protocol servers.

    Args:
        url: The URL of the MCP server.
        command: The command to run the MCP server.
        headers: The headers to pass to the MCP server.
        args: The arguments to pass to the MCP server.
        env: The environment variables to pass to the MCP server.
        timeout: The timeout for the MCP server.
        sampling_callback: The sampling callback to pass to the MCP server.
    """

    def __init__(
        self,
        *,
        url: str | None = None,
        command: str | None = None,
        headers: dict[str, str] | None = None,
        args: list[str] | None = None,
        env: dict[str, str] | None = None,
        timeout: int = 30,
        sampling_callback: SamplingFnT | None = None,
    ) -> None:
        self.url = url
        self.command = command
        self.args = args or []
        self.env = env or {}
        self.timeout = timeout
        self.headers = headers or {}
        self.sampling_callback = sampling_callback

        if not url and not command:
            raise ValueError("Either url or command must be provided")
        if url and command:
            raise ValueError("Only one of url or command must be provided")

    @property
    def _get_timeout(self) -> timedelta:
        return timedelta(seconds=self.timeout)

    @asynccontextmanager
    async def _session(self) -> AsyncIterator[ClientSession]:
        """Yield an initialized :class:`ClientSession` for a single operation."""

        if self.url:
            async with (
                streamablehttp_client(
                    self.url,
                    headers=self.headers or None,
                    timeout=self._get_timeout,
                ) as (read_stream, write_stream, _),
                ClientSession(
                    read_stream,
                    write_stream,
                    read_timeout_seconds=self._get_timeout,
                    sampling_callback=self.sampling_callback,
                ) as session,
            ):
                await session.initialize()
                yield session

        elif self.command:
            server_parameters = StdioServerParameters(
                command=self.command,
                args=self.args,
                env=self.env or None,
            )
            async with (
                stdio_client(server_parameters) as (read_stream, write_stream),
                ClientSession(
                    read_stream,
                    write_stream,
                    read_timeout_seconds=self._get_timeout,
                    sampling_callback=self.sampling_callback,
                ) as session,
            ):
                await session.initialize()
                yield session

    async def call_tool(
        self,
        tool_name: str,
        arguments: dict[str, Any] | None = None,
        progress_callback: ProgressFnT | None = None,
    ) -> types.CallToolResult:
        """
        Call a tool on the MCP server.

        Args:
            tool_name: The name of the tool to call.
            arguments: The arguments to pass to the tool.
            progress_callback: The progress callback to pass to the tool.

        Returns:
            The result of the tool call.
        """
        async with self._session() as session:
            return await session.call_tool(
                tool_name,
                arguments=arguments or {},
                progress_callback=progress_callback,
            )

    def list_tools(self) -> list[Tool]:
        """
        List the tools available on the MCP server.

        Returns:
            A list of :class:`Tool` objects.
        """
        return AsyncExecutor.get_instance().run(self.a_list_tools(), timeout=10)

    async def a_list_tools(self) -> list[Tool]:
        """
        List the tools available on the MCP server.

        Returns:
            A list of :class:`Tool` objects.
        """
        async with self._session() as session:
            result = await session.list_tools()

        tools: list[Tool] = []
        for mcp_tool in result.tools:
            t_name = mcp_tool.name

            def make_execute_tool(tool_name: str):
                async def execute_tool(**kwargs):
                    result = await self.call_tool(
                        tool_name=tool_name,
                        arguments=kwargs or {},
                    )
                    return result.model_dump_json()

                return execute_tool

            schema = mcp_tool.inputSchema or {}
            properties = schema.get("properties", {})
            required = schema.get("required", [])
            tools.append(
                Tool(
                    func=make_execute_tool(t_name),
                    name=mcp_tool.name,
                    description=mcp_tool.description,
                    properties=properties,
                    required=required,
                    strict=bool(schema.get("additionalProperties") is False),
                )
            )

        return tools

    async def list_resources(self) -> types.ListResourcesResult:
        """
        List the resources available on the MCP server.

        Returns:
            A :class:`types.ListResourcesResult` object.
        """
        async with self._session() as session:
            return await session.list_resources()

    async def list_resource_templates(self) -> types.ListResourceTemplatesResult:
        async with self._session() as session:
            return await session.list_resource_templates()

    async def read_resource(self, resource_uri: AnyUrl) -> types.ReadResourceResult:
        async with self._session() as session:
            return await session.read_resource(resource_uri)

    def list_prompts(self) -> types.ListPromptsResult:
        """
        List the prompts available on the MCP server.

        Returns:
            A :class:`types.ListPromptsResult` object.
        """
        return AsyncExecutor.get_instance().run(self.a_list_prompts(), timeout=10)

    async def a_list_prompts(self) -> types.ListPromptsResult:
        """
        List the prompts available on the MCP server.

        Returns:
            A :class:`types.ListPromptsResult` object.
        """
        async with self._session() as session:
            return await session.list_prompts()

    async def get_prompt(
        self, prompt_name: str, arguments: dict[str, str] | None = None
    ) -> types.GetPromptResult:
        """
        Get a prompt from the MCP server.
        """
        async with self._session() as session:
            return await session.get_prompt(prompt_name, arguments or {})



================================================
FILE: datapizza-ai-core/datapizza/tools/tools.py
================================================
import inspect
from collections.abc import Callable
from functools import wraps
from types import MethodType
from typing import Any

from .utils import (
    get_default_values,
    get_param_annotations,
    get_parameters,
    get_required_params,
)


class Tool:
    """Class that wraps a function while preserving its behavior and adding attributes."""

    def __init__(
        self,
        func: Callable | None = None,
        name: str | None = None,
        description: str | None = None,
        end: bool = False,
        properties: dict[str, dict[str, Any]] | None = None,
        required: list[str] | None = None,
        strict: bool = False,
    ):
        """
        Args:
            func (Callable | None): The function to wrap.
            name (str | None): The name of the tool.
            description (str | None): The description of the tool.
            end (bool): Whether the tool ends a chain of operations.
            properties (dict[str, dict[str, Any]] | None): The properties of the tool.
            required (list[str] | None): The required parameters of the tool.
            strict (bool): Whether the tool is strict.
        """
        self.func = func
        if not name and not func:
            raise ValueError("Must provide either name or function")

        self.name: str = name or func.__name__  # type: ignore
        self.strict: bool = strict
        self.description: str | None = description or (func.__doc__ if func else None)

        if func and (not properties or not required):
            self.required = get_required_params(inspect.signature(self.func))  # type: ignore
            param_annotations = get_param_annotations(inspect.signature(self.func))  # type: ignore
            default_values = get_default_values(inspect.signature(self.func))  # type: ignore
            self.properties = get_parameters(
                param_annotations, default_values=default_values
            )
        else:
            self.properties = properties
            self.required = required

        self.schema = self._get_function_schema()

        self.end_invoke = end
        if func:
            wraps(func)(self)

    def __call__(self, *args, **kwargs):
        if not self.func:
            raise ValueError("Function not set")

        return self.func(*args, **kwargs)

    def __get__(self, instance, owner=None):
        if instance is None:
            return self

        bound_decorated = Tool(
            func=MethodType(self.func, instance) if self.func else None,
            name=self.name,
            description=self.description,
            end=self.end_invoke,
            properties=self.properties,
            required=self.required,
        )

        return bound_decorated

    @classmethod
    def tool_from_dict(cls, tool_dict):
        return Tool(
            func=None,
            name=tool_dict.get("name"),
            description=tool_dict.get("description"),
            end=tool_dict.get("end"),
            properties=tool_dict.get("properties"),
            required=tool_dict.get("required"),
            strict=tool_dict.get("strict"),
        )

    def _get_function_schema(self):
        return {
            "name": self.name,
            "description": self.description
            or self.func.__doc__
            or f"Function to {self.name}",
            "parameters": {
                "type": "object",
                "properties": self.properties,
                "required": self.required,
            },
        }

    def to_dict(self) -> dict:
        """Convert the tool to a dictionary for JSON serialization."""
        return {
            "name": self.name,
            "description": self.description,
            "properties": self.properties,
            "required": self.required,
            "end_invoke": self.end_invoke,
        }


def tool(func=None, name=None, description=None, end=False, strict=False):
    """
    Decorator to wrap a function in a DecoratedFunc instance.

    Can be used as @tool or @tool(name="custom_name", description="...")

    Args:
        func: The function to decorate
        **attributes: Additional attributes to attach to the function

    Returns:
        DecoratedFunc: A callable object wrapping the original function
    """

    def decorator(f):
        return Tool(f, name=name, description=description, end=end, strict=strict)

    # Handle both @tool and @tool(...)
    if func is None:
        return decorator
    return decorator(func)



================================================
FILE: datapizza-ai-core/datapizza/tools/utils.py
================================================
import inspect
from typing import Any, Literal

import jsonref
from pydantic import BaseModel


class Parameters(BaseModel):
    """Parameters of a function as defined by the OpenAI API"""

    type: Literal["object"] = "object"
    properties: dict[str, dict[str, Any]]
    required: list[str]


def type2description(k: str, v) -> str:
    # handles Annotated
    if hasattr(v, "__metadata__"):
        retval = v.__metadata__[0]
        if isinstance(retval, str):
            return retval
        else:
            raise ValueError(
                f"Invalid description {retval} for parameter {k}, should be a string."
            )
    else:
        return f"Parameter {k}"


def type2schema(t: type[Any] | None) -> Any:
    from pydantic import TypeAdapter

    schema = TypeAdapter(t).json_schema()

    json_str = jsonref.dumps(schema)
    data = jsonref.loads(json_str)

    return data


def get_parameter_json_schema(
    k: str, v: Any, default_values: dict[str, Any]
) -> dict[str, Any]:
    """Get a JSON schema for a parameter as defined by the OpenAI API

    Args:
        k: The name of the parameter
        v: The type of the parameter
        default_values: The default values of the parameters of the function

    Returns:
        A Pydanitc model for the parameter
    """

    schema = type2schema(v)
    if k in default_values:
        dv = default_values[k]
        schema["default"] = dv

    schema["description"] = type2description(k, v)

    return schema


def get_param_annotations(
    typed_signature: inspect.Signature,
) -> dict:
    """Get the type annotations of the parameters of a function

    Args:
        typed_signature: The signature of the function with type annotations

    Returns:
        A dictionary of the type annotations of the parameters of the function
    """
    return {
        k: v.annotation
        for k, v in typed_signature.parameters.items()
        if v.annotation is not inspect.Signature.empty
    }


def get_parameters(
    param_annotations: dict,
    default_values: dict,
):
    """Get the parameters of a function as defined by the OpenAI API

    Args:
        param_annotations: The type annotations of the parameters of the function
        default_values: The default values of the parameters of the function

    Returns:
        A Pydantic model for the parameters of the function
    """
    return {
        k: get_parameter_json_schema(k, v, default_values)
        for k, v in param_annotations.items()
        if v is not inspect.Signature.empty
    }


def get_required_params(typed_signature: inspect.Signature) -> list[str]:
    """Get the required parameters of a function

    Args:
        signature: The signature of the function as returned by inspect.signature

    Returns:
        A list of the required parameters of the function
    """
    return [
        k
        for k, v in typed_signature.parameters.items()
        if v.default == inspect.Signature.empty and k != "self" and k != "kwargs"
    ]


def get_default_values(typed_signature: inspect.Signature) -> dict[str, Any]:
    """Get default values of parameters of a function

    Args:
        signature: The signature of the function as returned by inspect.signature

    Returns:
        A dictionary of the default values of the parameters of the function
    """
    return {
        k: v.default
        for k, v in typed_signature.parameters.items()
        if v.default != inspect.Signature.empty
    }



================================================
FILE: datapizza-ai-core/datapizza/tools/tests/__init__.py
================================================
[Empty file]


================================================
FILE: datapizza-ai-core/datapizza/tools/tests/test_tools.py
================================================
from typing import Any

from ..tools import Tool, tool


def mt_test_function():
    return "test"


@tool
def decorated_function(x: int, y: str = "default") -> str:
    """Test function docstring"""
    return f"{x} {y}"


def test_tool_with_function():
    # Test basic Tool initialization with a function
    t = Tool(func=mt_test_function)
    assert t.name == "mt_test_function"
    assert "Function to mt_test_function" in t.schema["description"]
    assert t() == "test"


def test_tool_decorator_with_params():
    @tool(name="custom_name", description="Custom description", end=True)
    def mt_test_function():
        return "test"

    assert mt_test_function.name == "custom_name"
    assert mt_test_function.description == "Custom description"
    assert mt_test_function.end_invoke is True
    assert mt_test_function() == "test"


def test_tool_decorator_without_params():
    @tool
    def mt_test_function():
        """Test function docstring"""
        return "test"

    assert mt_test_function.name == "mt_test_function"
    assert isinstance(mt_test_function, Tool)
    assert mt_test_function.description == "Test function docstring"
    assert mt_test_function() == "test"
    assert mt_test_function.end_invoke is False


def test_tool_with_simple_params():
    @tool
    def mt_test_function(x: int, y: str = "default"):
        return f"{x} {y}"

    assert mt_test_function.properties == {
        "x": {"type": "integer", "description": "Parameter x"},
        "y": {"type": "string", "description": "Parameter y", "default": "default"},
    }
    assert mt_test_function.required == ["x"]


def test_tool_with_complex_params():
    @tool
    def mt_test_function(x: list[int], y: str = "default"):
        return f"{x} {y}"

    assert mt_test_function.properties == {
        "x": {
            "type": "array",
            "items": {"type": "integer"},
            "description": "Parameter x",
        },
        "y": {"type": "string", "description": "Parameter y", "default": "default"},
    }
    assert mt_test_function.required == ["x"]


def test_tool_with_any_params():
    @tool
    def mt_test_function(x: Any):
        return x

    assert mt_test_function.properties == {"x": {"description": "Parameter x"}}
    assert mt_test_function.required == ["x"]



================================================
FILE: datapizza-ai-core/datapizza/tracing/__init__.py
================================================
from rich.console import Console

console = Console()

from datapizza.tracing.tracing import ContextTracing

__all__ = ["ContextTracing"]



================================================
FILE: datapizza-ai-core/datapizza/tracing/memory_exporter.py
================================================
import threading
import typing

from opentelemetry.sdk.trace import ReadableSpan
from opentelemetry.sdk.trace.export import (
    SimpleSpanProcessor,
    SpanExporter,
    SpanExportResult,
)


class InMemoryTraceExporter(SpanExporter):
    """Implementation of :class:`.SpanExporter` that stores spans in memory.
    This class can be used for testing purposes. It stores the exported spans
    in a list in memory that can be retrieved using the
    :func:`.get_finished_spans` method.
    """

    def __init__(self) -> None:
        self._finished_spans: dict[int, list[ReadableSpan]] = {}
        self._stopped = False
        self._lock = threading.Lock()

    def clear(self) -> None:
        """Clear list of collected spans."""
        with self._lock:
            self._finished_spans.clear()

    def clear_trace(self, trace_id):
        with self._lock:
            if trace_id in self._finished_spans:
                del self._finished_spans[trace_id]

    def get_finished_spans(self) -> dict[int, list[ReadableSpan]]:
        """Get list of collected spans."""
        with self._lock:
            return dict(self._finished_spans)

    def get_finished_spans_by_trace_id(self, trace_id: int) -> list[ReadableSpan]:
        with self._lock:
            return self._finished_spans.get(trace_id, [])

    def export(self, spans: typing.Sequence[ReadableSpan]) -> SpanExportResult:
        """Stores a list of spans in memory."""
        if self._stopped:
            return SpanExportResult.FAILURE
        with self._lock:
            for span in spans:
                context = span.get_span_context()
                if context:
                    if context.trace_id not in self._finished_spans:
                        self._finished_spans[context.trace_id] = []

                    self._finished_spans[context.trace_id].append(span)
        return SpanExportResult.SUCCESS

    def shutdown(self) -> None:
        """Shut downs the exporter.

        Calls to export after the exporter has been shut down will fail.
        """
        self._stopped = True

    def force_flush(self, timeout_millis: int = 30000) -> bool:
        return True


class ContextSpanProcessor(SimpleSpanProcessor):
    """Simple SpanProcessor implementation.

    SimpleSpanProcessor is an implementation of `SpanProcessor` that
    passes ended spans directly to the configured `SpanExporter`.
    """

    def __init__(self):
        super().__init__(InMemoryTraceExporter())
        self.tracing_ids = set()

    def start_trace(self, trace_id: int) -> None:
        self.tracing_ids.add(trace_id)

    def stop_trace(self, trace_id: int) -> None:
        if trace_id in self.tracing_ids:
            self.span_exporter.clear_trace(trace_id)  # type: ignore
            self.tracing_ids.remove(trace_id)

    def on_end(self, span: ReadableSpan) -> None:
        if span.get_span_context().trace_id not in self.tracing_ids:  # type: ignore
            return

        super().on_end(span)

    def get_spans_by_trace_id(self, trace_id: int) -> list[ReadableSpan]:
        return self.span_exporter.get_finished_spans_by_trace_id(trace_id)  # type: ignore



================================================
FILE: datapizza-ai-core/datapizza/tracing/tracing.py
================================================
import logging
from contextlib import contextmanager
from threading import Lock

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.trace import ProxyTracerProvider
from rich.console import Group
from rich.panel import Panel
from rich.table import Table

from datapizza.tracing import console
from datapizza.tracing.memory_exporter import ContextSpanProcessor

tracer = trace.get_tracer(__name__)
log = logging.getLogger(__name__)


def get_total_spans(spans):
    return len(spans)


def get_seconds_span_duration(span):
    return round((span.end_time - span.start_time) / 1000000000, 2)


def get_token_usage(spans):
    model_tokens = {}
    for span in spans:
        if span.attributes.get("type") == "generation":
            model = span.attributes.get("model_name", "unknown")
            prompt_tokens = span.attributes.get("prompt_tokens_used", 0)
            completion_tokens = span.attributes.get("completion_tokens_used", 0)
            cached_tokens = span.attributes.get("cached_tokens_used", 0)

            if model not in model_tokens:
                model_tokens[model] = {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "cached_tokens": 0,
                }

            model_tokens[model]["prompt_tokens"] += prompt_tokens
            model_tokens[model]["completion_tokens"] += completion_tokens
            model_tokens[model]["cached_tokens"] += cached_tokens
    return model_tokens


class ContextTracing:
    _instance = None
    _lock = Lock()
    _context_processor: ContextSpanProcessor | None = None

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
            return cls._instance

    def _set_context_processor(self):
        if isinstance(trace.get_tracer_provider(), ProxyTracerProvider):
            trace.set_tracer_provider(TracerProvider())

        tracer_provider = trace.get_tracer_provider()
        self._context_processor = ContextSpanProcessor()
        tracer_provider.add_span_processor(self._context_processor)  # type: ignore

    @contextmanager
    def trace(self, name: str | None = None):
        with self._lock:
            if not self._context_processor:
                self._set_context_processor()

        span = None
        trace_id = None

        class CurrentTrace:
            def __init__(self, trace_id, processor):
                self.trace_id = trace_id
                self.processor = processor

            def get_spans(self):
                return self.processor.get_spans_by_trace_id(self.trace_id)

        try:
            with tracer.start_as_current_span(name or "") as span:
                trace_id = span.get_span_context().trace_id
                self._context_processor.start_trace(trace_id)  # type: ignore
                yield CurrentTrace(trace_id, self._context_processor)
        except Exception as e:
            log.error(f"Error in trace collection: {e}")
            raise
        finally:
            if span and trace_id:
                try:
                    spans = self._context_processor.get_spans_by_trace_id(trace_id)  # type: ignore
                    total_span = get_total_spans(spans)
                    token_usage = get_token_usage(spans)
                    span_duration = get_seconds_span_duration(span)

                    # Create token usage table
                    table = Table(title="Token Usage")
                    table.add_column("Model")
                    table.add_column("Prompt Tokens")
                    table.add_column("Completion Tokens")
                    table.add_column("Cached Tokens")

                    for model, usage in token_usage.items():
                        table.add_row(
                            model,
                            str(usage["prompt_tokens"]),
                            str(usage["completion_tokens"]),
                            str(usage["cached_tokens"]),
                        )

                    panel = Panel(
                        Group(
                            f"Total Spans: {total_span}\nDuration: {span_duration}s",
                            table if token_usage else "No token usage",
                        ),
                        title=f"Trace Summary of [bold]{name}[/bold]",
                    )
                    console.print(panel)
                finally:
                    # Ensure cleanup even if display fails
                    self._context_processor.stop_trace(trace_id)  # type: ignore


@contextmanager
def generation_span(name: str | None = None):
    with tracer.start_as_current_span(name or "") as span:
        span.set_attribute("type", "generation")
        yield span


@contextmanager
def agent_span(name: str | None = None):
    with tracer.start_as_current_span(name or "") as span:
        span.set_attribute("type", "agent")
        yield span


@contextmanager
def tool_span(name: str | None = None):
    with tracer.start_as_current_span(name or "") as span:
        span.set_attribute("type", "tool")
        yield span



================================================
FILE: datapizza-ai-core/datapizza/tracing/tests/test_tracing.py
================================================
import threading
import time
from unittest.mock import Mock, patch

import pytest
from opentelemetry.sdk.trace import ReadableSpan
from opentelemetry.sdk.trace.export import SpanExportResult
from opentelemetry.trace import SpanContext

from datapizza.tracing.memory_exporter import (
    ContextSpanProcessor,
    InMemoryTraceExporter,
)
from datapizza.tracing.tracing import (
    ContextTracing,
    get_seconds_span_duration,
    get_total_spans,
)


class TestInMemoryTraceExporter:
    """Test suite for InMemoryTraceExporter class."""

    @pytest.fixture
    def exporter(self):
        """Create a fresh exporter for each test."""
        return InMemoryTraceExporter()

    @pytest.fixture
    def mock_span(self):
        """Create a mock ReadableSpan for testing."""
        span = Mock(spec=ReadableSpan)
        span_context = Mock(spec=SpanContext)
        span_context.trace_id = 12345
        span.get_span_context.return_value = span_context
        span.start_time = 1000000000  # 1 second in nanoseconds
        span.end_time = 2000000000  # 2 seconds in nanoseconds
        return span

    def test_initial_state(self, exporter):
        """Test exporter initial state."""
        assert len(exporter.get_finished_spans()) == 0
        assert not exporter._stopped
        assert exporter._finished_spans == {}

    def test_export_single_span(self, exporter, mock_span):
        """Test exporting a single span."""
        result = exporter.export([mock_span])

        assert result == SpanExportResult.SUCCESS
        spans = exporter.get_finished_spans_by_trace_id(12345)
        assert len(spans) == 1
        assert spans[0] == mock_span

    def test_export_multiple_spans_same_trace(self, exporter):
        """Test exporting multiple spans with the same trace ID."""
        span1 = Mock(spec=ReadableSpan)
        span2 = Mock(spec=ReadableSpan)

        for span in [span1, span2]:
            span_context = Mock(spec=SpanContext)
            span_context.trace_id = 12345
            span.get_span_context.return_value = span_context

        result = exporter.export([span1, span2])

        assert result == SpanExportResult.SUCCESS
        spans = exporter.get_finished_spans_by_trace_id(12345)
        assert len(spans) == 2
        assert span1 in spans
        assert span2 in spans

    def test_export_spans_different_traces(self, exporter):
        """Test exporting spans with different trace IDs."""
        span1 = Mock(spec=ReadableSpan)
        span2 = Mock(spec=ReadableSpan)

        span1_context = Mock(spec=SpanContext)
        span1_context.trace_id = 12345
        span1.get_span_context.return_value = span1_context

        span2_context = Mock(spec=SpanContext)
        span2_context.trace_id = 67890
        span2.get_span_context.return_value = span2_context

        result = exporter.export([span1, span2])

        assert result == SpanExportResult.SUCCESS
        assert len(exporter.get_finished_spans_by_trace_id(12345)) == 1
        assert len(exporter.get_finished_spans_by_trace_id(67890)) == 1

    def test_export_after_shutdown(self, exporter, mock_span):
        """Test that export fails after shutdown."""
        exporter.shutdown()

        result = exporter.export([mock_span])
        assert result == SpanExportResult.FAILURE
        assert len(exporter.get_finished_spans()) == 0

    def test_clear_all_spans(self, exporter, mock_span):
        """Test clearing all spans."""
        exporter.export([mock_span])
        assert len(exporter.get_finished_spans()) > 0

        exporter.clear()
        assert len(exporter.get_finished_spans()) == 0

    def test_clear_specific_trace(self, exporter):
        """Test clearing spans for a specific trace."""
        span1 = Mock(spec=ReadableSpan)
        span2 = Mock(spec=ReadableSpan)

        span1_context = Mock(spec=SpanContext)
        span1_context.trace_id = 12345
        span1.get_span_context.return_value = span1_context

        span2_context = Mock(spec=SpanContext)
        span2_context.trace_id = 67890
        span2.get_span_context.return_value = span2_context

        exporter.export([span1, span2])

        exporter.clear_trace(12345)

        assert len(exporter.get_finished_spans_by_trace_id(12345)) == 0
        assert len(exporter.get_finished_spans_by_trace_id(67890)) == 1

    def test_clear_nonexistent_trace(self, exporter):
        """Test clearing a trace that doesn't exist."""
        # Should not raise an exception
        exporter.clear_trace(99999)
        assert len(exporter.get_finished_spans()) == 0

    def test_get_nonexistent_trace(self, exporter):
        """Test getting spans for a nonexistent trace."""
        spans = exporter.get_finished_spans_by_trace_id(99999)
        assert spans == []

    def test_force_flush(self, exporter):
        """Test force flush functionality."""
        result = exporter.force_flush()
        assert result is True

        result = exporter.force_flush(timeout_millis=1000)
        assert result is True


class TestContextSpanProcessor:
    """Test suite for ContextSpanProcessor class."""

    @pytest.fixture
    def processor(self):
        """Create a fresh processor for each test."""
        return ContextSpanProcessor()

    @pytest.fixture
    def mock_span(self):
        """Create a mock ReadableSpan for testing."""
        span = Mock(spec=ReadableSpan)
        span_context = Mock(spec=SpanContext)
        span_context.trace_id = 12345
        span.get_span_context.return_value = span_context
        return span

    def test_initial_state(self, processor):
        """Test processor initial state."""
        assert len(processor.tracing_ids) == 0
        assert processor.span_exporter is not None

    def test_start_trace(self, processor):
        """Test starting a trace."""
        trace_id = "test_trace_123"
        processor.start_trace(trace_id)

        assert trace_id in processor.tracing_ids

    def test_stop_trace(self, processor, mock_span):
        """Test stopping a trace."""
        trace_id = 12345
        processor.start_trace(trace_id)
        processor.span_exporter.export([mock_span])

        # Verify span is in exporter
        spans = processor.get_spans_by_trace_id(trace_id)
        assert len(spans) == 1

        processor.stop_trace(trace_id)

        # Verify trace is removed from tracking and exporter
        assert trace_id not in processor.tracing_ids
        spans = processor.get_spans_by_trace_id(trace_id)
        assert len(spans) == 0

    def test_stop_nonexistent_trace(self, processor):
        """Test stopping a trace that doesn't exist."""
        # Should not raise an exception
        processor.stop_trace("nonexistent_trace")
        assert len(processor.tracing_ids) == 0

    def test_on_end_untracked_span(self, processor, mock_span):
        """Test on_end with an untracked span."""
        # Don't start trace for this span
        processor.on_end(mock_span)
        assert len(processor.get_spans_by_trace_id(12345)) == 0


class TestTracingHelperFunctions:
    """Test suite for tracing helper functions."""

    def test_get_spans_metrics(self):
        """Test span count calculation."""
        spans = [Mock(), Mock(), Mock()]
        count = get_total_spans(spans)
        assert count == 3

        empty_spans = []
        count = get_total_spans(empty_spans)
        assert count == 0

    def test_get_seconds_span_duration(self):
        """Test span duration calculation."""
        mock_span = Mock()
        mock_span.start_time = 1000000000  # 1 second in nanoseconds
        mock_span.end_time = 3500000000  # 3.5 seconds in nanoseconds

        duration = get_seconds_span_duration(mock_span)
        assert duration == 2.5  # 3.5 - 1.0 = 2.5 seconds

    def test_get_seconds_span_duration_zero(self):
        """Test span duration with same start and end time."""
        mock_span = Mock()
        mock_span.start_time = 1000000000
        mock_span.end_time = 1000000000

        duration = get_seconds_span_duration(mock_span)
        assert duration == 0.0


class TestMetricCollector:
    """Test suite for metric_collector context manager."""

    @pytest.fixture
    def mock_tracer(self):
        """Create a mock tracer."""
        return Mock()

    @pytest.fixture
    def mock_span(self):
        """Create a mock span with context."""
        span = Mock()
        span_context = Mock()
        span_context.trace_id = 12345
        span.get_span_context.return_value = span_context
        span.start_time = 1000000000
        span.end_time = 2000000000
        return span

    @patch("datapizza.tracing.tracing.ContextTracing._context_processor")
    @patch("datapizza.tracing.tracing.tracer")
    @patch("datapizza.tracing.tracing.console")
    def test_metric_collector_success(
        self, mock_console, mock_tracer, mock_context_processor, mock_span
    ):
        """Test successful metric collection."""
        # Setup mock tracer context manager
        mock_tracer.start_as_current_span.return_value.__enter__ = Mock(
            return_value=mock_span
        )
        mock_tracer.start_as_current_span.return_value.__exit__ = Mock(
            return_value=False
        )

        # Setup mock context processor
        mock_context_processor.get_spans_by_trace_id.return_value = [
            mock_span,
            mock_span,
        ]

        # Use the context manager
        with ContextTracing().trace("test_operation"):
            pass

        # Verify interactions
        mock_context_processor.start_trace.assert_called_once_with(12345)
        mock_context_processor.stop_trace.assert_called_once_with(12345)
        mock_context_processor.get_spans_by_trace_id.assert_called_once_with(12345)
        mock_console.print.assert_called_once()

    @patch("datapizza.tracing.tracing.ContextTracing._context_processor")
    @patch("datapizza.tracing.tracing.tracer")
    def test_metric_collector_exception_handling(
        self, mock_tracer, mock_context_processor, mock_span
    ):
        """Test metric collector handles exceptions properly."""
        # Setup mock tracer context manager
        mock_tracer.start_as_current_span.return_value.__enter__ = Mock(
            return_value=mock_span
        )
        mock_tracer.start_as_current_span.return_value.__exit__ = Mock(
            return_value=False
        )

        # Setup exception in the context
        with pytest.raises(ValueError), ContextTracing().trace("test_operation"):
            raise ValueError("Test exception")

        # Verify cleanup still happens
        mock_context_processor.start_trace.assert_called_once_with(12345)
        mock_context_processor.stop_trace.assert_called_once_with(12345)


class TestThreadSafety:
    """Test suite for thread safety of the tracing components."""

    def test_concurrent_export(self):
        """Test that concurrent exports don't cause race conditions."""
        exporter = InMemoryTraceExporter()
        results = []
        errors = []

        def export_spans(thread_id):
            try:
                for i in range(10):
                    span = Mock(spec=ReadableSpan)
                    span_context = Mock(spec=SpanContext)
                    span_context.trace_id = thread_id * 1000 + i
                    span.get_span_context.return_value = span_context

                    result = exporter.export([span])
                    results.append(result)
            except Exception as e:
                errors.append(e)

        # Start multiple threads
        threads = []
        for i in range(5):
            thread = threading.Thread(target=export_spans, args=(i,))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Verify no errors occurred
        assert len(errors) == 0
        assert len(results) == 50  # 5 threads * 10 spans each
        assert all(result == SpanExportResult.SUCCESS for result in results)

        # Verify all spans were stored
        all_spans = exporter.get_finished_spans()
        assert len(all_spans) == 50  # 50 different trace IDs

    def test_concurrent_clear_operations(self):
        """Test concurrent clear operations."""
        exporter = InMemoryTraceExporter()

        # Pre-populate with some spans
        for i in range(10):
            span = Mock(spec=ReadableSpan)
            span_context = Mock(spec=SpanContext)
            span_context.trace_id = i
            span.get_span_context.return_value = span_context
            exporter.export([span])

        errors = []

        def clear_operations():
            try:
                for i in range(5):
                    if i % 2 == 0:
                        exporter.clear()
                    else:
                        exporter.clear_trace(i)
                    time.sleep(
                        0.001
                    )  # Small delay to increase chance of race conditions
            except Exception as e:
                errors.append(e)

        # Start multiple threads doing clear operations
        threads = []
        for _i in range(3):
            thread = threading.Thread(target=clear_operations)
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Verify no errors occurred
        assert len(errors) == 0

    def test_processor_thread_safety(self):
        """Test thread safety of ContextSpanProcessor."""
        processor = ContextSpanProcessor()
        errors = []

        def processor_operations(thread_id):
            try:
                trace_id = 11333
                processor.start_trace(trace_id)

                # Create and process some spans
                for _i in range(5):
                    span = Mock(spec=ReadableSpan)
                    span_context = Mock(spec=SpanContext)
                    span_context.trace_id = trace_id
                    span.get_span_context.return_value = span_context

                    processor.on_end(span)

                # Get spans and stop trace
                processor.get_spans_by_trace_id(trace_id)
                processor.stop_trace(trace_id)

            except Exception as e:
                errors.append(e)

        # Start multiple threads
        threads = []
        for i in range(5):
            thread = threading.Thread(target=processor_operations, args=(i,))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Verify no errors occurred
        assert len(errors) == 0

        # Verify all traces were cleaned up
        assert len(processor.tracing_ids) == 0


class TestEdgeCases:
    """Test suite for edge cases and error conditions."""

    def test_empty_span_export(self):
        """Test exporting empty span list."""
        exporter = InMemoryTraceExporter()
        result = exporter.export([])

        assert result == SpanExportResult.SUCCESS
        assert len(exporter.get_finished_spans()) == 0

    def test_none_span_handling(self):
        """Test handling of None spans."""
        exporter = InMemoryTraceExporter()

        # This should raise an AttributeError when trying to call get_span_context()
        with pytest.raises(AttributeError):
            exporter.export([None])  # type: ignore

    def test_processor_with_invalid_trace_id_type(self):
        """Test processor with different trace ID types."""
        processor = ContextSpanProcessor()

        # Test with integer trace ID
        processor.start_trace(12345)
        assert 12345 in processor.tracing_ids

        processor.stop_trace(12345)
        assert 12345 not in processor.tracing_ids

    def test_multiple_shutdown_calls(self):
        """Test multiple shutdown calls don't cause issues."""
        exporter = InMemoryTraceExporter()

        exporter.shutdown()
        exporter.shutdown()  # Should not raise exception

        assert exporter._stopped is True



================================================
FILE: datapizza-ai-core/datapizza/type/__init__.py
================================================
from .type import (
    ROLE,
    Block,
    Chunk,
    DenseEmbedding,
    Embedding,
    EmbeddingFormat,
    FunctionCallBlock,
    FunctionCallResultBlock,
    Media,
    MediaBlock,
    MediaNode,
    Model,
    Node,
    NodeType,
    SparseEmbedding,
    StructuredBlock,
    TextBlock,
    ThoughtBlock,
)

__all__ = [
    "ROLE",
    "Block",
    "Chunk",
    "DenseEmbedding",
    "Embedding",
    "EmbeddingFormat",
    "FunctionCallBlock",
    "FunctionCallResultBlock",
    "Media",
    "MediaBlock",
    "MediaNode",
    "Model",
    "Node",
    "NodeType",
    "SparseEmbedding",
    "StructuredBlock",
    "TextBlock",
    "ThoughtBlock",
]



================================================
FILE: datapizza-ai-core/datapizza/type/type.py
================================================
import hashlib
import logging
import uuid
from abc import abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Any, Literal, TypeVar

from pydantic import BaseModel

from datapizza.tools.tools import Tool

log = logging.getLogger(__name__)

Model = TypeVar("Model", bound=BaseModel)


class ROLE(Enum):
    ASSISTANT = "assistant"
    USER = "user"
    SYSTEM = "system"
    TOOL = "tool"

    @property
    def google_role(self) -> str:
        if self == ROLE.USER:
            return "user"
        elif self == ROLE.ASSISTANT or self == ROLE.SYSTEM:
            return "model"
        elif self == ROLE.TOOL:
            return "tool"
        else:
            raise ValueError(f"Unknown role: {self}")

    @property
    def anthropic_role(self) -> str:
        if self == ROLE.USER:
            return "user"
        elif self == ROLE.ASSISTANT:
            return "assistant"
        elif self == ROLE.SYSTEM:
            return "model"
        elif self == ROLE.TOOL:
            return "assistant"
        else:
            raise ValueError(f"Unknown role: {self}")


class Block:
    """
    A class for storing the response from a client.
    """

    def __init__(self, type: str):
        self.type = type

    @classmethod
    def from_dict(cls, data: dict):
        match data["type"]:
            case "text":
                return TextBlock(content=data.get("content", ""))
            case "thought":
                return ThoughtBlock(content=data.get("content", ""))
            case "function":
                tool = Tool.tool_from_dict(data.get("tool"))
                return FunctionCallBlock(
                    id=data.get("id", ""),
                    arguments=data.get("arguments", {}),
                    name=data.get("name", ""),
                    tool=tool,
                )
            case "function_call_result":
                tool = Tool.tool_from_dict(data.get("tool"))
                return FunctionCallResultBlock(
                    id=data.get("id", ""), tool=tool, result=data.get("result", "")
                )
            case "structured":
                logging.warning(
                    "Structured Blocks clouldn't load BaseModel, dict loaded instead"
                )
                return StructuredBlock(**data)
            case "media":
                return MediaBlock.from_dict(data)
            case _:
                raise ValueError(f"Invalid block type: {data['type']}")

    @abstractmethod
    def __hash__(self) -> int:
        pass

    @abstractmethod
    def to_dict(self) -> dict:
        """Convert the block to a dictionary for JSON serialization."""
        pass


class TextBlock(Block):
    """
    A class for storing the text response from a client.
    """

    def __init__(self, content: str, type: str = "text"):
        """
        Initialize a TextBlock object.

        Args:
            content (str): The content of the text block.
            type (str, optional): The type of the text block. Defaults to "text".
        """
        self.content = content
        super().__init__(type)

    def __eq__(self, other):
        return isinstance(other, TextBlock) and self.content == other.content

    def __str__(self) -> str:
        return f"TextBlock(content={self.content})"

    def __repr__(self) -> str:
        return self.__str__()

    def __hash__(self) -> int:
        return int(hashlib.sha256(self.content.encode("utf-8")).hexdigest(), 16)

    def to_dict(self) -> dict:
        return {"type": self.type, "content": self.content}


class ThoughtBlock(Block):
    """
    A class for storing the thought from a client.
    """

    def __init__(self, content: str, type: str = "thought"):
        """
        Initialize a ThoughtBlock object.

        Args:
            content (str): The content of the thought block.
            type (str, optional): The type of the thought block. Defaults to "thought".
        """
        self.content = content
        super().__init__(type)

    def __eq__(self, other):
        return isinstance(other, ThoughtBlock) and self.content == other.content

    def __str__(self) -> str:
        return f"ThoughtBlock(content={self.content})"

    def __repr__(self) -> str:
        return self.__str__()

    def __hash__(self) -> int:
        return int(hashlib.sha256(self.content.encode("utf-8")).hexdigest(), 16)

    def to_dict(self) -> dict:
        return {"type": self.type, "content": self.content}


class FunctionCallBlock(Block):
    """
    A class for storing the function call from a client.
    """

    def __init__(
        self,
        id: str,
        arguments: dict[str, Any],
        name: str,
        tool: Tool,
        type: str = "function",
    ):
        """
        Initialize a FunctionCallBlock object.

        Args:
            id (str): The id of the function call block.
            arguments (dict[str, Any]): The arguments of the function call block.
            name (str): The name of the function call block.
            tool (Tool): The tool of the function call block.
        """
        self.id = id
        self.arguments = arguments
        self.name = name
        self.tool = tool
        super().__init__(type)

    def __eq__(self, other):
        return (
            isinstance(other, FunctionCallBlock)
            and self.id == other.id
            and self.arguments == other.arguments
            and self.name == other.name
        )

    def __str__(self) -> str:
        return f"FunctionCallBlock(id={self.id}, arguments={self.arguments}, name={self.name}, tool={self.tool})"

    def __repr__(self) -> str:
        return self.__str__()

    def __hash__(self) -> int:
        return int(hashlib.sha256(self.id.encode("utf-8")).hexdigest(), 16)

    def to_dict(self) -> dict:
        return {
            "type": self.type,
            "id": self.id,
            "arguments": self.arguments,
            "name": self.name,
            "tool": self.tool.to_dict(),
        }


class FunctionCallResultBlock(Block):
    """
    A class for storing the function call response from a client.
    """

    def __init__(
        self,
        id: str,
        tool: Tool,
        result: str,
        type: str = "function_call_result",
    ):
        """
        Initialize a FunctionCallResultBlock object.

        Args:
            id (str): The id of the function call result block.
            tool (Tool): The tool of the function call result block.
            result (str): The result of the function call result block.
        """
        self.id = id
        self.tool = tool
        self.result = result
        super().__init__(type)

    def __hash__(self) -> int:
        return int(hashlib.sha256(self.id.encode("utf-8")).hexdigest(), 16)

    def to_dict(self) -> dict:
        return {
            "type": self.type,
            "id": self.id,
            "tool": self.tool.to_dict(),
            "result": self.result,
        }


class StructuredBlock(Block):
    """
    A class for storing the structured response from a client.
    """

    def __init__(self, content: BaseModel, type: str = "structured"):
        """
        Initialize a StructuredBlock object.

        Args:
            content (BaseModel): The content of the structured block.
            type (str, optional): The type of the structured block. Defaults to "structured".
        """
        self.content = content
        super().__init__(type)

    def __hash__(self) -> int:
        return int(
            hashlib.sha256(self.content.model_dump_json().encode("utf-8")).hexdigest(),
            16,
        )

    def to_dict(self) -> dict:
        return {
            "type": self.type,
            "content": self.content.model_dump_json()
            if isinstance(self.content, BaseModel)
            else self.content,
        }


class Media:
    """
    A class for storing the media response from a client.
    """

    def __init__(
        self,
        *,
        extension: str | None = None,
        media_type: Literal["image", "video", "audio", "pdf"],
        source_type: Literal["url", "base64", "path", "pil", "raw"],
        source: Any,
        detail: str = "high",
    ):
        """
        A class for storing the media response from a client.

        arguments:
            extension (str, optional): The file extension of the media. Defaults to None.
            media_type (Literal["image", "video", "audio", "pdf"]): The type of media. Defaults to "image".
            source_type (Literal["url", "base64", "path", "pil", "raw"]): The source type of the media. Defaults to "url".
            source (Any): The source of the media. Defaults to None.
        """
        self.extension = extension
        self.media_type = media_type
        self.source_type = source_type
        self.source = source
        self.detail = detail

    def to_dict(self) -> dict:
        """Convert the media to a dictionary for JSON serialization."""
        return {
            "extension": self.extension,
            "media_type": self.media_type,
            "source_type": self.source_type,
            "source": str(self.source),  # Convert to string for JSON serialization
            "detail": self.detail,
        }


class MediaBlock(Block):
    """
    A class for storing the media response from a client.
    """

    def __init__(self, media: Media, type: str = "media"):
        """
        Initialize a MediaBlock object.

        Args:
            media (Media): The media of the media block.
            type (str, optional): The type of the media block. Defaults to "media".
        """
        self.media = media
        super().__init__(type)

    def __hash__(self) -> int:
        return int(hashlib.sha256(self.media.source.encode("utf-8")).hexdigest(), 16)

    def to_dict(self) -> dict:
        return {"type": self.type, "media": self.media.to_dict()}

    @classmethod
    def from_dict(cls, json_data):
        media_data = json_data.get("media")
        media = Media(**media_data)
        return MediaBlock(media=media)


class NodeType(Enum):
    SECTION = "section"
    PARAGRAPH = "paragraph"
    DOCUMENT = "document"
    SENTENCE = "sentence"
    PAGE = "page"
    TABLE = "table"
    FIGURE = "figure"


class Node:
    """Class representing a node in a document graph."""

    def __init__(
        self,
        children: list["Node"] | None = None,
        metadata: dict | None = None,
        node_type: NodeType = NodeType.SECTION,
        content: str | None = None,
    ):
        """
        Initialize a Node object.

        Args:
            children: List of child nodes
            metadata: Dictionary of metadata
            content: Content object for leaf nodes
        """
        self.children = children or []
        self.metadata = metadata or {}
        self.node_type = node_type
        self._content = content
        self.id = uuid.uuid4()

    @property
    def content(self) -> str:
        """Get the textual content of this node and its children."""
        if self.is_leaf:
            if self._content:
                return self._content
            # Handle other content types appropriately
            return ""

        # Add space or newline between child contents
        return " ".join([child.content for child in self.children])

    @property
    def is_leaf(self) -> bool:
        """Check if the node is a leaf node (has no children)."""
        return len(self.children) == 0

    def add_child(self, child: "Node") -> None:
        """Add a child node to this node."""
        self.children.append(child)

    def remove_child(self, child: "Node") -> bool:
        """Remove a child node from this node."""
        if child in self.children:
            self.children.remove(child)
            return True
        return False

    def __eq__(self, other: "Node") -> bool:
        """Check if two nodes are equal."""
        if not isinstance(other, Node):
            return False
        return self.id == other.id

    def __hash__(self) -> int:
        """Hash the node."""
        return hash(self.id)


class MediaNode(Node):
    """Class representing a media node in a document graph."""

    def __init__(
        self,
        media: Media,
        children: list["Node"] | None = None,
        metadata: dict | None = None,
        node_type: NodeType = NodeType.SECTION,
        content: str | None = None,
    ):
        super().__init__(
            children=children,
            metadata=metadata,
            node_type=node_type,
            content=content,
        )
        self.media = media


class EmbeddingFormat(Enum):
    DENSE = "dense"
    SPARSE = "sparse"


@dataclass
class Embedding:
    name: str


@dataclass
class DenseEmbedding(Embedding):
    vector: list[float]


@dataclass
class SparseEmbedding(Embedding):
    values: list[float]
    indices: list[int]


@dataclass
class Chunk:
    """
    A class for storing the chunk response from a client.
    """

    def __init__(
        self,
        id: str,
        text: str,
        embeddings: list[Embedding] | None = None,
        metadata: dict | None = None,
    ):
        """
        Initialize a Chunk object.

        Args:
            id (str): The id of the chunk.
            text (str): The text of the chunk.
            embeddings (list[Embedding], optional): The embeddings of the chunk. Defaults to [].
            metadata (dict, optional): The metadata of the chunk. Defaults to {}.
        """
        self.id = id
        self.text = text
        self.embeddings = embeddings or []
        self.metadata = metadata or {}



================================================
FILE: datapizza-ai-core/datapizza/type/tests/test_type.py
================================================
from datapizza.tools.tools import Tool, tool
from datapizza.type.type import (
    Block,
    FunctionCallBlock,
    FunctionCallResultBlock,
    Media,
    MediaBlock,
    TextBlock,
)


def test_hash():
    block = TextBlock(content="Hello, world!")
    print(hash(block))
    assert hash(block) == hash(block)


def test_hash_function_call_block():
    def test_tool():
        pass

    block = FunctionCallBlock(
        id="1",
        arguments="{}",
        name="test",
        tool=Tool(name="test", func=test_tool),
    )
    print(hash(block))
    assert hash(block) == hash(block)
    assert hash(block) == 418964443453056350


def test_block_new():
    block = Block.from_dict({"type": "text", "content": "Hello, world!"})
    assert isinstance(block, TextBlock)
    assert block.content == "Hello, world!"


def test_block_new_function_call():
    @tool
    def test_func():
        pass

    block = FunctionCallBlock(id="1", arguments={}, name="test", tool=test_func)

    assert isinstance(block, FunctionCallBlock)
    assert block.id == "1"
    assert block.arguments == {}
    assert block.name == "test"

    json_data = block.to_dict()
    Block.from_dict(json_data)

    assert isinstance(block, FunctionCallBlock)
    assert block.id == "1"
    assert isinstance(block.tool, Tool)


def test_block_new_function_call_result():
    @tool
    def test_func():
        pass

    block = FunctionCallResultBlock(id="1", tool=test_func, result={})

    json_data = block.to_dict()
    Block.from_dict(json_data)

    assert isinstance(block, FunctionCallResultBlock)
    assert block.id == "1"
    assert block.result == {}


def test_block_new_media():
    block = Block.from_dict(
        {
            "type": "media",
            "media": {
                "extension": "png",
                "media_type": "image",
                "source_type": "url",
                "source": "https://example.com/image.png",
                "detail": "high",
            },
        }
    )
    assert isinstance(block, MediaBlock)
    assert isinstance(block.media, Media)


# def test_byte_block():
#     block = ByteBlock(data=b"Hello, world!")
#     assert isinstance(block, ByteBlock)
#     assert block.data == b"Hello, world!"
#
#
# def test_byte_block_hash():
#     block = ByteBlock(data=b"Hello, world!")
#     block2 = ByteBlock(data=b"Hello, world!")
#     assert hash(block) == hash(block2)



================================================
FILE: datapizza-ai-embedders/image_embedder.py
================================================
import base64
import copy
import logging
import mimetypes
import os

import cohere
from datapizza.core.models import PipelineComponent
from datapizza.type.type import Chunk, DenseEmbedding, Media, MediaBlock

log = logging.getLogger(__name__)


class CohereImageClient(PipelineComponent):
    """Client for interacting with Cohere API for image embeddings.

    Can be initialized with pre-existing Cohere clients (sync and/or async)
    or with API credentials for lazy client initialization.
    """

    DEFAULT_MODEL = "embed-v4.0"
    DEFAULT_INPUT_TYPE = "image_document"

    def __init__(
        self,
        cohere_client: cohere.Client
        | cohere.AsyncClient
        | None = None,  # Sync or async client (for backward compatibility)
        a_cohere_client: cohere.AsyncClient
        | None = None,  # Dedicated asynchronous client
        api_key: str | None = None,
        base_url: str | None = None,  # For Azure or custom deployments
        model: str = DEFAULT_MODEL,
        embedding_type: str = "float",
    ):
        """Initialize the Cohere Image Client.

        Args:
            cohere_client: Pre-existing Cohere client (sync or async). For backward compatibility,
                          if an AsyncClient is passed here, it will be used as the async client.
            a_cohere_client: Pre-existing async Cohere client. Takes precedence over async client
                           passed to cohere_client parameter.
            api_key: Cohere API key. If not provided, will try COHERE_API_KEY or AZURE_COHERE_API_KEY
                    environment variables.
            base_url: Base URL for custom/Azure deployments. If not provided, will try
                     AZURE_COHERE_ENDPOINT environment variable.
            model: Model name to use for embeddings.
            embedding_type: Type of embeddings to retrieve ('float', 'int8', etc.).
        """
        # Initialize clients
        self.client: cohere.Client | None = None
        self.a_client: cohere.AsyncClient | None = None

        # Handle the main cohere_client parameter (backward compatibility)
        if cohere_client:
            if isinstance(cohere_client, cohere.AsyncClient):
                # If an async client was passed to the main parameter, use it as async client
                self.a_client = cohere_client
            else:
                # Otherwise, it's a sync client
                self.client = cohere_client

        # Handle dedicated async client parameter
        if a_cohere_client:
            self.a_client = a_cohere_client

        # Store credentials for lazy client initialization
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.embedding_type = embedding_type

        # If no clients are provided and no API key, try to get from environment
        if not cohere_client and not a_cohere_client and not api_key:
            resolved_api_key = os.environ.get("COHERE_API_KEY") or os.environ.get(
                "AZURE_COHERE_API_KEY"
            )
            if not resolved_api_key:
                raise ValueError(
                    "Either cohere_client/a_cohere_client must be provided, or API key must be "
                    "provided via api_key parameter or environment variables "
                    "(COHERE_API_KEY or AZURE_COHERE_API_KEY)"
                )
            self.api_key = resolved_api_key

    def _create_data_uri(self, media: Media) -> str:
        """Creates a data URI from a Media object."""
        if media.media_type != "image":
            raise ValueError(f"Media type must be 'image', got '{media.media_type}'")

        if media.source_type == "base64":
            # Already in base64 format
            if media.extension:
                mime_type = f"image/{media.extension.lstrip('.')}"
            else:
                mime_type = "image/jpeg"  # Default fallback
            return f"data:{mime_type};base64,{media.source}"

        elif media.source_type == "path":
            # Handle file path
            image_path = str(media.source)
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type or not mime_type.startswith("image"):
                raise ValueError(
                    f"Could not determine image type or invalid image type for {image_path}"
                )
            try:
                with open(image_path, "rb") as f:
                    image_data = f.read()
            except FileNotFoundError as e:
                raise FileNotFoundError(f"Image file not found at {image_path}") from e
            except Exception as e:
                raise OSError(f"Error reading image file {image_path}: {e}") from e

            base64_encoded_data = base64.b64encode(image_data)
            base64_string = base64_encoded_data.decode("utf-8")
            return f"data:{mime_type};base64,{base64_string}"

        elif media.source_type == "url":
            # For URLs, we'd need to download the image first
            # For now, raise an error as this would require additional dependencies
            raise NotImplementedError(
                "URL source type not yet supported for Cohere embeddings"
            )

        else:
            raise ValueError(f"Unsupported source type: {media.source_type}")

    def _get_client(self) -> cohere.Client:
        """Get the synchronous client"""
        if not self.client:
            self._set_client()

        if not self.client:
            raise ValueError("Cohere client is not initialized properly.")

        return self.client

    def _set_client(self):
        """Set up the synchronous client"""
        if not self.client:
            resolved_api_key = (
                self.api_key
                or os.environ.get("COHERE_API_KEY")
                or os.environ.get("AZURE_COHERE_API_KEY")
            )
            if not resolved_api_key:
                raise ValueError(
                    "Cohere API key must be provided or set in environment variables "
                    "(COHERE_API_KEY or AZURE_COHERE_API_KEY)"
                )

            resolved_base_url = self.base_url or os.environ.get("AZURE_COHERE_ENDPOINT")
            client_args: dict = {"api_key": resolved_api_key}
            if resolved_base_url:
                client_args["base_url"] = resolved_base_url
            self.client = cohere.ClientV2(**client_args)

    def _get_a_client(self) -> cohere.AsyncClient:
        """Get the asynchronous client"""
        if not self.a_client:
            self._set_a_client()

        if not self.a_client:
            raise ValueError("Cohere async client is not initialized properly.")

        return self.a_client

    def _set_a_client(self):
        """Set up the asynchronous client"""
        if not self.a_client:
            resolved_api_key = (
                self.api_key
                or os.environ.get("COHERE_API_KEY")
                or os.environ.get("AZURE_COHERE_API_KEY")
            )
            if not resolved_api_key:
                raise ValueError(
                    "Cohere API key must be provided or set in environment variables "
                    "(COHERE_API_KEY or AZURE_COHERE_API_KEY)"
                )

            resolved_base_url = self.base_url or os.environ.get("AZURE_COHERE_ENDPOINT")
            client_args: dict = {"api_key": resolved_api_key}
            if resolved_base_url:
                client_args["base_url"] = resolved_base_url
            self.a_client = cohere.AsyncClientV2(**client_args)

    def embed(self, media: Media) -> list[float]:
        """
        Generates embeddings for a single image using the Cohere API.

        Args:
            media: A Media object containing the image data.

        Returns:
            A single embedding vector.

        Raises:
            ValueError: If the Media object is invalid or embedding extraction fails.
            AttributeError: If the requested embedding type is not found in the Cohere response.
            Exception: If the Cohere API call fails.
        """
        image_uri = self._create_data_uri(media)

        try:
            client = self._get_client()
            response = client.embed(
                texts=[],  # Required but not used for image embeddings
                images=[image_uri],  # Only one image per call
                model=self.model,
                input_type=self.DEFAULT_INPUT_TYPE,
                embedding_types=[self.embedding_type],
            )

            # Extract embeddings based on the requested type
            embeddings_data = response.embeddings

            if not hasattr(embeddings_data, self.embedding_type):
                raise AttributeError(
                    f"Cohere response does not contain image embeddings of type '{self.embedding_type}'."
                )

            vector = getattr(embeddings_data, self.embedding_type)[
                0
            ]  # Get the first (only) vector

            return vector

        except Exception as e:
            log.error(f"Error during Cohere embedding generation for media: {e}")
            raise e

    async def a_embed(self, media: Media) -> list[float]:
        """
        Asynchronously generates embeddings for a single image using the Cohere API.

        Args:
            media: A Media object containing the image data.

        Returns:
            A single embedding vector.

        Raises:
            ValueError: If the Media object is invalid or embedding extraction fails.
            AttributeError: If the requested embedding type is not found in the Cohere response.
            Exception: If the Cohere API call fails.
        """
        image_uri = self._create_data_uri(media)

        try:
            a_client = self._get_a_client()
            response = await a_client.embed(
                texts=[],  # Required but not used for image embeddings
                images=[image_uri],  # Only one image per call
                model=self.model,
                input_type=self.DEFAULT_INPUT_TYPE,
                embedding_types=[self.embedding_type],
            )

            # Extract embeddings based on the requested type
            embeddings_data = response.embeddings

            if not hasattr(embeddings_data, self.embedding_type):
                raise AttributeError(
                    f"Cohere response does not contain image embeddings of type '{self.embedding_type}'."
                )

            vector = getattr(embeddings_data, self.embedding_type)[
                0
            ]  # Get the first (only) vector

            return vector

        except Exception as e:
            log.error(f"Error during Cohere embedding generation for media: {e}")
            raise e

    def _run(self, media: Media, **kwargs) -> list[float] | list[int]:
        """
        Pipeline component sync run method that generates embeddings for a single image.

        Args:
            media: A Media object containing the image data.
            **kwargs: Additional keyword arguments (ignored).

        Returns:
            A single embedding vector.
        """
        return self.embed(media)

    async def _a_run(self, media: Media, **kwargs) -> list[float] | list[int]:
        """
        Pipeline component async run method that generates embeddings for a single image.

        Args:
            media: A Media object containing the image data.
            **kwargs: Additional keyword arguments (ignored).

        Returns:
            A single embedding vector.
        """
        return await self.a_embed(media)


class NodeImageEmbedder(PipelineComponent):
    """Embeds image chunks using a CohereImageClient."""

    DEFAULT_EMBEDDING_NAME = "cohere_image_embedding"

    def __init__(
        self,
        client: CohereImageClient,
        embedding_name: str = DEFAULT_EMBEDDING_NAME,
    ):
        self.client = client
        self.embedding_name = embedding_name

    def _run(self, chunks: list[Chunk]) -> list[Chunk]:
        """
        Generates embeddings for a list of image Chunks.

        Args:
            chunks: A list of Chunk objects. Each chunk must have a `media_block` key
                    in its `metadata` dictionary containing a MediaBlock.

        Returns:
            The same list of Chunk objects, with the 'embeddings' list updated.
        """
        processed_chunks = copy.deepcopy(chunks)
        if not processed_chunks:
            return []

        # Process each chunk individually since we're now handling single images
        for chunk in processed_chunks:
            if not hasattr(chunk, "metadata") or not isinstance(chunk.metadata, dict):
                log.warning(
                    "Warning: Chunk is missing 'metadata' dictionary. Skipping."
                )
                continue

            media_block = chunk.metadata.get("media_block")
            if not media_block:
                log.warning(
                    f"Warning: Chunk with ID {chunk.id if hasattr(chunk, 'id') else 'N/A'} is missing 'media_block' in metadata. Skipping."
                )
                continue

            if not isinstance(media_block, MediaBlock):
                log.warning(
                    f"Warning: 'media_block' in chunk metadata is not a MediaBlock instance for chunk ID {chunk.id if hasattr(chunk, 'id') else 'N/A'}. Skipping."
                )
                continue

            # Validate that it's an image media block
            if media_block.media.media_type != "image":
                log.warning(
                    f"Warning: MediaBlock media_type is '{media_block.media.media_type}', expected 'image' for chunk ID {chunk.id if hasattr(chunk, 'id') else 'N/A'}. Skipping."
                )
                continue

            try:
                # Generate embedding for this single image
                embedding_vector = self.client.embed(media_block.media)

                new_embedding = DenseEmbedding(
                    name=self.embedding_name, vector=embedding_vector
                )
                if not hasattr(chunk, "embeddings") or chunk.embeddings is None:
                    chunk.embeddings = []
                    chunk.embeddings.append(new_embedding)

                # Remove the media_block from metadata after successful embedding generation
                # since it's no longer needed and can't be serialized to vectorstore
                chunk.metadata.pop("media_block", None)

            except (
                OSError,
                ValueError,
                FileNotFoundError,
                AttributeError,
                NotImplementedError,
            ) as e:
                # These errors are more likely to be raised by client.embed or _create_data_uri
                log.error(
                    f"Error processing image chunk {chunk.id if hasattr(chunk, 'id') else 'N/A'}: {e}. Skipping."
                )
                raise e
            except Exception as e:
                log.error(
                    f"Unexpected error during embedding generation for chunk {chunk.id if hasattr(chunk, 'id') else 'N/A'}: {e}. Skipping."
                )
                raise e

        return processed_chunks

    async def _a_run(self, chunks: list[Chunk]) -> list[Chunk]:
        """
        Asynchronously generates embeddings for a list of image Chunks.

        Args:
            chunks: A list of Chunk objects. Each chunk must have a `media_block` key
                    in its `metadata` dictionary containing a MediaBlock.

        Returns:
            The same list of Chunk objects, with the 'embeddings' list updated.
        """
        processed_chunks = copy.deepcopy(chunks)
        if not processed_chunks:
            return []

        # Process each chunk individually since we're now handling single images
        for chunk in processed_chunks:
            if not hasattr(chunk, "metadata") or not isinstance(chunk.metadata, dict):
                log.warning(
                    "Warning: Chunk is missing 'metadata' dictionary. Skipping."
                )
                continue

            media_block = chunk.metadata.get("media_block")
            if not media_block:
                log.warning(
                    f"Warning: Chunk with ID {chunk.id if hasattr(chunk, 'id') else 'N/A'} is missing 'media_block' in metadata. Skipping."
                )
                continue

            if not isinstance(media_block, MediaBlock):
                log.warning(
                    f"Warning: 'media_block' in chunk metadata is not a MediaBlock instance for chunk ID {chunk.id if hasattr(chunk, 'id') else 'N/A'}. Skipping."
                )
                continue

            # Validate that it's an image media block
            if media_block.media.media_type != "image":
                log.warning(
                    f"Warning: MediaBlock media_type is '{media_block.media.media_type}', expected 'image' for chunk ID {chunk.id if hasattr(chunk, 'id') else 'N/A'}. Skipping."
                )
                continue

            try:
                # Generate embedding for this single image asynchronously
                embedding_vector = await self.client.a_embed(media_block.media)

                new_embedding = DenseEmbedding(
                    name=self.embedding_name, vector=embedding_vector
                )
                if not hasattr(chunk, "embeddings") or chunk.embeddings is None:
                    chunk.embeddings = []
                    chunk.embeddings.append(new_embedding)

                # Remove the media_block from metadata after successful embedding generation
                # since it's no longer needed and can't be serialized to vectorstore
                chunk.metadata.pop("media_block", None)

            except (
                OSError,
                ValueError,
                FileNotFoundError,
                AttributeError,
                NotImplementedError,
            ) as e:
                # These errors are more likely to be raised by client.a_embed or _create_data_uri
                log.error(
                    f"Error processing image chunk {chunk.id if hasattr(chunk, 'id') else 'N/A'}: {e}. Skipping."
                )
                raise e
            except Exception as e:
                log.error(
                    f"Unexpected error during embedding generation for chunk {chunk.id if hasattr(chunk, 'id') else 'N/A'}: {e}. Skipping."
                )
                raise e

        return processed_chunks



================================================
FILE: datapizza-ai-embedders/cohere/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-embedders/cohere/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-embedders-cohere"
version = "0.0.3"
description = "Cohere embedder for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.0,<0.1.0",
    "cohere>=5.14.0,<6.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-embedders/cohere/datapizza/embedders/cohere/__init__.py
================================================
from .cohere import CohereEmbedder

__all__ = ["CohereEmbedder"]



================================================
FILE: datapizza-ai-embedders/cohere/datapizza/embedders/cohere/cohere.py
================================================
from datapizza.core.embedder import BaseEmbedder


class CohereEmbedder(BaseEmbedder):
    def __init__(
        self,
        *,
        api_key: str,
        model_name: str | None = None,
        base_url: str | None = None,
        input_type: str = "search_document",
    ):
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name

        self.input_type = input_type

        self.client = None
        self.a_client = None

    def _set_client(self):
        import cohere

        if not self.client:
            self.client = cohere.ClientV2(base_url=self.base_url, api_key=self.api_key)

    def _set_a_client(self):
        import cohere

        if not self.a_client:
            self.a_client = cohere.AsyncClientV2(
                base_url=self.base_url,
                api_key=self.api_key,
            )

    def embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_client()

        response = client.embed(
            texts=texts,
            model=model,
            input_type=self.input_type,
            embedding_types=["float"],
        )
        embeddings = response.embeddings.float
        return embeddings[0] if isinstance(text, str) else embeddings

    async def a_embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_a_client()
        response = await client.embed(
            texts=texts,
            model=model,
            input_type=self.input_type,
            embedding_types=["float"],
        )
        embeddings = response.embeddings.float
        return embeddings[0] if isinstance(text, str) else embeddings



================================================
FILE: datapizza-ai-embedders/cohere/tests/test_base.py
================================================
from datapizza.embedders.cohere import CohereEmbedder


def test_init_cohere_embedder():
    embedder = CohereEmbedder(api_key="test", base_url="test")
    assert embedder is not None



================================================
FILE: datapizza-ai-embedders/fastembedder/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-embedders/fastembedder/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-embedders-fastembedder"
version = "0.0.7"
description = "FastEmbed embedder for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.0,<0.1.0",
    "fastembed>=0.6.1",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-embedders/fastembedder/datapizza/embedders/fastembedder/__init__.py
================================================
from .fastembedder import FastEmbedder

__all__ = ["FastEmbedder"]



================================================
FILE: datapizza-ai-embedders/fastembedder/datapizza/embedders/fastembedder/fastembedder.py
================================================
import asyncio
import logging

import fastembed
from datapizza.core.embedder import BaseEmbedder
from datapizza.type import SparseEmbedding

log = logging.getLogger(__name__)


class FastEmbedder(BaseEmbedder):
    def __init__(
        self,
        model_name: str,
        embedding_name: str | None = None,
        cache_dir: str | None = None,
        **kwargs,
    ):
        self.model_name = model_name
        if embedding_name:
            self.embedding_name = embedding_name
        else:
            self.embedding_name = model_name

        self.cache_dir = cache_dir
        self.embedder = fastembed.SparseTextEmbedding(
            model_name=model_name, cache_dir=cache_dir, **kwargs
        )

    def embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> SparseEmbedding | list[SparseEmbedding]:
        # fastembed accepts both str and list[str]. Passing the list allows for batch processing.
        embeddings = self.embedder.embed(text)
        results = [
            SparseEmbedding(
                name=self.embedding_name,
                values=embedding.values.tolist(),
                indices=embedding.indices.tolist(),
            )
            for embedding in embeddings
        ]

        if isinstance(text, list):
            return results
        return results[0]

    async def a_embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> SparseEmbedding | list[SparseEmbedding]:
        return await asyncio.to_thread(self.embed, text)



================================================
FILE: datapizza-ai-embedders/fastembedder/tests/test_fastembedder.py
================================================
from datapizza.embedders.fastembedder import FastEmbedder


def test_init_fastembedder():
    embedder = FastEmbedder(model_name="Qdrant/bm25")
    assert embedder is not None



================================================
FILE: datapizza-ai-embedders/google/README.md
================================================
# Google Embedder

Google Generative AI embedder implementation for datapizza-ai

## Installation

```bash
pip install datapizza-ai-embedders-google
```

## Usage

```python
from datapizza.embedders.google import GoogleEmbedder

embedder = GoogleEmbedder(api_key="your-google-api-key")
embeddings = embedder.embed("Hello world", model_name="models/text-embedding-004")
```



================================================
FILE: datapizza-ai-embedders/google/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-embedders-google"
version = "0.0.5"
description = "Google embedder for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "google-genai>=1.3.0,<2.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-embedders/google/datapizza/embedders/google/__init__.py
================================================
from .google import GoogleEmbedder

__all__ = ["GoogleEmbedder"]



================================================
FILE: datapizza-ai-embedders/google/datapizza/embedders/google/google.py
================================================
from datapizza.core.embedder import BaseEmbedder

from google import genai
from google.genai import types


class GoogleEmbedder(BaseEmbedder):
    def __init__(
        self,
        *,
        api_key: str,
        model_name: str | None = None,
        task_type: str = "RETRIEVAL_DOCUMENT",
        output_dimensionality: int = 3072,
    ):
        self.api_key = api_key
        self.model_name = model_name
        self.task_type = task_type
        self.output_dimensionality = output_dimensionality

        self.client = None
        self.a_client = None

    def _set_client(self):
        if not self.client:
            client = genai.Client(api_key=self.api_key)
            self.client = client

    def _set_a_client(self):
        if not self.a_client:
            client = genai.Client(api_key=self.api_key)
            self.a_client = client

    def embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_client()

        result = client.models.embed_content(
            model=model,
            contents=texts,
            config=types.EmbedContentConfig(
                task_type=self.task_type,
                output_dimensionality=self.output_dimensionality,
            ),
        )

        res = [embedding.values for embedding in result.embeddings]

        return res[0] if isinstance(text, str) else res

    async def a_embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_a_client()

        result = await client.aio.models.embed_content(
            model=model,
            contents=texts,
            config=types.EmbedContentConfig(
                task_type=self.task_type,
                output_dimensionality=self.output_dimensionality,
            ),
        )

        res = [embedding.values for embedding in result.embeddings]

        return res[0] if isinstance(text, str) else res



================================================
FILE: datapizza-ai-embedders/google/tests/test_google_embedder.py
================================================
from datapizza.embedders.google import GoogleEmbedder


def test_google_embedder_init():
    embedder = GoogleEmbedder(api_key="test-key")
    assert embedder.api_key == "test-key"
    assert embedder.client is None
    assert embedder.a_client is None



================================================
FILE: datapizza-ai-embedders/mistral/README.md
================================================
# Mistral AI embedder

Mistral AI embedder implementation for datapizza-ai

## Installation

```bash
pip install datapizza-ai-embedders-mistral
```

## Usage

```python
from datapizza.embedders.mistral  import MistralEmbedder

embedder = MistralEmbedder(api_key="your-google-api-key")
embeddings = embedder.embed("Hello world", model_name="models/text-embedding-004")
```



================================================
FILE: datapizza-ai-embedders/mistral/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-embedders-mistral"
version = "0.0.1"
description = "Mistral embedder for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Ivan Buttinoni", email = "ivan.buttinoni@cibi.it"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.10,<0.1.0",
    "mistralai>=1.2.0,<2.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-embedders/mistral/datapizza/embedders/mistral/__init__.py
================================================
from .mistral import MistralEmbedder

__all__ = ["MistralEmbedder"]



================================================
FILE: datapizza-ai-embedders/mistral/datapizza/embedders/mistral/mistral.py
================================================
from datapizza.core.embedder import BaseEmbedder


class MistralEmbedder(BaseEmbedder):
    def __init__(
        self,
        *,
        api_key: str,
        base_url: str | None = None,
        model_name: str | None = None,
    ):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url

        self.client = None
        self.a_client = None

    def _set_client(self) -> None:
        import mistralai

        if not self.client:
            self.client = mistralai.Mistral(api_key=self.api_key)

    def _set_a_client(self) -> None:
        import mistralai

        if not self.a_client:
            self.a_client = mistralai.Mistral(api_key=self.api_key)

    def embed(self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        """Embed text into dense embeddings using Mistral API.

        Args:
            text: Single text string or list of text strings to embed.
            model_name: Optional model name override. If not provided, uses instance model_name.

        Returns:
            list[float] | list[list[float]]: Single embedding vector for string input, or list of embeddings for list input.

        Raises:
            ValueError: If model name is not provided as argument or instance.
        """
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_client()
        embedding_response = client.embeddings.create(
            inputs=texts,
            model=model,
            server_url=self.base_url
        )

        embeddings = [embedding_response_data.embedding for embedding_response_data in embedding_response.data]
        return embeddings[0] if isinstance(text, str) else embeddings

    async def a_embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        """Embed text into dense embeddings using Mistral API asynchronously.

        Args:
            text: Single text string or list of text strings to embed.
            model_name: Optional model name override. If not provided, uses instance model_name.

        Returns:
            list[float] | list[list[float]]: Single embedding vector for string input, or list of embeddings for list input.

        Raises:
            ValueError: If model name is not provided as argument or instance.
        """
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_a_client()
        embedding_response = await client.embeddings.create_async(
            inputs=texts,
            model=model,
            server_url=self.base_url
        )

        embeddings = [embedding_response_data.embedding for embedding_response_data in embedding_response.data]
        return embeddings[0] if isinstance(text, str) else embeddings





================================================
FILE: datapizza-ai-embedders/mistral/tests/test_mistral_embedder.py
================================================
from datapizza.embedders.mistral import MistralEmbedder


def test_openai_embedder_init():
    embedder = MistralEmbedder(api_key="mistral-test-key")
    assert embedder.api_key == "mistral-test-key"
    assert embedder.base_url is None
    assert embedder.client is None
    assert embedder.a_client is None


def test_openai_embedder_init_with_base_url():
    embedder = MistralEmbedder(api_key="mistral-test-key", base_url="https://api.mistral.ai")
    assert embedder.api_key == "mistral-test-key"
    assert embedder.base_url == "https://api.mistral.ai"



================================================
FILE: datapizza-ai-embedders/openai/README.md
================================================
# OpenAI Embedder

OpenAI embedder implementation.

## Installation

```bash
pip install datapizza-ai-embedders-openai
```

## Usage

```python
from datapizza.embedders.openai import OpenAIEmbedder

embedder = OpenAIEmbedder(api_key="your-openai-api-key")
embeddings = embedder.embed("Hello world", model_name="text-embedding-ada-002")
```



================================================
FILE: datapizza-ai-embedders/openai/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-embedders-openai"
version = "0.0.5"
description = "OpenAI embedder for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "openai>=2.0.0,<3.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-embedders/openai/datapizza/embedders/openai/__init__.py
================================================
from .openai import OpenAIEmbedder

__all__ = ["OpenAIEmbedder"]



================================================
FILE: datapizza-ai-embedders/openai/datapizza/embedders/openai/openai.py
================================================
from datapizza.core.embedder import BaseEmbedder


class OpenAIEmbedder(BaseEmbedder):
    def __init__(
        self,
        *,
        api_key: str,
        model_name: str | None = None,
        base_url: str | None = None,
    ):
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name

        self.client = None
        self.a_client = None

    def _set_client(self):
        import openai

        if not self.client:
            self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)

    def _set_a_client(self):
        import openai

        if not self.a_client:
            self.a_client = openai.AsyncOpenAI(
                api_key=self.api_key, base_url=self.base_url
            )

    def embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_client()

        response = client.embeddings.create(input=texts, model=model)

        embeddings = [embedding.embedding for embedding in response.data]
        return embeddings[0] if isinstance(text, str) else embeddings

    async def a_embed(
        self, text: str | list[str], model_name: str | None = None
    ) -> list[float] | list[list[float]]:
        model = model_name or self.model_name
        if not model:
            raise ValueError("Model name is required.")

        texts = [text] if isinstance(text, str) else text

        client = self._get_a_client()
        response = await client.embeddings.create(input=texts, model=model)

        embeddings = [embedding.embedding for embedding in response.data]
        return embeddings[0] if isinstance(text, str) else embeddings



================================================
FILE: datapizza-ai-embedders/openai/tests/test_openai_embedder.py
================================================
from datapizza.embedders.openai import OpenAIEmbedder


def test_openai_embedder_init():
    embedder = OpenAIEmbedder(api_key="test-key")
    assert embedder.api_key == "test-key"
    assert embedder.base_url is None
    assert embedder.client is None
    assert embedder.a_client is None


def test_openai_embedder_init_with_base_url():
    embedder = OpenAIEmbedder(api_key="test-key", base_url="https://api.openai.com/v1")
    assert embedder.api_key == "test-key"
    assert embedder.base_url == "https://api.openai.com/v1"



================================================
FILE: datapizza-ai-eval/metrics.py
================================================
import math

import numpy as np
import sacrebleu
from rouge_score import rouge_scorer
from sklearn.metrics.pairwise import cosine_similarity


# Helper for similarity-based metrics
def _get_similarity_scores(
    retrieved_embeddings: list[np.ndarray], ground_truth_embeddings: list[np.ndarray]
) -> np.ndarray:
    """Calculates a similarity matrix between retrieved and ground truth embeddings."""
    if not retrieved_embeddings or not ground_truth_embeddings:
        return np.array([[]])
    # Ensure embeddings are 2D arrays for cosine_similarity
    ret_emb_np = np.array(retrieved_embeddings)
    gt_emb_np = np.array(ground_truth_embeddings)
    if ret_emb_np.ndim == 1:  # Single retrieved embedding
        ret_emb_np = ret_emb_np.reshape(1, -1)
    if gt_emb_np.ndim == 1:  # Single ground truth embedding
        gt_emb_np = gt_emb_np.reshape(1, -1)
    return cosine_similarity(ret_emb_np, gt_emb_np)


def precision_at_k_exact(
    retrieved_chunks: list[str], ground_truth_chunks: list[str], k: int
) -> float:
    """Calculates Precision@k based on exact string matches.

    Precision@k measures the fraction of relevant items among the first `k`
    retrieved items. Relevance is determined by an exact match between a
    retrieved chunk and any ground truth chunk.

    Args:
        retrieved_chunks (list[str]): The list of strings retrieved by the system.
            Assumes that the retrieved chunks are unique.
        ground_truth_chunks (list[str]): The list of ground truth strings
            considered relevant. Assumes that the ground truth chunks are unique.
        k (int): The number of top retrieved chunks to evaluate.

    Returns:
        float: The Precision@k score, a value between 0.0 and 1.0.
            Returns 0.0 if `k` is 0 or `retrieved_chunks` is empty.

    Raises:
        ValueError: If `ground_truth_chunks` is empty.
    """
    if not retrieved_chunks or k == 0:
        return 0.0
    if not ground_truth_chunks:
        raise ValueError("Ground truth chunks cannot be empty")

    top_k_retrieved = retrieved_chunks[:k]
    relevant_hits = 0
    # Use set intersection to find relevant hits, assuming ground truth chunks and top k retrieved chunks are unique
    relevant_hits = len(set(top_k_retrieved) & set(ground_truth_chunks))

    return relevant_hits / k


def precision_at_k_similarity(
    retrieved_embeddings: list[np.ndarray],
    ground_truth_embeddings: list[np.ndarray],
    k: int,
    similarity_threshold: float = 0.8,
) -> float:
    """Calculates Precision@k based on cosine similarity between embeddings.

    Precision@k measures the fraction of relevant items among the top `k`
    retrieved items. A retrieved item is considered relevant if its cosine
    similarity to *any* ground truth item's embedding is at or above the
    `similarity_threshold`.

    Args:
        retrieved_embeddings (list[np.ndarray]): list of embedding vectors
            (NumPy arrays) for the retrieved items.
        ground_truth_embeddings (list[np.ndarray]): list of embedding vectors
            (NumPy arrays) for the ground truth items.
        k (int): The number of top retrieved embeddings to consider.
        similarity_threshold (float, optional): The cosine similarity threshold
            for relevance. Defaults to 0.8.

    Returns:
        float: The Precision@k score (a value between 0.0 and 1.0).
            Returns 0.0 if `k` is 0 or `retrieved_embeddings` is empty.

    Raises:
        ValueError: If `ground_truth_embeddings` is empty.
    """
    if not retrieved_embeddings or k == 0:
        return 0.0
    if not ground_truth_embeddings:
        raise ValueError("Ground truth embeddings cannot be empty")

    top_k_retrieved_embeddings = retrieved_embeddings[:k]

    similarity_matrix = _get_similarity_scores(
        top_k_retrieved_embeddings, ground_truth_embeddings
    )

    if similarity_matrix.size == 0:
        raise ValueError(
            "Similarity matrix cannot be empty",
            "This code should not be reached, please check the inputs",
        )

    relevant_hits = 0
    # For each of the top k retrieved items, check if it's similar to ANY ground truth item.
    for i in range(similarity_matrix.shape[0]):  # Iterate over retrieved items
        if np.any(similarity_matrix[i, :] >= similarity_threshold):
            relevant_hits += 1

    return relevant_hits / k


def recall_at_k_exact(
    retrieved_chunks: list[str], ground_truth_chunks: list[str], k: int
) -> float:
    """
    Calculates Recall@k based on exact string matches.

    Recall@k measures the fraction of ground truth items that appear in the top `k`
    retrieved items. A ground truth item is considered recalled if it exactly matches
    any of the top `k` retrieved items.

    Args:
        retrieved_chunks (list[str]): list of strings representing the retrieved items.
        ground_truth_chunks (list[str]): list of strings representing the ground truth items.
        k (int): The number of top retrieved chunks to consider.

    Returns:
        float: The Recall@k score (a value between 0.0 and 1.0).
            Returns 0.0 if `k` is 0 or `retrieved_chunks` is empty.

    Raises:
        ValueError: If `ground_truth_chunks` is empty.
    """
    if not retrieved_chunks or k == 0:
        return 0.0
    if not ground_truth_chunks:
        raise ValueError("Ground truth chunks cannot be empty")

    top_k_retrieved = retrieved_chunks[:k]
    relevant_hits = 0
    # Use a set for faster lookups of ground truth for uniqueness
    ground_truth_set = set(ground_truth_chunks)
    # Count relevant hits (ground truth items found in top_k retrieved items)
    retrieved_set = set(top_k_retrieved)
    hit_gt_items = ground_truth_set.intersection(retrieved_set)
    relevant_hits = len(hit_gt_items)

    return relevant_hits / len(ground_truth_set)


def recall_at_k_similarity(
    retrieved_embeddings: list[np.ndarray],
    ground_truth_embeddings: list[np.ndarray],
    k: int,
    similarity_threshold: float = 0.8,
) -> float:
    """
    Calculates Recall@k based on cosine similarity between embedding vectors.

    Recall@k measures the fraction of ground truth items that are similar to any of the top `k`
    retrieved items. A ground truth item is considered recalled if its cosine similarity
    with any of the top `k` retrieved items exceeds the specified threshold.

    Args:
        retrieved_embeddings (list[np.ndarray]): list of embedding vectors representing the retrieved items.
        ground_truth_embeddings (list[np.ndarray]): list of embedding vectors representing the ground truth items.
        k (int): The number of top retrieved embeddings to consider.
        similarity_threshold (float, optional): The minimum cosine similarity threshold for considering
            a ground truth item as recalled. Defaults to 0.8.

    Returns:
        float: The Recall@k score (a value between 0.0 and 1.0).
            Returns 0.0 if `k` is 0 or `retrieved_embeddings` is empty.

    Raises:
        ValueError: If `ground_truth_embeddings` is empty or if the similarity matrix computation fails.
    """
    if not ground_truth_embeddings:
        raise ValueError("Ground truth embeddings cannot be empty")
    if not retrieved_embeddings or k == 0:
        return 0.0

    top_k_retrieved_embeddings = retrieved_embeddings[:k]

    similarity_matrix = _get_similarity_scores(
        top_k_retrieved_embeddings, ground_truth_embeddings
    )

    if similarity_matrix.size == 0:
        raise ValueError(
            "Similarity matrix cannot be empty",
            "This code should not be reached, please check the inputs",
        )

    # For each ground truth item, check if it's similar to ANY of the top k retrieved items.
    # This counts how many ground truth items were successfully recalled.
    recalled_gt_count = 0
    for j in range(similarity_matrix.shape[1]):  # Iterate over ground truth items
        if np.any(similarity_matrix[:, j] >= similarity_threshold):
            recalled_gt_count += 1

    return recalled_gt_count / len(ground_truth_embeddings)


def f1_at_k_exact(
    retrieved_chunks: list[str], ground_truth_chunks: list[str], k: int
) -> float:
    """Calculates F1-score@k based on exact string matches.

    F1-score@k is the harmonic mean of Precision@k and Recall@k, providing a balanced
    measure of retrieval performance that considers both precision and recall.

    Args:
        retrieved_chunks (list[str]): The list of strings retrieved by the system.
            Assumes that the retrieved chunks are unique.
        ground_truth_chunks (list[str]): The list of ground truth strings
            considered relevant. Assumes that the ground truth chunks are unique.
        k (int): The number of top retrieved chunks to evaluate.

    Returns:
        float: The F1-score@k value, between 0.0 and 1.0.
            Returns 0.0 if either precision or recall is 0.
    """
    precision = precision_at_k_exact(retrieved_chunks, ground_truth_chunks, k)
    recall = recall_at_k_exact(retrieved_chunks, ground_truth_chunks, k)

    if precision + recall == 0:
        return 0.0

    return 2 * (precision * recall) / (precision + recall)


def f1_at_k_similarity(
    retrieved_embeddings: list[np.ndarray],
    ground_truth_embeddings: list[np.ndarray],
    k: int,
    similarity_threshold: float = 0.8,
) -> float:
    """Calculates F1-score@k based on cosine similarity between embeddings.

    F1-score@k is the harmonic mean of Precision@k and Recall@k, providing a balanced
    measure of retrieval performance that considers both precision and recall.
    Relevance is determined by cosine similarity exceeding the specified threshold.

    Args:
        retrieved_embeddings (list[np.ndarray]): The list of embedding vectors retrieved by the system.
        ground_truth_embeddings (list[np.ndarray]): The list of ground truth embedding vectors
            considered relevant.
        k (int): The number of top retrieved embeddings to evaluate.
        similarity_threshold (float, optional): The minimum cosine similarity threshold
            for considering two embeddings as similar. Defaults to 0.8.

    Returns:
        float: The F1-score@k value, between 0.0 and 1.0.
            Returns 0.0 if either precision or recall is 0.
    """
    precision = precision_at_k_similarity(
        retrieved_embeddings, ground_truth_embeddings, k, similarity_threshold
    )
    recall = recall_at_k_similarity(
        retrieved_embeddings, ground_truth_embeddings, k, similarity_threshold
    )

    if precision + recall == 0:
        return 0.0

    return 2 * (precision * recall) / (precision + recall)


def log_rank_score(rank, n, gamma=1.0):
    """Compute log-rank score for a given 1-based rank."""
    if n == 1:
        # If there's only one item, its rank must be 1 for it to be considered.
        # The score reflects a perfect ranking for this single item if it's relevant.
        return 1.0

    # For n > 1:
    # Note: If gamma = 0, this formula results in math.log(1)/math.log(1) = 0/0 = NaN.
    # Users should ensure gamma > 0 or handle the gamma=0 case separately if needed,
    # as the behavior for gamma=0 is not explicitly defined by this formula here.
    numerator = math.log(1 + gamma * (rank - 1))
    denominator = math.log(1 + gamma * (n - 1))

    # With n > 1 and assuming gamma >= 0:
    # If gamma > 0, denominator = log(1 + positive) > log(1) = 0. So, no division by zero.
    # If gamma = 0, denominator = log(1) = 0. Numerator is also log(1)=0. This leads to NaN.
    # The function relies on the caller to provide meaningful gamma values.
    return 1 - (numerator / denominator)


def hybrid_log_rank_score_exact(
    retrieved_chunks: list[str],
    ground_truth_chunks: list[str],
    gamma: float = 1.0,
    alpha: float = 0.5,
) -> float:
    """
    Compute hybrid score that combines recall and rank-based scoring using exact matches.

    Parameters:
    - retrieved_chunks: list of chunk IDs (ranked top to bottom)
    - ground_truth_chunks: list of relevant chunk IDs (must not be empty).
    - gamma: log curve control parameter for log_rank_score.
    - alpha: recall vs rank-quality tradeoff [0 (only rank), 1 (only recall)].

    Returns:
    - hybrid_score: float between 0 and 1.

    Raises:
        ValueError: If `ground_truth_chunks` is empty.
    """
    if not ground_truth_chunks:
        raise ValueError(
            "Ground truth chunks cannot be empty for hybrid_log_rank_score_exact."
        )

    N = len(retrieved_chunks)
    if N == 0:  # No retrieved chunks
        return 0.0  # Recall is 0, rank quality is 0

    gt_set = set(ground_truth_chunks)  # Not empty due to the check above
    retrieved_set = set(retrieved_chunks)

    retrieved_relevant = gt_set.intersection(retrieved_set)

    # Recall component
    recall = len(retrieved_relevant) / len(gt_set)  # len(gt_set) > 0

    # Rank-quality component
    rank_scores = []
    # N (len(retrieved_chunks)) is > 0 here.
    for chunk in retrieved_relevant:
        try:
            rank = retrieved_chunks.index(chunk) + 1  # 1-based rank
            score = log_rank_score(rank, N, gamma)
            rank_scores.append(score)
        except ValueError:
            # This should ideally not happen if chunk is from retrieved_relevant.
            # Handles cases like duplicates in retrieved_chunks not reflected in retrieved_set logic if any.
            pass

    rank_quality = sum(rank_scores) / len(gt_set)  # len(gt_set) > 0

    # Hybrid score
    return alpha * recall + (1 - alpha) * rank_quality


def hybrid_log_rank_score_similarity(
    retrieved_embeddings: list[np.ndarray],
    ground_truth_embeddings: list[np.ndarray],
    similarity_threshold: float = 0.8,
    gamma: float = 1.0,
    alpha: float = 0.5,
) -> float:
    """
    Compute hybrid score that combines recall and rank-based scoring using cosine similarity.

    Parameters:
    - retrieved_embeddings: list of embedding vectors for retrieved items.
    - ground_truth_embeddings: list of embedding vectors for ground truth items (must not be empty).
    - similarity_threshold: The cosine similarity threshold for relevance.
    - gamma: log curve control parameter for log_rank_score.
    - alpha: recall vs rank-quality tradeoff [0 (only rank), 1 (only recall)].

    Returns:
    - hybrid_score: float between 0 and 1.

    Raises:
        ValueError: If `ground_truth_embeddings` is empty or if similarity matrix calculation fails unexpectedly.
    """
    if not ground_truth_embeddings:
        raise ValueError(
            "Ground truth embeddings cannot be empty for hybrid_log_rank_score_similarity."
        )

    num_retrieved = len(retrieved_embeddings)
    num_gt = len(ground_truth_embeddings)  # Known to be > 0

    if num_retrieved == 0:
        return 0.0  # No retrieved items, so recall is 0, rank quality is 0.

    similarity_matrix = _get_similarity_scores(
        retrieved_embeddings, ground_truth_embeddings
    )

    # _get_similarity_scores handles empty retrieved_embeddings internally.
    # If num_retrieved > 0 and num_gt > 0, similarity_matrix should be valid.
    if similarity_matrix.shape != (num_retrieved, num_gt):
        # This might indicate an issue if _get_similarity_scores behaves unexpectedly with non-empty inputs
        # or if inputs to _get_similarity_scores were altered (e.g. by slicing) before call in other contexts.
        # For this function, direct pass-through should yield expected dimensions.
        pass  # Allow to proceed, subsequent operations might fail if shape is truly problematic.

    # Recall component: Fraction of ground truth items similar to at least one retrieved item.
    recalled_gt_indices = set()
    # similarity_matrix shape is (num_retrieved, num_gt)
    for j in range(num_gt):  # Iterate over ground truth items (columns)
        if np.any(similarity_matrix[:, j] >= similarity_threshold):
            recalled_gt_indices.add(j)

    recall = len(recalled_gt_indices) / num_gt

    # Rank-quality component:
    # For each *recalled* ground truth item, find the log_rank_score of the *best-ranked* retrieved item that hit it.
    # Sum these scores and normalize by the total number of ground truth items.

    best_rank_for_recalled_gt = {}  # Stores {gt_index: best_rank (1-based)}
    for gt_idx in recalled_gt_indices:
        min_rank_for_this_gt = float("inf")
        for i in range(num_retrieved):  # Iterate over retrieved items (rows)
            if similarity_matrix[i, gt_idx] >= similarity_threshold:
                min_rank_for_this_gt = min(min_rank_for_this_gt, i + 1)
        if min_rank_for_this_gt != float("inf"):
            best_rank_for_recalled_gt[gt_idx] = min_rank_for_this_gt

    rank_scores_sum = 0.0
    # N for log_rank_score is num_retrieved (which is > 0 here)
    for _, rank in best_rank_for_recalled_gt.items():
        rank_scores_sum += log_rank_score(rank, num_retrieved, gamma)

    rank_quality = rank_scores_sum / num_gt  # num_gt > 0

    return alpha * recall + (1 - alpha) * rank_quality


def bleu_score(retrieved_chunk: str, ground_truth_chunks: list[str]) -> float:
    """
    Calculates BLEU score for a single retrieved chunk against multiple ground truth references using sacrebleu.
    Args:
        retrieved_chunk: The retrieved string (hypothesis).
        ground_truth_chunks: A list of ground truth strings (references).
    Returns:
        BLEU score (0-100 scale from sacrebleu, will be divided by 100).
    """
    if not ground_truth_chunks:
        return 0.0  # No references to compare against
    if not retrieved_chunk:
        # If hypothesis is empty, BLEU is 0, unless all references are also empty.
        if all(not gt for gt in ground_truth_chunks):
            return 1.0  # Or 100.0 then scaled. Let's return 0.0 to be consistent with no overlap.
            # sacrebleu sentence_bleu with empty hypothesis and non-empty refs gives 0.
            # If refs are also empty, it might raise an error or give 0.
            # Let's ensure refs are not all empty for the 1.0 case
        is_any_ref_non_empty = any(bool(gt) for gt in ground_truth_chunks)
        if not is_any_ref_non_empty:  # All refs are empty, hypothesis is empty
            return 1.0
        return 0.0  # Empty hypothesis, at least one non-empty ref

    # sacrebleu.sentence_bleu expects a single hypothesis string and a list of reference strings.
    # It returns a BLEUScore object. The actual score is in score.score
    # Note: sacrebleu scores are typically 0-100. We should scale to 0-1 for consistency with other metrics.
    try:
        # Sacrebleu's sentence_bleu can take a list of strings for references
        bleu_result = sacrebleu.sentence_bleu(
            retrieved_chunk,
            ground_truth_chunks,  # tokenize="flores101"
        )  # Using a common tokenizer
        return bleu_result.score / 100.0
    except Exception as e:
        # Could be due to various issues, e.g., empty references with non-empty hypothesis
        # For robustness, return 0.0 on error. Check sacrebleu docs for specific error handling.
        # print(f"Sacrebleu error: {e}") # For debugging
        print(f"Sacrebleu error: {e}")
        return 0.0


def corpus_bleu_score(
    retrieved_chunks: list[str], ground_truth_chunks: list[str]
) -> float:
    """
    Calculates an aggregated BLEU score based on maximizing BLEU for each ground truth chunk.

    For each ground_truth_chunk:
    1. It finds the retrieved_chunk that maximizes the BLEU score when paired with
       the current ground_truth_chunk (the single ground_truth_chunk is used as the reference list for bleu_score).
    2. This maximum BLEU score (a float between 0.0 and 1.0) is recorded.
    The final corpus BLEU score is the average of these recorded maximum BLEU scores.

    This aggregation strategy is analogous to how `corpus_rouge_scores` aggregates ROUGE metrics.

    Args:
        retrieved_chunks (list[str]): A list of retrieved strings (hypotheses).
        ground_truth_chunks (list[str]): A list of ground truth strings (references).
            Each string in this list is treated as an individual reference to be maximized against.

    Returns:
        float: The aggregated BLEU score, a value between 0.0 and 1.0.
               Returns 0.0 if `retrieved_chunks` is empty and `ground_truth_chunks` is not.
               The behavior for both empty `retrieved_chunks` and empty `ground_truth_chunks`
               is governed by the `ValueError` for empty `ground_truth_chunks`.

    Raises:
        ValueError: If `ground_truth_chunks` is empty.
    """
    if not ground_truth_chunks:
        raise ValueError("ground_truth_chunks cannot be empty for corpus_bleu.")

    if not retrieved_chunks:
        # If there are no retrieved chunks, no matches can be found for any ground truth.
        # Since ground_truth_chunks is guaranteed to be non-empty here,
        # the average max BLEU score will be 0.
        return 0.0

    num_gt = len(ground_truth_chunks)
    sum_of_max_bleu_scores = 0.0

    for gt_chunk in ground_truth_chunks:
        max_bleu_for_this_gt = 0.0  # BLEU scores are between 0.0 and 1.0

        # Determine the best BLEU score for the current gt_chunk against all retrieved_chunks
        for r_chunk in retrieved_chunks:
            # bleu_score expects a list of references, so [gt_chunk] is used.
            # The bleu_score function handles cases like r_chunk and/or gt_chunk being empty.
            current_bleu = bleu_score(r_chunk, [gt_chunk])
            if current_bleu > max_bleu_for_this_gt:
                max_bleu_for_this_gt = current_bleu

        sum_of_max_bleu_scores += max_bleu_for_this_gt

    # The average of the maximum BLEU scores found for each ground truth chunk.
    # num_gt is guaranteed to be > 0 here due to the initial check.
    return sum_of_max_bleu_scores / num_gt


def rouge_score(
    retrieved_chunk: str, ground_truth_chunk: str
) -> dict[str, dict[str, float]]:
    """
    Calculates ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L).
    Args:
        retrieved_chunk: The retrieved string (hypothesis).
        ground_truth_chunk: The ground truth string (reference).
    Returns:
        A dictionary where keys are ROUGE types ('rouge1', 'rouge2', 'rougeL')
        and values are dictionaries containing 'precision', 'recall', and 'fmeasure' scores.
    """
    rouge_types = ["rouge1", "rouge2", "rougeL"]
    if not retrieved_chunk or not ground_truth_chunk:
        # If either is empty, ROUGE scores are typically 0 for P, R, F.
        return {
            rtype: {"precision": 0.0, "recall": 0.0, "fmeasure": 0.0}
            for rtype in rouge_types
        }

    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)
    # The scores object is a dict where keys are ROUGE types (e.g., 'rouge1')
    # and values are Score objects (precision, recall, fmeasure).
    scores_obj = scorer.score(ground_truth_chunk, retrieved_chunk)

    # Extract P, R, F scores for convenience
    output_scores = {}
    for rtype in rouge_types:
        output_scores[rtype] = {
            "precision": scores_obj[rtype].precision,
            "recall": scores_obj[rtype].recall,
            "fmeasure": scores_obj[rtype].fmeasure,
        }
    return output_scores


def corpus_rouge_scores(
    retrieved_chunks: list[str], ground_truth_chunks: list[str]
) -> dict[str, dict[str, float]]:
    """
    Calculates ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) using a specific aggregation.

    For each ROUGE type (e.g., ROUGE-1):
    1. For each ground_truth_chunk:
        a. It finds the retrieved_chunk that maximizes the 'recall' for that ROUGE type
           when paired with the current ground_truth_chunk.
        b. The 'precision', 'recall', and 'fmeasure' from this specific
           (retrieved_chunk_maximizing_recall, ground_truth_chunk) pair are recorded.
    2. The recorded 'precision' scores (one for each ground_truth_chunk) are averaged
       to get the final corpus 'precision' for that ROUGE type.
    3. The same averaging is done for 'recall' and 'fmeasure' scores.

    Args:
        retrieved_chunks (list[str]): A list of retrieved strings (hypotheses).
        ground_truth_chunks (list[str]): A list of ground truth strings (references).

    Returns:
        dict[str, dict[str, float]]: A dictionary where keys are ROUGE types
                                     ('rouge1', 'rouge2', 'rougeL') and values are
                                     dictionaries containing the aggregated 'precision',
                                     'recall', and 'fmeasure' scores.
                                     Returns all 0.0 if retrieved_chunks is empty.
    Raises:
        ValueError: If `ground_truth_chunks` is empty.
    """
    rouge_types = ["rouge1", "rouge2", "rougeL"]
    default_metrics = {"precision": 0.0, "recall": 0.0, "fmeasure": 0.0}

    final_corpus_scores = {rtype: default_metrics.copy() for rtype in rouge_types}

    if not retrieved_chunks:
        return final_corpus_scores  # All zeros if no retrieved chunks

    if not ground_truth_chunks:
        raise ValueError("ground_truth_chunks cannot be empty for corpus_rouge_scores.")

    num_gt = len(ground_truth_chunks)

    # Stores the sum of P, R, F1 from the best-recall pairs for each ROUGE type
    summed_scores_for_corpus = {rtype: default_metrics.copy() for rtype in rouge_types}

    for gt_chunk in ground_truth_chunks:
        # For this gt_chunk, find the best r_chunk for each ROUGE type based on its recall
        # These will store the P,R,F dict of the best pair for each ROUGE type
        best_scores_for_this_gt_per_rtype = dict.fromkeys(rouge_types)
        # Tracks the max recall found for this gt_chunk for each ROUGE type
        max_recall_for_this_gt_per_rtype = dict.fromkeys(rouge_types, -1.0)

        for r_chunk in retrieved_chunks:
            # rouge_score returns: {'rouge1': {'precision':..,'recall':..,'fmeasure':..}, ...}
            pair_all_rouge_scores = rouge_score(r_chunk, gt_chunk)

            for rtype in rouge_types:
                current_rtype_metrics = pair_all_rouge_scores[rtype]
                current_recall = current_rtype_metrics["recall"]

                if current_recall > max_recall_for_this_gt_per_rtype[rtype]:
                    max_recall_for_this_gt_per_rtype[rtype] = current_recall
                    best_scores_for_this_gt_per_rtype[rtype] = current_rtype_metrics

        # After checking all r_chunks for the current gt_chunk:
        # Add the P, R, F1 from the "best recall" pair to the overall sums
        for rtype in rouge_types:
            scores_from_best_pair = best_scores_for_this_gt_per_rtype[rtype]
            if scores_from_best_pair:
                summed_scores_for_corpus[rtype]["precision"] += scores_from_best_pair[
                    "precision"
                ]
                summed_scores_for_corpus[rtype]["recall"] += scores_from_best_pair[
                    "recall"
                ]
                summed_scores_for_corpus[rtype]["fmeasure"] += scores_from_best_pair[
                    "fmeasure"
                ]
            # If scores_from_best_pair is None (e.g., all r_chunks had 0 recall, or max_recall remained -1.0),
            # it means this gt_chunk contributes 0.0 to the sums for this rtype,
            # which is implicitly handled as summed_scores_for_corpus started at 0.0.

    # Calculate averages
    if num_gt > 0:
        for rtype in rouge_types:
            final_corpus_scores[rtype]["precision"] = (
                summed_scores_for_corpus[rtype]["precision"] / num_gt
            )
            final_corpus_scores[rtype]["recall"] = (
                summed_scores_for_corpus[rtype]["recall"] / num_gt
            )
            final_corpus_scores[rtype]["fmeasure"] = (
                summed_scores_for_corpus[rtype]["fmeasure"] / num_gt
            )

    return final_corpus_scores



================================================
FILE: datapizza-ai-modules/parsers/azure/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-modules/parsers/azure/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-parsers-azure"
version = "0.0.5"
description = "Azure Document Intelligence parser for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "aiofiles>=24.1.0",
    "azure-ai-documentintelligence>=1.0.1,<2.0.0",
    "pymupdf>=1.25.4,<2.0.0",
    "pillow>=11.3.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-modules/parsers/azure/datapizza/modules/parsers/azure/__init__.py
================================================
from .azure_parser import AzureParser

__all__ = ["AzureParser"]



================================================
FILE: datapizza-ai-modules/parsers/azure/datapizza/modules/parsers/azure/azure_parser.py
================================================
import json
from typing import Any

import aiofiles
from datapizza.core.modules.parser import Parser
from datapizza.core.utils import extract_media
from datapizza.type import Media, MediaNode, Node, NodeType

from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.aio import (
    DocumentIntelligenceClient as AsyncDocumentIntelligenceClient,
)
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, AnalyzeResult
from azure.core.credentials import AzureKeyCredential


class AzureParser(Parser):
    """
    Parser that creates a hierarchical tree structure from Azure AI Document Intelligence response.
    The hierarchy goes from document -> pages -> paragraphs/tables -> lines/cells -> words.

    params:
        api_key: str
        endpoint: str
        result_type: str = "markdown", "text"
    """

    def __init__(self, api_key: str, endpoint: str, result_type: str = "text"):
        self.api_key = api_key
        self.endpoint = endpoint
        self.result_type = result_type
        self.parser = None  # self._create_parser()
        self.a_parser = None  # self._create_a_parser()

    def _create_parser(self):
        document_intelligence_client = DocumentIntelligenceClient(
            endpoint=self.endpoint, credential=AzureKeyCredential(self.api_key)
        )
        return document_intelligence_client

    def _create_a_parser(self):
        parser = AsyncDocumentIntelligenceClient(
            endpoint=self.endpoint, credential=AzureKeyCredential(self.api_key)
        )
        return parser

    def _get_parser(self):
        if not self.parser:
            self.parser = self._create_parser()
        return self.parser

    def _get_a_parser(self):
        if not self.a_parser:
            self.a_parser = self._create_a_parser()
        return self.a_parser

    def _parse_file(self, file_path: str) -> Node:
        """Parse an Azure Document Intelligence JSON file into a Node structure."""
        with open(file_path) as file:
            json_data = json.load(file)

        return self._parse_json(json_data, file_path=file_path)

    def _get_missing_paragraphs(self, json_data: dict) -> list[str]:
        """Get missing paragraphs from the Azure Document Intelligence JSON data."""

        sections = json_data.get("sections", [])
        figures = json_data.get("figures", [])
        tables = json_data.get("tables", [])

        all_paragraphs = [
            "/paragraphs/" + str(x) for x in range(len(json_data.get("paragraphs", [])))
        ]

        elements = []

        def _process_section(section):
            for element in section.get("elements", []):
                if "paragraph" in element:
                    elements.append(element)
                elif "section" in element:
                    section_idx = element.split("/")[2]
                    next_section = sections[int(section_idx)]
                    _process_section(next_section)

        for section in sections:
            _process_section(section)

        def _process_figure(figure):
            for element in figure.get("elements", []):
                if "paragraph" in element:
                    elements.append(element)
                elif "section" in element:
                    section_idx = element.split("/")[2]
                    next_section = sections[int(section_idx)]
                    _process_section(next_section)

        for figure in figures:
            _process_figure(figure)

        def _process_table(table):
            for element in table.get("elements", []):
                if "paragraph" in element:
                    elements.append(element)
                elif "section" in element:
                    section_idx = element.split("/")[2]
                    next_section = sections[int(section_idx)]
                    _process_section(next_section)

        for table in tables:
            _process_table(table)

        missing = [x for x in all_paragraphs if x not in elements]

        return missing

    def _insert_missing_paragraphs(self, json_data: dict) -> dict:
        """Insert missing paragraphs into the Azure Document Intelligence JSON data."""

        missing = self._get_missing_paragraphs(json_data)

        def _insert_paragraph_recursive(section, p_idx, p):
            for i, element in enumerate(section.get("elements", [])):
                if "paragraph" in element:
                    if int(element.split("/")[2]) > int(p_idx):
                        section["elements"].insert(i, p)
                        return True
                elif "section" in element:
                    section_idx = element.split("/")[2]
                    next_section = json_data["sections"][int(section_idx)]
                    if _insert_paragraph_recursive(next_section, p_idx, p):
                        return True
            return False

        for p in missing:
            idx = int(p.split("/")[2])

            for section in json_data.get("sections", []):
                if _insert_paragraph_recursive(section, idx, p):
                    break
        return json_data

    def _parse_json(self, json_data: dict, file_path: str) -> Node:
        """
        Parse Azure Document Intelligence JSON into a hierarchical Node structure.

        Args:
            json_data: The Azure Document Intelligence JSON response

        Returns:
            A Node representing the document with hierarchical structure
        """
        # Create root document node

        json_data = self._insert_missing_paragraphs(json_data)

        document_node = Node(
            children=[],
            metadata=self._extract_document_metadata(json_data),
            node_type=NodeType.DOCUMENT,
        )

        # Process each page in the document
        analyze_result = json_data  # .get('analyzeResult', {})
        sections = analyze_result.get("sections", [])

        document_node.children = self._process_children_elements(
            sections[0], analyze_result, file_path=file_path
        )

        return document_node

    def _process_children_elements(
        self,
        parent_object: dict[str, Any],
        analyze_result: dict[str, Any],
        file_path: str,
    ) -> list[Node]:
        """Process children elements of a section."""
        children_nodes = []
        elements = parent_object.get("elements", [])
        for _element_idx, element in enumerate(elements):
            if "paragraph" in element:
                paragrap_index = element.split("/")[2]

                paragraph = analyze_result.get("paragraphs", [])[int(paragrap_index)]
                paragraph_node = self._create_paragraph_node(paragraph)
                paragraph_node.children = self._process_children_elements(
                    paragraph, analyze_result, file_path=file_path
                )
                children_nodes.append(paragraph_node)

            elif "table" in element:
                table_index = element.split("/")[2]
                table = analyze_result.get("tables", [])[int(table_index)]
                table_node = self._create_media_node(
                    media=table,
                    node_type=NodeType.TABLE,
                    content_result=analyze_result.get("content", ""),
                    file_path=file_path,
                )
                table_node.children = self._process_children_elements(
                    table, analyze_result, file_path=file_path
                )
                children_nodes.append(table_node)

            elif "figures" in element:
                image_index = element.split("/")[2]
                image = analyze_result.get("figures", [])[int(image_index)]
                image_node = self._create_media_node(
                    media=image,
                    node_type=NodeType.FIGURE,
                    content_result=analyze_result.get("content", ""),
                    file_path=file_path,
                )
                image_node.children = self._process_children_elements(
                    image, analyze_result, file_path=file_path
                )
                children_nodes.append(image_node)

            elif "section" in element:
                section_index = element.split("/")[2]
                section = analyze_result.get("sections", [])[int(section_index)]
                section_node = Node(children=[], node_type=NodeType.SECTION)
                section_node.children = self._process_children_elements(
                    section, analyze_result, file_path=file_path
                )
                children_nodes.append(section_node)

        return children_nodes

    def _transform_cells_to_markdown(
        self, table_data: dict[str, Any], content_result: str
    ) -> str:
        """Transforms table cells from Azure response to a markdown table string."""
        cells = table_data.get("cells", [])
        if not cells:
            return ""

        offset = table_data.get("spans", [{}])[0].get("offset")
        length = table_data.get("spans", [{}])[0].get("length")
        if offset is None or length is None:
            return ""

        markdown_table = content_result[offset : offset + length]

        return markdown_table

    def _create_media_node(
        self,
        media: dict[str, Any],
        node_type: NodeType,
        content_result: str,
        file_path: str,
    ) -> Node:
        """Create a node for an media with its child elements."""
        # Get bounding regions
        bounding_regions = media.get("boundingRegions", [])

        if file_path and bounding_regions:
            base64_image = extract_media(
                coordinates=bounding_regions[0]["polygon"],
                file_path=file_path,
                page_number=bounding_regions[0]["pageNumber"],
            )

            media_obj = Media(
                media_type="image",
                source=base64_image,
                source_type="base64",
                extension="png",
            )
        else:
            raise ValueError("No bounding regions found for media")

        content = None
        metadata = {
            "boundingRegions": bounding_regions,
        }
        if node_type == NodeType.TABLE:
            content = self._transform_cells_to_markdown(media, content_result)
            metadata["rowCount"] = media.get("rowCount")
            metadata["columnCount"] = media.get("columnCount")

        # Create MediaNode with bounding regions metadata
        image_node = MediaNode(
            media=media_obj,
            children=[],
            node_type=node_type,
            metadata=metadata,
            content=content,
        )
        return image_node

    def _extract_document_metadata(self, json_data: dict[str, Any]) -> dict[str, Any]:
        """Extract document-level metadata from the Azure response."""
        metadata = {}
        analyze_result = json_data.get("analyzeResult", {})

        # Add document-level metadata
        if "documentResults" in analyze_result:
            doc_results = analyze_result["documentResults"]
            if doc_results and len(doc_results) > 0:
                metadata.update(doc_results[0].get("fields", {}))

        # Add model information if available
        metadata["modelId"] = analyze_result.get("modelId")
        metadata["apiVersion"] = analyze_result.get("apiVersion")

        return metadata

    # def _create_table_node(self, table: Dict[str, Any]) -> Node:
    #     """Create a node for a table with its child lines and words."""
    #     table_node = Node(
    #         children=[],
    #         node_type=NodeType.TABLE,
    #         content=table.get("content", ""),
    #         metadata={
    #             "boundingRegions": table.get("boundingRegions", []),
    #         },
    #     )
    #     return table_node

    def _create_paragraph_node(self, paragraph: dict[str, Any]) -> Node:
        """Create a node for a paragraph with its child lines and words."""
        para_node = Node(
            children=[],
            node_type=NodeType.PARAGRAPH,
            content=paragraph.get("content", ""),
            metadata={
                "boundingRegions": paragraph.get("boundingRegions", {}),
            },
        )
        return para_node

    def parse_with_azure_ai(self, file_path: str) -> dict:
        """
        Parse a Document with Azure AI Document Intelligence into a json dictionary.

        Args:
            file_path: Path to the document

        Returns:
            A dictionary with the Azure AI Document Intelligence response
        """

        with open(file_path, "rb") as file:
            file_content = file.read()

        parser = self._get_parser()
        poller = parser.begin_analyze_document(
            "prebuilt-layout",
            AnalyzeDocumentRequest(bytes_source=file_content),
            output_content_format=self.result_type,
        )
        result: AnalyzeResult = poller.result()
        return result.as_dict()

    async def a_parse_with_azure_ai(self, file_path: str) -> dict:
        """
        Parse a Document with Azure AI Document Intelligence into a json dictionary.

        Args:
            file_path: Path to the document

        Returns:
            A dictionary with the Azure AI Document Intelligence response
        """
        async with aiofiles.open(file_path, "rb") as file:
            file_content = await file.read()

        parser = self._get_a_parser()
        async with parser:
            poller = await parser.begin_analyze_document(
                "prebuilt-layout",
                AnalyzeDocumentRequest(bytes_source=file_content),
                output_content_format=self.result_type,
            )
            result: AnalyzeResult = await poller.result()
            return result.as_dict()

    def parse(self, file_path: str, metadata: dict | None = None) -> Node:
        """
        Parse a Document with Azure AI Document Intelligence into a Node
        structure.

        Args:
            file_path: Path to the document
            metadata: Optional metadata to be merged into the root document
                node. Defaults to None.

        Returns:
            A Node representing the document with hierarchical structure

        Raises:
            TypeError: If metadata is not a dict or None
        """
        # Validate metadata type
        if metadata is not None and not isinstance(metadata, dict):
            raise TypeError(
                f"metadata must be a dict or None, got {type(metadata).__name__}"
            )

        result_dict = self.parse_with_azure_ai(file_path)
        document_node = self._parse_json(result_dict, file_path=file_path)

        # Merge provided metadata into the document node's metadata
        if metadata:
            document_node.metadata.update(metadata)

        return document_node

    def __call__(self, file_path: str, metadata: dict | None = None) -> Node:
        """
        Allow the parser to be called directly as a function.

        Args:
            file_path: Path to the document
            metadata: Optional metadata to be merged into the root document node

        Returns:
            A Node representing the document with hierarchical structure
        """
        return self.parse(file_path, metadata)

    async def a_parse(self, file_path: str, metadata: dict | None = None) -> Node:
        """
        Async version of parse().

        Args:
            file_path: Path to the document
            metadata: Optional metadata to be merged into the root document
                node. Defaults to None.

        Returns:
            A Node representing the document with hierarchical structure

        Raises:
            TypeError: If metadata is not a dict or None
        """
        # Validate metadata type
        if metadata is not None and not isinstance(metadata, dict):
            raise TypeError(
                f"metadata must be a dict or None, got {type(metadata).__name__}"
            )

        result_dict = await self.a_parse_with_azure_ai(file_path)
        document_node = self._parse_json(result_dict, file_path=file_path)

        # Merge provided metadata into the document node's metadata
        if metadata:
            document_node.metadata.update(metadata)

        return document_node



================================================
FILE: datapizza-ai-modules/parsers/azure/tests/test_azure_parser.py
================================================
import json
import os

import pytest
from datapizza.type import NodeType

from datapizza.modules.parsers.azure import AzureParser


@pytest.fixture
def sample_azure_result():
    with open(
        os.path.join(os.path.dirname(__file__), "attention_wikipedia_test.json"),
    ) as f:
        sample_result = json.load(f)
    return sample_result


@pytest.fixture
def azure_parser():
    return AzureParser(
        api_key="dummy_key", endpoint="https://dummy-endpoint", result_type="text"
    )


def test_azure_parser_parse(azure_parser, sample_azure_result):
    # Call the public method instead of internal _parse_json
    sample_file_path = os.path.join(
        os.path.dirname(__file__), "attention_wikipedia_test.pdf"
    )
    result = azure_parser._parse_json(sample_azure_result, file_path=sample_file_path)

    assert result.node_type == NodeType.DOCUMENT
    assert result.children
    assert len(result.content) > 30000

    # check if there is at least one child with node_type == NodeType.PARAGRAPH do recursive search
    def check_paragraph(node):
        if node.node_type == NodeType.PARAGRAPH:
            assert len(node.content) > 0
        for child in node.children:
            check_paragraph(child)

    check_paragraph(result)

    # check if there is at least one child with node_type == NodeType.IMAGE do recursive search
    def check_figure(node):
        if node.node_type == NodeType.FIGURE:
            assert node.media.source is not None
        for child in node.children:
            check_figure(child)

    check_figure(result)


def test_parse_with_metadata(azure_parser, sample_azure_result):
    """Test that parse() correctly merges user-provided metadata."""
    sample_file_path = os.path.join(
        os.path.dirname(__file__), "attention_wikipedia_test.pdf"
    )
    user_metadata = {
        "source": "user_upload",
        "custom_field": "test_value",
    }

    result = azure_parser._parse_json(sample_azure_result, file_path=sample_file_path)
    # Manually apply metadata as parse() would
    result.metadata.update(user_metadata)

    assert result.metadata["source"] == "user_upload"
    assert result.metadata["custom_field"] == "test_value"
    # Ensure original metadata is preserved
    assert "modelId" in result.metadata or "apiVersion" in result.metadata


def test_parse_with_none_metadata(azure_parser, sample_azure_result):
    """Test that parse() works correctly when metadata is None."""
    sample_file_path = os.path.join(
        os.path.dirname(__file__), "attention_wikipedia_test.pdf"
    )
    result = azure_parser._parse_json(sample_azure_result, file_path=sample_file_path)

    assert result.node_type == NodeType.DOCUMENT
    assert result.metadata is not None


def test_parse_metadata_type_validation(azure_parser):
    """Test that parse() raises TypeError for invalid metadata type."""
    sample_file_path = os.path.join(
        os.path.dirname(__file__), "attention_wikipedia_test.pdf"
    )

    with pytest.raises(TypeError, match="metadata must be a dict or None"):
        azure_parser.parse(sample_file_path, metadata="invalid_string")

    with pytest.raises(TypeError, match="metadata must be a dict or None"):
        azure_parser.parse(sample_file_path, metadata=123)

    with pytest.raises(TypeError, match="metadata must be a dict or None"):
        azure_parser.parse(sample_file_path, metadata=["list", "of", "items"])


def test_parse_metadata_override(azure_parser, sample_azure_result):
    """Test that user metadata overrides parser-generated metadata."""
    sample_file_path = os.path.join(
        os.path.dirname(__file__), "attention_wikipedia_test.pdf"
    )
    # Simulate parser-generated metadata with modelId
    result = azure_parser._parse_json(sample_azure_result, file_path=sample_file_path)
    original_model_id = result.metadata.get("modelId")

    # Override with user metadata
    user_metadata = {"modelId": "custom_model"}
    result.metadata.update(user_metadata)

    # User metadata should override
    assert result.metadata["modelId"] == "custom_model"
    assert result.metadata["modelId"] != original_model_id


def test_call_method_with_metadata(azure_parser, sample_azure_result, monkeypatch):
    """Test that __call__() method supports metadata parameter."""
    sample_file_path = os.path.join(
        os.path.dirname(__file__), "attention_wikipedia_test.pdf"
    )
    user_metadata = {"source": "direct_call"}

    # Mock parse_with_azure_ai to avoid actual API call
    monkeypatch.setattr(
        azure_parser, "parse_with_azure_ai", lambda fp: sample_azure_result
    )

    result = azure_parser(sample_file_path, metadata=user_metadata)

    assert result.metadata["source"] == "direct_call"



================================================
FILE: datapizza-ai-modules/parsers/docling/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-modules/parsers/docling/mypy.ini
================================================
[mypy-fitz.*]
ignore_missing_imports = True

[mypy-docling.*]
ignore_missing_imports = True

[mypy-datapizza.*]
ignore_missing_imports = True

[mypy]
explicit_package_bases = True


================================================
FILE: datapizza-ai-modules/parsers/docling/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-parsers-docling"
version = "0.0.8"
description = "Doclign parser for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "docling>=2.53.0",
    "easyocr>=1.7.0"
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-modules/parsers/docling/datapizza/modules/parsers/docling/__init__.py
================================================
from .docling_parser import DoclingParser
from .ocr_options import OCREngine, OCROptions

__all__ = ["DoclingParser", "OCREngine", "OCROptions"]



================================================
FILE: datapizza-ai-modules/parsers/docling/datapizza/modules/parsers/docling/docling_parser.py
================================================
import json
import warnings
from pathlib import Path
from typing import Any

from datapizza.core.modules.parser import Parser
from datapizza.type import Node, NodeType
from datapizza.type.type import Media, MediaNode

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import (
    DocumentConverter,
    PdfFormatOption,
)

from .ocr_options import OCROptions
from .utils import extract_media_from_docling_bbox


class DoclingParser(Parser):
    """
    Parser that converts files using Docling and then converts the resulting
    DoclingDocument JSON into a datapizza Node tree.
    Supported file extensions: https://docling-project.github.io/docling/usage/supported_formats/

    - Accepts files directly and processes them using Docling DocumentConverter
    - Logical-only hierarchy (no page nodes)
    - Paragraphs are leaf nodes; no sentence splitting by default
    - Full preservation of Docling items stored in node.metadata["docling_raw"],
      with convenience fields (docling_type, docling_label, bbox, page_no, self_ref)
    - Reading order follows body.children ($ref list)
    - Images and tables are mapped to FIGURE and TABLE nodes respectively, with bbox-only metadata
    """
    def __init__(
        self,
        json_output_dir: str | None = None,
        ocr_options: OCROptions | None = None,
    ):
        """
        Initialize DoclingParser with optional OCR configuration.

        Args:
            json_output_dir: Optional directory to save intermediate Docling JSON results.
            ocr_options: OCR configuration. Defaults to EasyOCR (backward compatible) if not provided.
        """
        self.converter = None
        # Optional directory to save intermediate Docling JSON results
        self.json_output_dir = json_output_dir
        self.ocr_options = ocr_options or OCROptions()

    def _create_converter(self):
        """
        Create a Docling DocumentConverter with configured OCR options.

        Uses self.ocr_options to determine which OCR engine to use and
        passes the appropriate configuration to Docling.

        Returns:
            Configured DocumentConverter instance
        """
        # Convert internal OCR options to Docling pipeline kwargs
        pipeline_kwargs = self.ocr_options.to_docling_pipeline_options()
        pipeline_options = PdfPipelineOptions(**pipeline_kwargs)

        return DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            },
        )

    def _get_converter(self):
        """Get or create converter instance."""
        if not self.converter:
            self.converter = self._create_converter()
        return self.converter

    def _is_json_file(self, file_path: str) -> bool:
        """Check if the file is a JSON file."""
        return Path(file_path).suffix.lower() == ".json"

    def parse_to_json(self, file_path: str) -> dict:
        """
        Parse a supported file using Docling, or if json_path is provided, load that
        Docling JSON directly and skip conversion.

        Args:
            file_path: Path to the source File (required for media extraction)
            json_path: Optional path to a Docling JSON file to skip conversion

        Returns:
            Docling document as a dictionary
        """

        converter = self._get_converter()
        result = converter.convert(file_path)
        doc_dict = result.document.export_to_dict()

        # Optionally persist intermediate Docling JSON beside the pipeline output
        if self.json_output_dir:
            out_dir = Path(self.json_output_dir)
            out_dir.mkdir(parents=True, exist_ok=True)
            out_path = out_dir / f"{Path(file_path).stem}.json"
            with open(out_path, "w", encoding="utf-8") as fp:
                json.dump(doc_dict, fp, ensure_ascii=False, indent=2)

        return doc_dict

    def _json_to_node(self, json_data: dict, file_path: str | None = None) -> Node:
        """
        Convert Docling JSON into a Node hierarchy.
        """
        # Root document with full raw preservation
        document_node = Node(children=[], node_type=NodeType.DOCUMENT, metadata={})
        # Preserve entire Docling raw at root for losslessness
        document_node.metadata["docling_raw"] = json_data
        document_node.metadata["schema_name"] = json_data.get("schema_name")
        document_node.metadata["version"] = json_data.get("version")
        document_node.metadata["name"] = json_data.get("name")
        document_node.metadata["origin"] = json_data.get("origin")

        # Prepare ref resolver over top-level collections
        collections: dict[str, dict[str, Any]] = {}
        for key, value in json_data.items():
            if isinstance(value, list):
                collections[key] = {
                    f"#/{key}/{i}": item for i, item in enumerate(value)
                }

        def resolve_ref(ref_obj: Any) -> tuple[str, dict[str, Any]] | None:
            """
            Resolve a Docling $ref object or a direct item.

            Returns (collection_name, item_dict) when resolved from ref, or
            (derived_type, item_dict) when passed a direct item.
            """
            if isinstance(ref_obj, dict) and "$ref" in ref_obj:
                ref_path = ref_obj["$ref"]
                # find collection that has this ref
                for coll_name, index_map in collections.items():
                    if ref_path in index_map:
                        return coll_name, index_map[ref_path]
                return None
            elif isinstance(ref_obj, dict) and "self_ref" in ref_obj:
                # Direct item already expanded
                # Infer collection from self_ref prefix if present
                self_ref: str = ref_obj.get("self_ref", "")
                coll_guess = (
                    self_ref.split("/")[1] if self_ref.startswith("#/") else "unknown"
                )
                return coll_guess, ref_obj
            else:
                return None

        # Reading order source
        body: dict[str, Any] = json_data.get("body", {})
        body_children: list[Any] = body.get("children", [])

        # Maintain a stack of open sections based on level
        section_stack: list[tuple[int, Node]] = []  # list of (level, node)
        # Accumulate consecutive list items for markdown list rendering
        current_list_items: list[dict[str, Any]] = []

        def current_parent() -> Node:
            return section_stack[-1][1] if section_stack else document_node

        def push_section(
            level: int, header_item: dict[str, Any], coll_name: str
        ) -> Node:
            # Pop deeper/equal levels
            while section_stack and section_stack[-1][0] >= level:
                section_stack.pop()

            section_node = Node(children=[], node_type=NodeType.SECTION, metadata={})
            # Preserve header as metadata
            section_node.metadata["docling_raw"] = header_item
            section_node.metadata["docling_type"] = coll_name
            section_node.metadata["docling_label"] = header_item.get("label")
            section_node.metadata["level"] = header_item.get("level")
            _add_common_metadata(section_node.metadata, header_item)

            parent = current_parent()
            parent.add_child(section_node)
            section_stack.append((level, section_node))

            # Also emit a heading paragraph as markdown line under the section
            heading_text: str = header_item.get("text", "") or ""
            # Bump heading level by 1 to match expected markdown (level 1 -> '##')
            hlevel = max(1, min(6, int(level or 1) + 1))
            heading_md: str = f"{'#' * hlevel} {heading_text}".rstrip()
            if heading_md.strip():
                heading_para = Node(
                    children=[],
                    node_type=NodeType.PARAGRAPH,
                    metadata={},
                    content=heading_md + "\n\n",
                )
                # Keep original header in paragraph metadata too for direct association
                heading_para.metadata["docling_header_ref"] = header_item.get(
                    "self_ref"
                )
                heading_para.metadata["markdown_rendering"] = "heading"
                # Also carry over full Docling metadata so headers are discoverable as nodes
                heading_para.metadata["docling_raw"] = header_item
                heading_para.metadata["docling_type"] = coll_name
                heading_para.metadata["docling_label"] = header_item.get("label")
                heading_para.metadata["level"] = header_item.get("level")
                _add_common_metadata(heading_para.metadata, header_item)
                section_node.add_child(heading_para)
            return section_node

        def _normalize_coord_origin(origin_val: Any) -> str:
            """Normalize coord origin to either 'BOTTOMLEFT' or 'TOPLEFT'."""
            if hasattr(origin_val, "name"):
                origin_str = str(getattr(origin_val, "name", "")).upper()
            else:
                origin_str = str(origin_val).upper()
            if "." in origin_str:
                origin_str = origin_str.split(".")[-1]
            origin_str = origin_str.replace("_", "").replace("-", "")
            if origin_str not in {"BOTTOMLEFT", "TOPLEFT"}:
                # Default to Docling default
                origin_str = "BOTTOMLEFT"
            return origin_str

        def _union_two_bboxes(b1: dict[str, Any], b2: dict[str, Any]) -> dict[str, Any]:
            """Return the union bbox of two Docling-style rects, honoring coord origin.

            Expects keys l,t,r,b and optional coord_origin (defaults to BOTTOMLEFT).
            The resulting bbox uses the origin of b1 if present, otherwise b2.
            """
            origin = _normalize_coord_origin(
                b1.get("coord_origin", b2.get("coord_origin", "BOTTOMLEFT"))
            )
            l1, r1 = float(b1.get("l", 0.0)), float(b1.get("r", 0.0))
            t1, btm1 = float(b1.get("t", 0.0)), float(b1.get("b", 0.0))
            l2, r2 = float(b2.get("l", 0.0)), float(b2.get("r", 0.0))
            t2, btm2 = float(b2.get("t", 0.0)), float(b2.get("b", 0.0))

            left = min(l1, l2)
            right = max(r1, r2)
            if origin == "BOTTOMLEFT":
                top = max(t1, t2)
                bottom = min(btm1, btm2)
            else:  # TOPLEFT
                top = min(t1, t2)
                bottom = max(btm1, btm2)

            return {
                "l": left,
                "t": top,
                "r": right,
                "b": bottom,
                "coord_origin": origin,
            }

        def _merge_bboxes_per_page(
            prov_list: list[dict[str, Any]],
        ) -> dict[int, dict[str, Any]]:
            """Merge multiple bbox entries by page into a single union per page.

            Returns a mapping {page_no: merged_bbox}.
            Ignores malformed entries that lack page_no or bbox.
            """
            page_to_bbox: dict[int, dict[str, Any]] = {}
            for prov in prov_list:
                try:
                    page_no = int((prov or {}).get("page_no") or 0)
                except Exception:
                    page_no = 0
                bbox = (prov or {}).get("bbox") or None
                if page_no <= 0 or not isinstance(bbox, dict):
                    continue
                if page_no not in page_to_bbox:
                    page_to_bbox[page_no] = bbox
                else:
                    page_to_bbox[page_no] = _union_two_bboxes(
                        page_to_bbox[page_no], bbox
                    )
            return page_to_bbox

        def _add_common_metadata(
            metadata: dict[str, Any], item: dict[str, Any]
        ) -> None:
            metadata["self_ref"] = item.get("self_ref")
            metadata["prov"] = item.get("prov")
            # Merge multi-bbox per page; keep a representative bbox for the first page
            if isinstance(item.get("prov"), list) and len(item["prov"]) > 0:
                merged = _merge_bboxes_per_page(item["prov"])  # {page_no: bbox}
                if merged:
                    page_nos_sorted = sorted(merged.keys())
                    first_page = page_nos_sorted[0]
                    metadata["page_no"] = first_page
                    metadata["bbox"] = merged[first_page]
                    if len(merged) > 1:
                        metadata["page_nos"] = page_nos_sorted
                        metadata["bbox_per_page"] = merged
                    # Expose Azure-style boundingRegions so downstream splitter/vectorstore receive them
                    # We approximate polygons as rectangles from Docling bbox; values expressed in inches
                    bounding_regions: list[dict[str, Any]] = []
                    for pg in page_nos_sorted:
                        bb = merged.get(pg) or {}
                        try:
                            l = float(bb.get("l", 0.0)) / 72.0
                            r = float(bb.get("r", 0.0)) / 72.0
                            t = float(bb.get("t", 0.0)) / 72.0
                            btm = float(bb.get("b", 0.0)) / 72.0
                        except Exception:
                            # Skip malformed bbox
                            continue
                        polygon = [
                            l,
                            btm,
                            r,
                            btm,
                            r,
                            t,
                            l,
                            t,
                        ]
                        bounding_regions.append(
                            {
                                "pageNumber": int(pg),
                                "polygon": polygon,
                                # Preserve original coord origin for consumers that need it
                                "coordOrigin": (bb.get("coord_origin") or "BOTTOMLEFT"),
                            }
                        )
                    if bounding_regions:
                        metadata["boundingRegions"] = bounding_regions
            # pass through common fields
            for k in ("label", "content_layer", "orig", "text"):
                if k in item:
                    metadata[k] = item.get(k)

        def _create_paragraph_with_sentences(
            text_item: dict[str, Any], coll_name: str
        ) -> Node:
            paragraph_node = Node(
                children=[], node_type=NodeType.PARAGRAPH, metadata={}
            )
            paragraph_node.metadata["docling_raw"] = text_item
            paragraph_node.metadata["docling_type"] = coll_name
            paragraph_node.metadata["docling_label"] = text_item.get("label")
            _add_common_metadata(paragraph_node.metadata, text_item)

            text_content: str = text_item.get("text", "") or ""
            # Store full paragraph text without arbitrary sentence splitting
            # Ensure separation with trailing blank line
            paragraph_node._content = (text_content + "\n\n") if text_content else ""

            return paragraph_node

        def _flush_current_list(parent: Node) -> None:
            nonlocal current_list_items
            if not current_list_items:
                return
            # Create a list parent and one child per bullet item
            list_parent = Node(children=[], node_type=NodeType.PARAGRAPH, metadata={})
            list_parent.metadata["markdown_rendering"] = "list"
            # Preserve full raw items for losslessness
            list_parent.metadata["docling_list_items_raw"] = current_list_items

            num_items = len(current_list_items)
            for idx, li in enumerate(current_list_items):
                li_text: str = li.get("text", "") or ""
                bullet_node = Node(
                    children=[], node_type=NodeType.PARAGRAPH, metadata={}
                )
                bullet_node.metadata["markdown_rendering"] = "list_item"
                bullet_node.metadata["docling_raw"] = li
                bullet_node.metadata["docling_type"] = "texts"
                bullet_node.metadata["docling_label"] = "list_item"
                _add_common_metadata(bullet_node.metadata, li)
                # Ensure separation: one newline per item, two newlines after the last one
                trailing_newlines = "\n\n" if idx == num_items - 1 else "\n"
                bullet_node._content = (
                    f"- {li_text}" if li_text else "-"
                ) + trailing_newlines
                list_parent.add_child(bullet_node)

            parent.add_child(list_parent)
            current_list_items = []

        def _extract_base64_from_pdf_bbox(prov_entry: dict[str, Any]) -> str | None:
            if not file_path or Path(file_path).suffix.lower() != ".pdf":
                return None
            page_no = int((prov_entry or {}).get("page_no") or 0)
            bbox = (prov_entry or {}).get("bbox") or {}
            if page_no <= 0 or not isinstance(bbox, dict):
                return None
            try:
                return extract_media_from_docling_bbox(
                    bbox=bbox, file_path=file_path, page_number=page_no
                )
            except Exception:
                return None

        def _create_figure_node(item: dict[str, Any], coll_name: str) -> Node:
            media_b64: str | None = None
            prov_list = item.get("prov") or []
            if isinstance(prov_list, list) and len(prov_list) > 0:
                merged = _merge_bboxes_per_page(prov_list)
                if merged:
                    first_page = sorted(merged.keys())[0]
                    media_b64 = _extract_base64_from_pdf_bbox(
                        {"page_no": first_page, "bbox": merged[first_page]}
                    )

            if media_b64:
                media = Media(
                    media_type="image",
                    source_type="base64",
                    source=media_b64,
                    extension="png",
                )
                figure_node: Node = MediaNode(
                    media=media,
                    children=[],
                    node_type=NodeType.FIGURE,
                    metadata={},
                )
            else:
                figure_node = Node(children=[], node_type=NodeType.FIGURE, metadata={})

            figure_node.metadata["docling_raw"] = item
            figure_node.metadata["docling_type"] = coll_name
            figure_node.metadata["docling_label"] = item.get("label", "picture")
            _add_common_metadata(figure_node.metadata, item)
            return figure_node

        def _create_table_node(item: dict[str, Any], coll_name: str) -> Node:
            # Build markdown table content from Docling table data when possible
            content_md: str | None = None
            data = item.get("data", {}) or {}
            table_cells: list[dict[str, Any]] = data.get("table_cells", []) or []
            num_rows: int = int(data.get("num_rows") or 0)
            num_cols: int = int(data.get("num_cols") or 0)

            if table_cells and num_rows > 0 and num_cols > 0:
                # Construct a 2D grid of cell strings, best-effort for simple (no span) cases
                grid: list[list[str]] = [[""] * num_cols for _ in range(num_rows)]
                for cell in table_cells:
                    r0 = int(cell.get("start_row_offset_idx") or 0)
                    c0 = int(cell.get("start_col_offset_idx") or 0)
                    txt = str(cell.get("text") or "")
                    # Prefer first fill; ignore spans for markdown simplicity
                    if 0 <= r0 < num_rows and 0 <= c0 < num_cols and grid[r0][c0] == "":
                        grid[r0][c0] = txt

                # Build markdown lines
                header = grid[0] if num_rows > 0 else []
                # If header is empty, we still create a minimal table
                if header:
                    header_line = (
                        "| " + " | ".join([_escape_md(x) for x in header]) + " |"
                    )
                    sep_line = "| " + " | ".join(["---"] * len(header)) + " |"
                    body_lines = []
                    for r in range(1, num_rows):
                        row_line = (
                            "| " + " | ".join([_escape_md(x) for x in grid[r]]) + " |"
                        )
                        body_lines.append(row_line)
                    content_md = "\n".join([header_line, sep_line, *body_lines])
                else:
                    # Fallback single row from first non-empty row
                    for r in range(num_rows):
                        if any(grid[r]):
                            header = grid[r]
                            header_line = (
                                "| "
                                + " | ".join([_escape_md(x) for x in header])
                                + " |"
                            )
                            sep_line = "| " + " | ".join(["---"] * len(header)) + " |"
                            content_md = "\n".join([header_line, sep_line])
                            break

            if content_md is not None:
                content_md = content_md + "\n\n"

            media_b64: str | None = None
            prov_list = item.get("prov") or []
            if isinstance(prov_list, list) and len(prov_list) > 0:
                merged = _merge_bboxes_per_page(prov_list)
                if merged:
                    first_page = sorted(merged.keys())[0]
                    media_b64 = _extract_base64_from_pdf_bbox(
                        {"page_no": first_page, "bbox": merged[first_page]}
                    )

            if media_b64:
                media = Media(
                    media_type="image",
                    source_type="base64",
                    source=media_b64,
                    extension="png",
                )
                table_node: Node = MediaNode(
                    media=media,
                    children=[],
                    node_type=NodeType.TABLE,
                    metadata={},
                    content=content_md,
                )
            else:
                table_node = Node(
                    children=[],
                    node_type=NodeType.TABLE,
                    metadata={},
                    content=content_md,
                )
            table_node.metadata["docling_raw"] = item
            table_node.metadata["docling_type"] = coll_name
            table_node.metadata["docling_label"] = item.get("label", "table")
            _add_common_metadata(table_node.metadata, item)
            return table_node

        def _append_children_to_media(media_node: Node, item: dict[str, Any]) -> None:
            """
            Append Docling child references (children/captions) directly to the given
            media node (figure/table), deduplicating ref order.
            """
            merged_ref_paths: list[str] = []
            for key in ("children", "captions"):
                seq = item.get(key) or []
                for ref_obj in seq:
                    if isinstance(ref_obj, dict) and "$ref" in ref_obj:
                        merged_ref_paths.append(ref_obj["$ref"])
                    elif isinstance(ref_obj, dict) and ref_obj.get("self_ref"):
                        merged_ref_paths.append(ref_obj["self_ref"])  # unlikely here

            seen: set[str] = set()
            ordered_unique_refs: list[str] = []
            for ref_path in merged_ref_paths:
                if ref_path not in seen:
                    seen.add(ref_path)
                    ordered_unique_refs.append(ref_path)

            for ref_path in ordered_unique_refs:
                resolved_child = resolve_ref({"$ref": ref_path})
                if resolved_child is None:
                    continue
                child_coll, child_item = resolved_child
                child_label = child_item.get("label")

                if child_coll == "texts":
                    node = _create_paragraph_with_sentences(child_item, child_coll)
                    media_node.add_child(node)
                    continue
                if child_coll == "pictures":
                    node = _create_figure_node(child_item, child_coll)
                    # recursively attach nested children if present
                    if (child_item.get("children") or []) or (
                        child_item.get("captions") or []
                    ):
                        _append_children_to_media(node, child_item)
                    media_node.add_child(node)
                    continue
                if child_coll == "tables":
                    node = _create_table_node(child_item, child_coll)
                    if (child_item.get("children") or []) or (
                        child_item.get("captions") or []
                    ):
                        _append_children_to_media(node, child_item)
                    media_node.add_child(node)
                    continue

                # Fallback for unknown child types
                fallback = Node(children=[], node_type=NodeType.PARAGRAPH, metadata={})
                fallback.metadata["docling_raw"] = child_item
                fallback.metadata["docling_type"] = child_coll
                fallback.metadata["docling_label"] = child_label
                _add_common_metadata(fallback.metadata, child_item)
                media_node.add_child(fallback)

        # Walk reading order
        for child in body_children:
            resolved = resolve_ref(child)
            if resolved is None:
                continue
            coll_name, item = resolved
            label = item.get("label")

            if coll_name == "texts" and label == "section_header":
                # close any pending list before new section
                _flush_current_list(current_parent())
                level = int(item.get("level") or 1)
                push_section(level, item, coll_name)
                continue

            parent = current_parent()

            # Expand group containers by iterating their children in order
            if coll_name == "groups":
                children_in_group: list[Any] = item.get("children", []) or []
                for group_child in children_in_group:
                    inner_resolved = resolve_ref(group_child)
                    if inner_resolved is None:
                        continue
                    inner_coll, inner_item = inner_resolved
                    inner_label = inner_item.get("label")

                    # handle nested group containers (e.g., a list made of sub-groups)
                    if inner_coll == "groups":
                        nested_children: list[Any] = (
                            inner_item.get("children", []) or []
                        )
                        for nested_child in nested_children:
                            nested_resolved = resolve_ref(nested_child)
                            if nested_resolved is None:
                                continue
                            nested_coll, nested_item = nested_resolved
                            nested_label = nested_item.get("label")

                            if nested_coll == "texts":
                                if nested_label == "list_item":
                                    current_list_items.append(nested_item)
                                    continue
                                _flush_current_list(parent)
                                paragraph_node = _create_paragraph_with_sentences(
                                    nested_item, nested_coll
                                )
                                parent.add_child(paragraph_node)
                                continue

                            if nested_coll == "pictures":
                                _flush_current_list(parent)
                                figure_node = _create_figure_node(
                                    nested_item, nested_coll
                                )
                                if (nested_item.get("children") or []) or (
                                    nested_item.get("captions") or []
                                ):
                                    _append_children_to_media(figure_node, nested_item)
                                parent.add_child(figure_node)
                                continue

                            if nested_coll == "tables":
                                _flush_current_list(parent)
                                table_node = _create_table_node(
                                    nested_item, nested_coll
                                )
                                if (nested_item.get("children") or []) or (
                                    nested_item.get("captions") or []
                                ):
                                    _append_children_to_media(table_node, nested_item)
                                parent.add_child(table_node)
                                continue

                            # Fallback for unknown items inside nested groups
                            _flush_current_list(parent)
                            fallback_node = Node(
                                children=[], node_type=NodeType.PARAGRAPH, metadata={}
                            )
                            fallback_node.metadata["docling_raw"] = nested_item
                            fallback_node.metadata["docling_type"] = nested_coll
                            fallback_node.metadata["docling_label"] = nested_label
                            _add_common_metadata(fallback_node.metadata, nested_item)
                            parent.add_child(fallback_node)
                        # proceed with next item in this group after processing nested group
                        continue

                    if inner_coll == "texts" and inner_label == "section_header":
                        _flush_current_list(parent)
                        level = int(inner_item.get("level") or 1)
                        push_section(level, inner_item, inner_coll)
                        # update parent after pushing a new section
                        parent = current_parent()
                        continue

                    if inner_coll == "texts":
                        if inner_label == "list_item":
                            current_list_items.append(inner_item)
                            continue
                        _flush_current_list(parent)
                        paragraph_node = _create_paragraph_with_sentences(
                            inner_item, inner_coll
                        )
                        parent.add_child(paragraph_node)
                        continue

                    if inner_coll == "pictures":
                        _flush_current_list(parent)
                        figure_node = _create_figure_node(inner_item, inner_coll)
                        if (inner_item.get("children") or []) or (
                            inner_item.get("captions") or []
                        ):
                            _append_children_to_media(figure_node, inner_item)
                        parent.add_child(figure_node)
                        continue

                    if inner_coll == "tables":
                        _flush_current_list(parent)
                        table_node = _create_table_node(inner_item, inner_coll)
                        if (inner_item.get("children") or []) or (
                            inner_item.get("captions") or []
                        ):
                            _append_children_to_media(table_node, inner_item)
                        parent.add_child(table_node)
                        continue

                    # Fallback for unknown items inside groups
                    _flush_current_list(parent)
                    fallback_node = Node(
                        children=[], node_type=NodeType.PARAGRAPH, metadata={}
                    )
                    fallback_node.metadata["docling_raw"] = inner_item
                    fallback_node.metadata["docling_type"] = inner_coll
                    fallback_node.metadata["docling_label"] = inner_label
                    _add_common_metadata(fallback_node.metadata, inner_item)
                    parent.add_child(fallback_node)
                # after expanding a group, continue with next body child
                continue

            if coll_name == "texts":
                if label == "list_item":
                    # accumulate for list rendering
                    current_list_items.append(item)
                    continue
                # if a non-list text arrives, flush any pending list
                _flush_current_list(parent)
                # Treat all textual items as paragraphs with sentence leaves
                paragraph_node = _create_paragraph_with_sentences(item, coll_name)
                parent.add_child(paragraph_node)
                continue

            if coll_name == "pictures":
                _flush_current_list(parent)
                figure_node = _create_figure_node(item, coll_name)
                if (item.get("children") or []) or (item.get("captions") or []):
                    _append_children_to_media(figure_node, item)
                parent.add_child(figure_node)
                continue

            if coll_name == "tables":
                _flush_current_list(parent)
                table_node = _create_table_node(item, coll_name)
                if (item.get("children") or []) or (item.get("captions") or []):
                    _append_children_to_media(table_node, item)
                parent.add_child(table_node)
                continue

            # Fallback: unknown collection â†’ store as paragraph metadata
            _flush_current_list(parent)
            fallback_node = Node(children=[], node_type=NodeType.PARAGRAPH, metadata={})
            fallback_node.metadata["docling_raw"] = item
            fallback_node.metadata["docling_type"] = coll_name
            fallback_node.metadata["docling_label"] = label
            _add_common_metadata(fallback_node.metadata, item)
            parent.add_child(fallback_node)

        # flush any pending list at end
        _flush_current_list(current_parent())

        return document_node

    def parse(
        self,
        file_path: str | None = None,
        metadata: dict | None = None,
        *,
        pdf_path: str | None = None,
    ) -> Node:
        """
        Parse a document into a datapizza Node tree using Docling.

        Args:
            file_path: Path to the document to parse. (New preferred parameter)
            metadata: Optional metadata to be merged into the root document
                node. Defaults to None.
            pdf_path: [Deprecated] Backward-compatible alias for file_path.

        Returns:
            Node: The root document node.

        Raises:
            ValueError: If file_path is not provided
            TypeError: If metadata is not a dict or None
        """
        # Validate metadata type
        if metadata is not None and not isinstance(metadata, dict):
            raise TypeError(f"metadata must be a dict or None, got {type(metadata).__name__}")

        if pdf_path is not None:
            warnings.warn(
                "The 'pdf_path' parameter is deprecated and will be removed "
                "in a future version. Use 'file_path' instead.",
                DeprecationWarning,
                stacklevel=2,
            )
            file_path = file_path or pdf_path

        if not file_path:
            raise ValueError("Missing required argument: file_path")

        json_data = self.parse_to_json(file_path=file_path)
        document_node = self._json_to_node(json_data, file_path=file_path)

        # Merge provided metadata into the document node's metadata
        if metadata:
            document_node.metadata.update(metadata)

        return document_node


def _escape_md(text: str) -> str:
    """
    Escape markdown-sensitive characters minimally inside table cells.
    """
    if not text:
        return ""
    # Replace pipe to avoid breaking table columns; escape backticks minimally
    return text.replace("|", "\\|").replace("`", "\\`")



================================================
FILE: datapizza-ai-modules/parsers/docling/datapizza/modules/parsers/docling/ocr_options.py
================================================
"""
OCR (Optical Character Recognition) configuration for Docling parser.

This module provides flexible OCR engine management, allowing users to:
- Select between different OCR engines (EasyOCR, Tesseract)
- Disable OCR entirely
- Configure engine-specific options

Backward compatibility: Default is EasyOCR if no options are provided.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Any


class OCREngine(str, Enum):
    """
    Supported OCR engines for document processing.

    Attributes:
        EASY_OCR: Use EasyOCR for text recognition
        TESSERACT: Use Tesseract OCR engine
        NONE: Disable OCR - use only PDF text extraction
    """

    EASY_OCR = "easy_ocr"
    TESSERACT = "tesseract"
    NONE = "none"

    def __str__(self) -> str:
        """Return human-readable name for the OCR engine."""
        return {
            self.EASY_OCR: "EasyOCR",
            self.TESSERACT: "Tesseract",
            self.NONE: "None",
        }[self]


@dataclass
class OCROptions:
    """
    Configuration for OCR processing in Docling.

    This dataclass allows fine-grained control over OCR behavior during
    document parsing. Each OCR engine has its own set of parameters.

    Attributes:
        engine: The OCR engine to use (default: EASY_OCR for backward compatibility)
        easy_ocr_force_full_page: Force full page OCR with EasyOCR (default: True)
        tesseract_lang: Language codes for Tesseract as list (default: ["eng"])
                       Examples: ["auto"], ["ita"], ["ita", "eng"], ["eng", "fra"]
        tesseract_config: Additional Tesseract configuration string (default: "")
    """

    engine: OCREngine = field(default=OCREngine.EASY_OCR)
    # EasyOCR specific options
    easy_ocr_force_full_page: bool = field(default=True)
    # Tesseract specific options
    tesseract_lang: list[str] = field(default_factory=lambda: ["eng"])
    tesseract_config: str = field(default="")

    def to_docling_pipeline_options(self) -> dict[str, Any]:
        """
        Convert OCR options to Docling PdfPipelineOptions configuration.

        This method translates our internal OCR configuration to the format
        expected by Docling's DocumentConverter.

        Returns:
            Dictionary of pipeline kwargs suitable for PdfPipelineOptions.
            Always includes "do_table_structure": True for consistency.
        """
        pipeline_kwargs: dict[str, Any] = {"do_table_structure": True}

        if self.engine == OCREngine.NONE:
            # No OCR - Docling will use built-in PDF text extraction only
            return pipeline_kwargs

        if self.engine == OCREngine.EASY_OCR:
            # Import here to avoid dependency issues if EasyOCR not installed
            from docling.datamodel.pipeline_options import EasyOcrOptions

            ocr_options = EasyOcrOptions(
                force_full_page_ocr=self.easy_ocr_force_full_page
            )
            pipeline_kwargs["ocr_options"] = ocr_options
            return pipeline_kwargs

        if self.engine == OCREngine.TESSERACT:
            # Import here to support optional Tesseract dependency
            from docling.datamodel.pipeline_options import TesseractOcrOptions

            # tesseract_lang is already a list, pass it directly
            ocr_options = TesseractOcrOptions(lang=self.tesseract_lang)
            pipeline_kwargs["ocr_options"] = ocr_options
            return pipeline_kwargs

        # Fallback (should not reach here if Enum is exhaustive)
        return pipeline_kwargs




================================================
FILE: datapizza-ai-modules/parsers/docling/datapizza/modules/parsers/docling/utils.py
================================================
def extract_media_from_docling_bbox(
    bbox: dict,
    file_path: str,
    page_number: int,
    *,
    zoom: float = 2.0,
) -> str:
    """Extract a base64 PNG crop from a PDF using a Docling-style bbox.

    The bbox is expected to be a dict with keys:
    - l, t, r, b: floats in PDF coordinate space
    - coord_origin: "BOTTOMLEFT" (Docling default) or "TOPLEFT" (string or enum)
    """
    try:
        import fitz
    except ImportError as e:
        raise ImportError(
            "PyMuPDF is not installed. Please install it using `pip install PyMuPDF`."
        ) from e

    try:
        from PIL import Image
    except ImportError as e:
        raise ImportError(
            "PIL is not installed. Please install it using `pip install Pillow`."
        ) from e

    if not isinstance(bbox, dict):
        raise ValueError("bbox must be a dict with keys l,t,r,b,coord_origin")

    l = float(bbox.get("l", 0.0))
    r = float(bbox.get("r", 0.0))
    t = float(bbox.get("t", 0.0))
    b = float(bbox.get("b", 0.0))

    # Normalize coord origin: accept enum (with .name), strings like
    # "CoordOrigin.TOPLEFT", "TOP_LEFT", etc.
    origin_val = bbox.get("coord_origin", "BOTTOMLEFT")
    if hasattr(origin_val, "name"):
        origin_str = str(getattr(origin_val, "name", "")).upper()
    else:
        origin_str = str(origin_val).upper()
    if "." in origin_str:
        origin_str = origin_str.split(".")[-1]
    origin_str = origin_str.replace("_", "").replace("-", "")
    if origin_str not in {"BOTTOMLEFT", "TOPLEFT"}:
        raise ValueError(f"Unsupported coord_origin: {origin_val!r}")

    with fitz.open(file_path) as doc:
        page = doc[page_number - 1]
        mat = fitz.Matrix(zoom, zoom)
        pix = page.get_pixmap(matrix=mat)  # type: ignore

        img = Image.frombytes("RGB", (pix.width, pix.height), pix.samples)  # type: ignore
        x_scale = pix.width / page.rect.width
        y_scale = pix.height / page.rect.height

        if origin_str == "BOTTOMLEFT":
            left = l * x_scale
            right = r * x_scale
            top = pix.height - (t * y_scale)
            bottom = pix.height - (b * y_scale)
        else:
            left = l * x_scale
            right = r * x_scale
            top = t * y_scale
            bottom = b * y_scale

        left_i = max(0, min(round(left), img.width))
        right_i = max(0, min(round(right), img.width))
        top_i = max(0, min(round(top), img.height))
        bottom_i = max(0, min(round(bottom), img.height))

        # Ensure proper ordering after rounding/clamping
        left_i, right_i = (left_i, right_i) if left_i <= right_i else (right_i, left_i)
        top_i, bottom_i = (top_i, bottom_i) if top_i <= bottom_i else (bottom_i, top_i)

        if right_i - left_i <= 1 or bottom_i - top_i <= 1:
            raise ValueError("Invalid crop region computed from bbox")

        crop = img.crop((left_i, top_i, right_i, bottom_i))

        import base64
        import io

        buf = io.BytesIO()
        crop.save(buf, format="PNG")
        return base64.b64encode(buf.getvalue()).decode("utf-8")



================================================
FILE: datapizza-ai-modules/parsers/docling/datapizza/modules/parsers/docling/tests/conftest.py
================================================
import pytest

from datapizza.modules.parsers.docling.docling_parser import DoclingParser


@pytest.fixture
def mock_docling_parser(monkeypatch, tmp_path):
    """
    Fixture to create a DoclingParser instance with mocked converter behavior.
    """
    parser = DoclingParser(json_output_dir=str(tmp_path))

    # Mock converter and convert() result
    class MockResult:
        def __init__(self):
            self.document = self

        def export_to_dict(self):
            # Minimal fake Docling JSON
            return {
                "schema_name": "docling_test",
                "version": "1.0",
                "name": "mock_doc",
                "origin": "unit_test",
                "body": {"children": []},
            }

    class MockConverter:
        def convert(self, file_path):
            return MockResult()

    monkeypatch.setattr(parser, "_get_converter", lambda: MockConverter())
    return parser



================================================
FILE: datapizza-ai-modules/parsers/docling/datapizza/modules/parsers/docling/tests/test_docling_parser.py
================================================
import warnings

import pytest
from datapizza.type import Node, NodeType

from datapizza.modules.parsers.docling.docling_parser import DoclingParser
from datapizza.modules.parsers.docling.ocr_options import OCREngine, OCROptions


def test_parse_with_file_path(mock_docling_parser, tmp_path):
    """Test that parse() works with the new 'file_path' parameter."""
    dummy_file = tmp_path / "dummy.pdf"
    dummy_file.write_text("fake-pdf-content")

    node = mock_docling_parser.parse(file_path=str(dummy_file))
    assert isinstance(node, Node)
    assert node.node_type == NodeType.DOCUMENT
    assert node.metadata["name"] == "mock_doc"
    assert node.metadata["schema_name"] == "docling_test"


def test_parse_with_pdf_path_deprecated(mock_docling_parser, tmp_path):
    """Test parse() with deprecated 'pdf_path' and issues warning."""
    dummy_file = tmp_path / "legacy.pdf"
    dummy_file.write_text("fake-pdf")

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")
        node = mock_docling_parser.parse(pdf_path=str(dummy_file))

        assert isinstance(node, Node)
        assert len(w) == 1
        assert issubclass(w[0].category, DeprecationWarning)
        assert "pdf_path" in str(w[0].message)


def test_parse_with_both_file_and_pdf_path(mock_docling_parser, tmp_path):
    """Ensure file_path takes precedence if both are given."""
    dummy_file1 = tmp_path / "primary.pdf"
    dummy_file1.write_text("pdf1")
    dummy_file2 = tmp_path / "secondary.pdf"
    dummy_file2.write_text("pdf2")

    with warnings.catch_warnings(record=True) as w:
        node = mock_docling_parser.parse(
            file_path=str(dummy_file1), pdf_path=str(dummy_file2)
        )

        # Expect a warning but use file_path
        assert len(w) == 1
        assert node.metadata["name"] == "mock_doc"


def test_parse_missing_file_path_raises():
    parser = DoclingParser()
    with pytest.raises(
        ValueError, match="Missing required argument: file_path"
    ):
        parser.parse()


def test_parser_ocr_options_backward_compatibility(mock_docling_parser):
    """Test parser works without explicit OCR options (backward compat)."""
    # Parser created without ocr_options should use default (EasyOCR)
    assert mock_docling_parser.ocr_options.engine == OCREngine.EASY_OCR
    assert (
        mock_docling_parser.ocr_options.easy_ocr_force_full_page is True
    )


def test_parser_with_custom_ocr_options(mock_docling_parser, monkeypatch):
    """Test parser with custom OCR options."""
    custom_options = OCROptions(
        engine=OCREngine.TESSERACT,
        tesseract_lang=["ita"],
    )
    parser = DoclingParser(ocr_options=custom_options)

    assert parser.ocr_options.engine == OCREngine.TESSERACT
    assert parser.ocr_options.tesseract_lang == ["ita"]


def test_parser_with_multilingual_tesseract(mock_docling_parser):
    """Test parser with multiple languages for Tesseract."""
    custom_options = OCROptions(
        engine=OCREngine.TESSERACT,
        tesseract_lang=["ita", "eng", "fra"],
    )
    parser = DoclingParser(ocr_options=custom_options)

    assert parser.ocr_options.engine == OCREngine.TESSERACT
    assert parser.ocr_options.tesseract_lang == ["ita", "eng", "fra"]


def test_parser_with_autodetect_tesseract(mock_docling_parser):
    """Test parser with autodetect for Tesseract."""
    custom_options = OCROptions(
        engine=OCREngine.TESSERACT,
        tesseract_lang=["auto"],
    )
    parser = DoclingParser(ocr_options=custom_options)

    assert parser.ocr_options.engine == OCREngine.TESSERACT
    assert parser.ocr_options.tesseract_lang == ["auto"]


def test_parser_with_ocr_disabled(mock_docling_parser):
    """Test parser with OCR disabled."""
    custom_options = OCROptions(engine=OCREngine.NONE)
    parser = DoclingParser(ocr_options=custom_options)

    assert parser.ocr_options.engine == OCREngine.NONE


def test_parser_preserves_json_output_dir_with_ocr_options(tmp_path):
    """Test parser preserves json_output_dir when using custom OCR options."""
    custom_options = OCROptions(engine=OCREngine.TESSERACT)
    parser = DoclingParser(
        json_output_dir=str(tmp_path),
        ocr_options=custom_options,
    )

    assert parser.json_output_dir == str(tmp_path)
    assert parser.ocr_options.engine == OCREngine.TESSERACT


def test_parse_with_metadata(mock_docling_parser, tmp_path):
    """Test that parse() correctly merges user-provided metadata."""
    dummy_file = tmp_path / "test.pdf"
    dummy_file.write_text("fake-pdf-content")

    user_metadata = {
        "source": "user_upload",
        "custom_field": "test_value",
    }

    node = mock_docling_parser.parse(
        file_path=str(dummy_file), metadata=user_metadata
    )

    assert node.metadata["source"] == "user_upload"
    assert node.metadata["custom_field"] == "test_value"
    # Ensure original metadata is preserved
    assert node.metadata["name"] == "mock_doc"
    assert node.metadata["schema_name"] == "docling_test"


def test_parse_with_none_metadata(mock_docling_parser, tmp_path):
    """Test that parse() works correctly when metadata is None."""
    dummy_file = tmp_path / "test.pdf"
    dummy_file.write_text("fake-pdf-content")

    node = mock_docling_parser.parse(
        file_path=str(dummy_file), metadata=None
    )

    assert isinstance(node, Node)
    assert node.node_type == NodeType.DOCUMENT
    assert node.metadata is not None


def test_parse_metadata_type_validation(mock_docling_parser, tmp_path):
    """Test that parse() raises TypeError for invalid metadata type."""
    dummy_file = tmp_path / "test.pdf"
    dummy_file.write_text("fake-pdf-content")

    with pytest.raises(TypeError, match="metadata must be a dict or None"):
        mock_docling_parser.parse(
            file_path=str(dummy_file), metadata="invalid_string"
        )

    with pytest.raises(TypeError, match="metadata must be a dict or None"):
        mock_docling_parser.parse(file_path=str(dummy_file), metadata=123)

    with pytest.raises(TypeError, match="metadata must be a dict or None"):
        mock_docling_parser.parse(
            file_path=str(dummy_file), metadata=["list", "of", "items"]
        )


def test_parse_metadata_override(mock_docling_parser, tmp_path):
    """Test that user metadata overrides parser-generated metadata."""
    dummy_file = tmp_path / "test.pdf"
    dummy_file.write_text("fake-pdf-content")

    # First, get the default metadata
    node1 = mock_docling_parser.parse(file_path=str(dummy_file))
    original_name = node1.metadata.get("name")
    assert original_name == "mock_doc"

    # Now override with user metadata
    user_metadata = {"name": "custom_name"}
    node2 = mock_docling_parser.parse(
        file_path=str(dummy_file), metadata=user_metadata
    )

    # User metadata should override
    assert node2.metadata["name"] == "custom_name"
    assert node2.metadata["name"] != original_name


def test_parse_with_metadata_and_deprecated_pdf_path(
    mock_docling_parser, tmp_path
):
    """Test metadata works with deprecated pdf_path parameter."""
    dummy_file = tmp_path / "legacy.pdf"
    dummy_file.write_text("fake-pdf")

    user_metadata = {"source": "legacy_path"}

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")
        node = mock_docling_parser.parse(
            pdf_path=str(dummy_file), metadata=user_metadata
        )

        assert isinstance(node, Node)
        assert node.metadata["source"] == "legacy_path"
        assert len(w) == 1
        assert issubclass(w[0].category, DeprecationWarning)



================================================
FILE: datapizza-ai-modules/rerankers/cohere/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-modules/rerankers/cohere/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-rerankers-cohere"
version = "0.0.4"
description = "Cohere reranker for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.0,<0.1.0",
    "cohere>=5.14.0,<6.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-modules/rerankers/cohere/datapizza/modules/rerankers/cohere/__init__.py
================================================
from .cohere_reranker import CohereReranker

__all__ = ["CohereReranker"]



================================================
FILE: datapizza-ai-modules/rerankers/cohere/datapizza/modules/rerankers/cohere/cohere_reranker.py
================================================
from datapizza.core.modules.reranker import Reranker
from datapizza.type import Chunk


class CohereReranker(Reranker):
    """A reranker that uses the Cohere API to rerank documents."""

    def __init__(
        self,
        api_key: str,
        endpoint: str,
        top_n: int = 10,
        threshold: float | None = None,
        model: str = "model",
    ):
        """
        Args:
            api_key: The API key for the Cohere API.
            endpoint: The endpoint for the Cohere API.
            top_n: The number of documents to return.
            threshold: The threshold for the reranker.
        """

        self.api_key = api_key
        self.endpoint = endpoint
        self.top_n = top_n
        self.threshold = threshold
        self.model = model

        self.client = None
        self.a_client = None

    def _set_client(self):
        import cohere

        if not self.client:
            self.client = cohere.ClientV2(base_url=self.endpoint, api_key=self.api_key)

    def _set_a_client(self):
        import cohere

        if not self.a_client:
            self.a_client = cohere.AsyncClientV2(
                base_url=self.endpoint,
                api_key=self.api_key,
            )

    def _get_client(self):
        if not self.client:
            self._set_client()

        if not self.client:
            raise RuntimeError("Client not set")

        return self.client

    def _get_a_client(self):
        if not self.a_client:
            self._set_a_client()

        if not self.a_client:
            raise RuntimeError("Client not set")

        return self.a_client

    def rerank(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        """
        Rerank documents based on query.

        Args:
            query: The query to rerank documents by.
            documents: The documents to rerank.

        Returns:
            The reranked documents.
        """
        client = self._get_client()

        response = client.rerank(
            model=self.model,
            query=query,
            documents=[single_document.text for single_document in documents],
            top_n=self.top_n,
        )

        result_chunks: list[Chunk] = []

        for document in response.results:
            index = document.index
            relevance_score = document.relevance_score

            if self.threshold is not None and relevance_score < self.threshold:
                continue

            original_chunk = documents[index]
            updated_metadata = {
                **original_chunk.metadata,
                "reranker_score": relevance_score,
            }
            result_chunks.append(
                Chunk(
                    id=original_chunk.id,
                    text=original_chunk.text,
                    embeddings=original_chunk.embeddings,
                    metadata=updated_metadata,
                )
            )

        return result_chunks

    async def a_rerank(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        """
        Rerank documents based on query.

        Args:
            query: The query to rerank documents by.
            documents: The documents to rerank.

        Returns:
            The reranked documents.
        """
        if not documents:
            return []

        client = self._get_a_client()

        response = await client.rerank(
            model=self.model,
            query=query,
            documents=[single_document.text for single_document in documents],
            top_n=self.top_n,
        )

        result_chunks: list[Chunk] = []

        for result in response.results:
            index = result.index
            score = result.relevance_score

            # Apply threshold filtering if specified
            if self.threshold is not None and score < self.threshold:
                continue

            original_chunk = documents[index]

            # Add reranker_score to metadata
            updated_metadata = {**original_chunk.metadata, "reranker_score": score}
            result_chunks.append(
                Chunk(
                    id=original_chunk.id,
                    text=original_chunk.text,
                    embeddings=original_chunk.embeddings,
                    metadata=updated_metadata,
                )
            )

        return result_chunks



================================================
FILE: datapizza-ai-modules/rerankers/together/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-modules/rerankers/together/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-rerankers-together"
version = "0.0.3"
description = "Together AI reranker for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.0,<0.1.0",
    "together>=1.5.4,<2.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-modules/rerankers/together/datapizza/modules/rerankers/together/__init__.py
================================================
from .together_reranker import TogetherReranker

__all__ = ["TogetherReranker"]



================================================
FILE: datapizza-ai-modules/rerankers/together/datapizza/modules/rerankers/together/together_reranker.py
================================================
from datapizza.core.modules.reranker import Reranker
from datapizza.type import Chunk


class TogetherReranker(Reranker):
    """A reranker that uses the Together API to rerank documents."""

    def __init__(
        self,
        api_key: str,
        model: str,
        top_n: int = 10,
        threshold: float | None = None,
    ):
        """Initialize the TogetherReranker.

        Args:
            api_key (str): Together API key
            model (str): Model name to use for reranking
            top_n (int): Number of top documents to return
            threshold (Optional[float]): Minimum relevance score threshold. If None, no filtering is applied.
        """
        try:
            from together import Together
        except Exception as e:
            raise ValueError(f"Error importing together: {e}") from e

        self.client = Together(api_key=api_key)
        self.model = model
        self.top_n = top_n
        self.threshold = threshold

    def rerank(self, query: str, documents: list[Chunk]) -> list[Chunk]:
        """
        Rerank documents based on query.

        Args:
            query: The query to rerank documents by.
            documents: The documents to rerank.

        Returns:
            The reranked documents.
        """
        if not documents:
            return []

        top_n = min(self.top_n, len(documents))

        response = self.client.rerank.create(
            model=self.model,
            query=query,
            documents=[doc.text for doc in documents],
            top_n=top_n,
        )

        if response.results is None:
            return []

        # Create a list of (index, score) tuples from results
        scored_indices = [(r.index, r.relevance_score) for r in response.results]

        if self.threshold is not None:
            # Filter by threshold and sort by score
            scored_indices = [
                (idx, score) for idx, score in scored_indices if score >= self.threshold
            ]

        # Sort by score descending and extract just the indices
        sorted_indices = [
            idx for idx, _ in sorted(scored_indices, key=lambda x: x[1], reverse=True)
        ]

        # Return reranked documents in order
        return [documents[idx] for idx in sorted_indices]



================================================
FILE: datapizza-ai-tools/duckduckgo/README.md
================================================
[Empty file]


================================================
FILE: datapizza-ai-tools/duckduckgo/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-tools-duckduckgo"
version = "0.0.4"
description = "Azure Document Intelligence parser for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Datapizza", email = "datapizza@datapizza.tech"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "ddgs>=9.6.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-tools/duckduckgo/datapizza/tools/duckduckgo/__init__.py
================================================
from .base import DuckDuckGoSearchTool

__all__ = ["DuckDuckGoSearchTool"]



================================================
FILE: datapizza-ai-tools/duckduckgo/datapizza/tools/duckduckgo/base.py
================================================
from ddgs import DDGS

from datapizza.tools import Tool


class DuckDuckGoSearchTool(Tool):
    """
    The DuckDuckGo Search tool.
    It allows you to search the web for the given query.
    """

    def __init__(self):
        """Initializes the DuckDuckGoSearch tool."""
        super().__init__(
            name="duckduckgo_search",
            description="Enables DuckDuckGo Search for grounding model responses.",
            func=self.__call__,
        )

    def _format_results(self, results: list[str]) -> str:
        """Format the results."""
        return "## Search Results\n\n" + "\n\n".join(
            [
                f"[{result['title']}]({result['href']})\n{result['body']}"
                for result in results
            ]
        )

    def __call__(self, query: str) -> list[str]:
        """Invoke the tool."""
        res = self.search(query)
        return self._format_results(res)

    def search(self, query: str) -> list[str]:
        """Search the web for the given query."""
        with DDGS() as ddg:
            results = list(ddg.text(query))
            return results



================================================
FILE: datapizza-ai-tools/duckduckgo/tests/test_ddgs_tools.py
================================================
from datapizza.tools.duckduckgo import DuckDuckGoSearchTool


def test_duckduckgo_search():
    tool = DuckDuckGoSearchTool()

    assert tool.name == "duckduckgo_search"



================================================
FILE: datapizza-ai-tools/filesystem/README.md
================================================
<div align="center">
<img src="https://github.com/datapizza-labs/datapizza-ai/raw/main/docs/assets/logo_bg_dark.png" alt="Datapizza AI Logo" width="200" height="200">

# Datapizza AI - FileSystem Tool

**A tool for Datapizza AI that allows agents to interact with the local file system.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

</div>

---

This tool provides a robust and easy-to-use interface for `datapizza-ai` agents to perform various operations on the local file system, including listing, reading, writing, creating, deleting, moving, copying, and replacing content within files and directories.

> **âš ï¸ Warning: Risk of Data Loss and System Modification**
>
> Operations performed by this tool directly affect your local file system. Using functions like `delete_file`, `delete_directory`, and `write_file` can lead to permanent data loss or unintended system modifications if not used carefully. Exercise extreme caution. Before performing critical operations, consider the following:
>
> - Always double-check the paths and parameters.
> - Test operations in a safe, isolated environment (e.g., a temporary directory).
> - Ensure you have recent backups of important data.

## âš™ï¸ How it Works

The `FileSystem` is a class that, once initialized, exposes several distinct functionalities to an agent:

1.  `list_directory(path: str)`: Lists all files and directories in a given path. Returns a formatted string of entries.
2.  `read_file(file_path: str)`: Reads the content of a specified file. Returns the file's content as a string.
3.  `write_file(file_path: str, content: str)`: Writes content to a specified file. Creates the file if it does not exist. Returns a success or error message.
4.  `create_directory(path: str)`: Creates a new directory at the specified path. Returns a success or error message.
5.  `delete_file(file_path: str)`: Deletes a specified file. Returns a success or error message.
6.  `delete_directory(path: str, recursive: bool = False)`: Deletes a specified directory. If `recursive` is True, deletes the directory and all its contents. Returns a success or error message.
7.  `move_item(source_path: str, destination_path: str)`: Moves or renames a file or directory from `source_path` to `destination_path`. Returns a success or error message.
8.  `copy_file(source_path: str, destination_path: str)`: Copies a file from `source_path` to `destination_path`. Returns a success or error message.
9.  `replace_in_file(file_path: str, old_string: str, new_string: str)`: Replaces a string in a file only if it appears exactly once. For safety, `old_string` should contain context to be unique.

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install the core framework
pip install datapizza-ai

# Install the FileSystem tool
pip install datapizza-ai-tools-filesystem
```

### 2. Example: Creating a File System Management Agent

In this example, we'll create an agent that can perform various file system operations within a temporary directory.

```python
import os
import tempfile
import shutil
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.filesystem import FileSystem

# ---
# Setup: Create a temporary directory for the example
# ---
temp_dir_path = tempfile.mkdtemp()
print(f"Working in temporary directory: {temp_dir_path}")

# Create some initial files/directories for demonstration
with open(os.path.join(temp_dir_path, "initial_file.txt"), "w") as f:
    f.write("This is the initial content.")

os.makedirs(os.path.join(temp_dir_path, "initial_dir"), exist_ok=True)

with open(os.path.join(temp_dir_path, "initial_dir", "nested_file.txt"), "w") as f:
    f.write("Nested content here.")

# ---
# End of Setup
# ---


# 1. Initialize the FileSystem
fs_tool = FileSystem()

# 2. Initialize a client (e.g., OpenAI)
client = OpenAIClient(api_key="YOUR_API_KEY")

# 3. Create an agent and provide it with the file system tools
agent = Agent(
    name="filesystem_manager",
    client=client,
    system_prompt=f"""You are an expert and careful file system manager. Your primary goal is to perform file system operations as requested by the user within the directory: {temp_dir_path}.

Follow these steps:
1.  Use `list_directory` to inspect the contents of directories.
2.  Use `read_file` to view file contents.
3.  Use `write_file` to create or modify files.
4.  Use `create_directory` to make new folders.
5.  Use `delete_file` or `delete_directory` to remove items.
6.  Use `move_item` to rename or move files/directories.
7.  Use `copy_file` to duplicate files.
8.  Use `replace_in_file` to modify file content.

""",
    tools=[
        fs_tool.list_directory,
        fs_tool.read_file,
        fs_tool.write_file,
        fs_tool.create_directory,
        fs_tool.delete_file,
        fs_tool.delete_directory,
        fs_tool.move_item,
        fs_tool.copy_file,
        fs_tool.replace_in_file,
    ]
)

# 4. Run the agent to perform file system tasks
print("--- Query 1: List initial directory contents ---")
response = agent.run(f"List the contents of the directory: {temp_dir_path}")
print(f"Agent Response: {response.text}")

print("--- Query 2: Create a new file ---")
response = agent.run(f"Create a file named 'new_document.txt' in {temp_dir_path} with the content 'Hello from Datapizza AI!'")
print(f"Agent Response: {response.text}")

print("--- Query 3: Read the new file ---")
response = agent.run(f"Read the content of 'new_document.txt' in {temp_dir_path}")
print(f"Agent Response: {response.text}")

print("--- Query 4: Create a new directory ---")
response = agent.run(f"Create a directory named 'reports' inside {temp_dir_path}")
print(f"Agent Response: {response.text}")

print("--- Query 5: Move a file ---")
response = agent.run(f"Move 'new_document.txt' from {temp_dir_path} to the 'reports' directory and rename it to 'report_draft.txt'")
print(f"Agent Response: {response.text}")

print("--- Query 6: Copy a file ---")
response = agent.run(f"Copy 'initial_file.txt' from {temp_dir_path} to {temp_dir_path}/reports and name the copy 'initial_file_copy.txt'")
print(f"Agent Response: {response.text}")

print("--- Query 7: Replace content in a file ---")
response = agent.run(f"In the file '{temp_dir_path}/initial_file.txt', replace the unique string 'initial content' with 'updated content'")
print(f"Agent Response: {response.text}")

print("--- Query 8: Delete a file ---")
response = agent.run(f"Delete the file '{temp_dir_path}/reports/initial_file_copy.txt'")
print(f"Agent Response: {response.text}")

print("--- Query 9: Delete a directory recursively ---")
response = agent.run(f"Delete the 'initial_dir' directory inside {temp_dir_path} including all its contents.")
print(f"Agent Response: {response.text}")

# ---
# Teardown: Clean up the temporary directory
# ---
shutil.rmtree(temp_dir_path)
print(f"Cleaned up temporary directory: {temp_dir_path}")
# ---
# End of Teardown
# ---
```

### Expected Output:

```
Working in temporary directory: /tmp/tmp_XXXXXX (actual path will vary)
--- Query 1: List initial directory contents ---
Agent Response: [DIR] initial_dir
[FILE] initial_file.txt

--- Query 2: Create a new file ---
Agent Response: Successfully wrote to file '/tmp/tmp_XXXXXX/new_document.txt'.

--- Query 3: Read the new file ---
Agent Response: Hello from Datapizza AI!

--- Query 4: Create a new directory ---
Agent Response: Successfully created directory '/tmp/tmp_XXXXXX/reports'.

--- Query 5: Move a file ---
Agent Response: Successfully moved '/tmp/tmp_XXXXXX/new_document.txt' to '/tmp/tmp_XXXXXX/reports/report_draft.txt'.

--- Query 6: Copy a file ---
Agent Response: Successfully copied '/tmp/tmp_XXXXXX/initial_file.txt' to '/tmp/tmp_XXXXXX/reports/initial_file_copy.txt'.

--- Query 7: Replace content in a file ---
Agent Response: Replacement successful in file '/tmp/tmp_XXXXXX/initial_file.txt'.

--- Query 8: Delete a file ---
Agent Response: Successfully deleted file '/tmp/tmp_XXXXXX/reports/initial_file_copy.txt'.

--- Query 9: Delete a directory recursively ---
Agent Response: Successfully deleted directory '/tmp/tmp_XXXXXX/initial_dir'.

Cleaned up temporary directory: /tmp/tmp_XXXXXX (actual path will vary)
```



================================================
FILE: datapizza-ai-tools/filesystem/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-tools-filesystem"
version = "0.0.3"
description = "A tool for interacting with the local file system."
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Nilthon Jhon Rojas Apumayta", email = "oneill.jhon97@gmail.com"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-tools/filesystem/datapizza/tools/filesystem/__init__.py
================================================
from .filesystem import FileSystem, string_matches_patterns

__all__ = ["FileSystem", "string_matches_patterns"]



================================================
FILE: datapizza-ai-tools/filesystem/datapizza/tools/filesystem/filesystem.py
================================================
import fnmatch
import os
import re

from datapizza.tools import tool


def string_matches_patterns(string_to_check: str, patterns: list[str]) -> bool:
    def _check_pattern(string_to_check: str, pattern: str) -> bool:
        # glob patterns
        regex = fnmatch.translate(pattern.lower())
        if re.match(regex, string_to_check.lower()):
            return True
        # regex patterns
        try:
            if re.match(pattern, string_to_check.lower()):
                return True
        except re.error:
            pass

        return False

    if len(patterns) == 0:
        return True

    return any(_check_pattern(string_to_check, pattern) for pattern in patterns)


class FileSystem:
    """A collection of tools for interacting with the local file system."""

    def __init__(self, paths_to_include=None, paths_to_exclude=None) -> None:
        """
        Initialize the FileSystem. You can set `paths_to_include` and `paths_to_exclude` as glob or regex patterns on paths to reduce the scope of the tool.
        By default, all paths are included. Exclusion patterns are evaluated after the inclusion patterns, Therefore, exclusion patterns should apply subfilters on the inclusion patterns to be effective.
        Example usage::

                        FileSystem() #includes the whole file system
                        FileSystem(paths_to_include=["/project/dir/*"]) # includes all the files in /project/dir/
                        FileSystem(paths_to_exclude=["/project/dir/.env"], paths_to_include=["/project/dir/*"]) # includes all the files and directories in /project/dir/ except .env
                        FileSystem(paths_to_exclude=["*/.env"], paths_to_include=["/project/dir/*"]) # as above, includes all the files and directories in /project/dir/ except .env
                        FileSystem(paths_to_exclude=["/data/archive.zip"], paths_to_include=["*.zip", "*.txt"]) #includes all txt and zip files except /data/archive.zip

        Args:
                paths_to_include (list[str], optional): Define a list of glob or regular expression patterns for paths to include. Defaults to None.
                paths_to_exclude (list[str], optional): Define a list of glob or regular expression patterns to exclude. It is evaluated after inclusion patterns. Defaults to None.
        """
        self.include_patterns = paths_to_include if paths_to_include else ["*"]
        self.exclude_patterns = paths_to_exclude if paths_to_exclude else []

    def is_path_valid(self, path: str) -> bool:
        if string_matches_patterns(path, self.include_patterns):
            return not (
                self.exclude_patterns
                and string_matches_patterns(path, self.exclude_patterns)
            )
        return False

    @tool
    def list_directory(self, path: str) -> str:
        """
        Lists all valid files and directories in a given path.
        :param path: The path of the directory to list.
        """

        if not os.path.isdir(path):
            return f"Error: Path '{path}' is not a valid directory."

        try:
            entries = os.listdir(path)
            if not entries:
                return f"The directory '{path}' is empty."

            formatted_entries = []
            for entry in entries:
                entry_path = os.path.join(path, entry)
                if self.is_path_valid(entry_path):
                    if os.path.isdir(entry_path):
                        formatted_entries.append(f"[DIR] {entry}")
                    else:
                        formatted_entries.append(f"[FILE] {entry}")

            return "\n".join(formatted_entries)
        except Exception as e:
            return f"An error occurred: {e}"

    @tool
    def read_file(self, file_path: str) -> str:
        """
        Reads the content of a specified file.
        :param file_path: The path of the file to read.
        """
        if not self.is_path_valid(file_path):
            return f"Path '{file_path}' is outside the tool's scope."
        try:
            with open(file_path, encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            return f"Error: File '{file_path}' not found."
        except Exception as e:
            return f"An error occurred: {e}"

    @tool
    def write_file(self, file_path: str, content: str) -> str:
        """
        Writes content to a specified file. Creates the file if it does not exist.
        :param file_path: The path of the file to write to.
        :param content: The content to write to the file.
        """
        if not self.is_path_valid(file_path):
            return f"Path '{file_path}' is outside the tool's scope."
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)
            return f"Successfully wrote to file '{file_path}'."
        except Exception as e:
            return f"An error occurred: {e}"

    @tool
    def create_directory(self, path: str) -> str:
        """
        Creates a new directory at the specified path.
        :param path: The path where the new directory should be created.
        """
        if not self.is_path_valid(path):
            return f"Path '{path}' is outside the tool's scope."
        try:
            os.makedirs(path, exist_ok=True)
            return f"Successfully created directory '{path}'."
        except Exception as e:
            return f"An error occurred while creating directory '{path}': {e}"

    @tool
    def delete_file(self, file_path: str) -> str:
        """
        Deletes a specified file.
        :param file_path: The path of the file to delete.
        """
        if not self.is_path_valid(file_path):
            return f"Path '{file_path}' is outside the tool's scope."
        try:
            os.remove(file_path)
            return f"Successfully deleted file '{file_path}'."
        except FileNotFoundError:
            return f"Error: File '{file_path}' not found."
        except Exception as e:
            return f"An error occurred while deleting file '{file_path}': {e}"

    @tool
    def delete_directory(self, path: str, recursive: bool = False) -> str:
        """
        Deletes a specified directory.
        :param path: The path of the directory to delete.
        :param recursive: If True, deletes the directory and all its contents.
        """
        if not self.is_path_valid(path):
            return f"Path '{path}' is outside the tool's scope."
        try:
            if not os.path.exists(path):
                return f"Error: Directory '{path}' not found."
            if recursive:
                import shutil

                shutil.rmtree(path)
            else:
                os.rmdir(path)
            return f"Successfully deleted directory '{path}'."
        except OSError as e:
            return f"An error occurred while deleting directory '{path}': {e}"
        except Exception as e:
            return f"An unexpected error occurred: {e}"

    @tool
    def move_item(self, source_path: str, destination_path: str) -> str:
        """
        Moves or renames a file or directory.
        :param source_path: The current path of the file or directory.
        :param destination_path: The new path for the file or directory.
        """
        if not self.is_path_valid(source_path):
            return f"Path '{source_path}' is outside the tool's scope."
        if not self.is_path_valid(destination_path):
            return f"Path '{destination_path}' is outside the tool's scope."
        try:
            os.rename(source_path, destination_path)
            return f"Successfully moved '{source_path}' to '{destination_path}'."
        except FileNotFoundError:
            return f"Error: Source '{source_path}' not found."
        except Exception as e:
            return f"An error occurred while moving '{source_path}' to '{destination_path}': {e}"

    @tool
    def copy_file(self, source_path: str, destination_path: str) -> str:
        """
        Copies a file from source to destination.
        :param source_path: The path of the file to copy.
        :param destination_path: The destination path for the new file.
        """
        if not self.is_path_valid(source_path):
            return f"Path '{source_path}' is outside the tool's scope."
        if not self.is_path_valid(destination_path):
            return f"Path '{destination_path}' is outside the tool's scope."
        try:
            import shutil

            shutil.copy2(source_path, destination_path)
            return f"Successfully copied '{source_path}' to '{destination_path}'."
        except FileNotFoundError:
            return f"Error: Source file '{source_path}' not found."
        except Exception as e:
            return f"An error occurred while copying '{source_path}' to '{destination_path}': {e}"

    @tool
    def replace_in_file(self, file_path: str, old_string: str, new_string: str) -> str:
        """
        Replaces a string in a file, but only if it appears exactly once.
        To ensure precision, the 'old_string' should include enough context (e.g., surrounding lines)
        to uniquely identify the target location.

        :param file_path: The path of the file to modify.
        :param old_string: The exact block of text to be replaced (including context).
        :param new_string: The new block of text to insert.
        """

        if not self.is_path_valid(file_path):
            return f"Path '{file_path}' is outside the tool's scope."
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            occurrences = content.count(old_string)

            if occurrences == 0:
                return f"Error: The specified 'old_string' was not found in the file '{file_path}'. No changes were made."

            if occurrences > 1:
                return f"Error: {occurrences} occurrences found in '{file_path}'. Replacement requires a unique match."

            new_content = content.replace(old_string, new_string, 1)

            with open(file_path, "w", encoding="utf-8") as f:
                f.write(new_content)

            return f"Replacement successful in file '{file_path}'."

        except FileNotFoundError:
            return f"Error: File '{file_path}' not found."
        except Exception as e:
            return f"An error occurred: {e}"



================================================
FILE: datapizza-ai-tools/filesystem/tests/test_file_path_matches_pattern.py
================================================
import pytest

from datapizza.tools.filesystem import string_matches_patterns

test_cases = [
    # (file_path, regex_patterns, expected_result)
    ("/home/user/file.sys", ["*.sys"], True),
    ("/home/user/file.sys", ["/home/*.sys"], True),
    ("/home/user/file.sys", ["/home/user/*.sys"], True),
    ("/home/user/file.sys", ["*.txt"], False),
    # Additional test cases
    ("/var/log/syslog.log", ["*.log"], True),
    ("/var/log/syslog.log", ["syslog.log"], False),
    ("/var/log/syslog.log", ["*syslog.log"], True),
    ("/var/log/syslog.log", ["/var/log/syslog.log"], True),
    ("/var/log/syslog.log", ["*.txt"], False),
    ("/data/report.pdf", ["*.PDF"], True),  # Case-insensitive match
    ("/data/report.pdf", ["report.*"], False),
    ("/data/report.pdsss", ["*report.*"], True),
    ("/data/report/pdsss", ["*report*"], True),
    ("/data/archive.tar.gz", ["*.tar.gz"], True),
    ("/data/archive.tar.gz", ["*.tar.gz", "*.txt"], True),
    ("/data/archive.tar.gz", ["*.zip", "*.txt"], False),
    ("/data/archive.tar.gz", ["*.zip", "/data/archive.tar.gz"], True),
    ("/data/archive.tar.gz", ["*"], True),
    # Complex regex patterns with explanations
    # 1. Match any .log file in /var/log directory
    ("/var/log/syslog.log", [r"^/var/log/.*\.log$"], True),
    # 2. Match .txt files in /var/log with specific naming
    ("/var/log/syslog.log", [r"^/var/log/[^/]+\.txt$"], False),
    # 3. Match date-formatted PDF reports (YYYYMMDD)
    ("/data/reports/20230801.pdf", [r"^/data/reports/\d{8}\.pdf$"], True),
    # 4. Reject non-date formatted PDF reports
    ("/data/reports/summary.pdf", [r"^/data/reports/\d{8}\.pdf$"], False),
    # 5. Match backup files in daily/weekly subdirectories
    ("/backup/daily/logs.tar.gz", [r"^/backup/(?:daily|weekly)/.*"], True),
    # 6. Reject monthly backup files (not in daily/weekly)
    ("/backup/monthly/logs.tar.gz", [r"^/backup/(?:daily|weekly)/.*"], False),
    # 7. Match localized document files (en/it)
    ("/home/user/docs/resume_en.docx", [r"^/home/user/docs/\w+_(en|it)\.docx$"], True),
    # 8. Reject non-supported language documents
    ("/home/user/docs/resume_fr.docx", [r"^/home/user/docs/\w+_(en|it)\.docx$"], False),
]


@pytest.mark.parametrize("file_path, patterns, expected", test_cases)
def test_matches_regex(file_path, patterns, expected):
    result = string_matches_patterns(file_path, patterns)
    assert result == expected, f"Failed for {file_path} with patterns {patterns}"


def test_empty_patterns_list():
    assert (
        string_matches_patterns("/any/path", []) == True
    ), "Should return True with empty patterns list"



================================================
FILE: datapizza-ai-tools/filesystem/tests/test_filesystem.py
================================================
from unittest.mock import patch

import pytest

from datapizza.tools.filesystem import FileSystem


@pytest.fixture
def fs_tool():
    return FileSystem()


@pytest.fixture
def temp_dir(tmp_path):
    d = tmp_path / "test_dir"
    d.mkdir()
    (d / "file1.txt").write_text("hello")
    (d / "subdir").mkdir()
    (d / "subdir" / "file2.txt").write_text("world")
    return d


def test_list_directory(fs_tool, temp_dir):
    result = fs_tool.list_directory(str(temp_dir))
    assert "[FILE] file1.txt" in result
    assert "[DIR] subdir" in result


def test_list_directory_with_scope(fs_tool, temp_dir):
    fs_tool.include_patterns = ["*.txt"]
    with patch(
        "os.listdir", return_value=["file_to_include.txt", "file_to_include.py"]
    ):
        result = fs_tool.list_directory(str(temp_dir))
    assert "[FILE] file_to_include.txt" in result
    assert "[FILE] file_to_include.py" not in result


def test_list_directory_empty(fs_tool, tmp_path):
    empty_dir = tmp_path / "empty"
    empty_dir.mkdir()
    result = fs_tool.list_directory(str(empty_dir))
    assert result == f"The directory '{empty_dir!s}' is empty."


def test_list_directory_not_found(fs_tool):
    result = fs_tool.list_directory("non_existent_dir")
    assert "is not a valid directory" in result


def test_read_file(fs_tool, temp_dir):
    file_path = temp_dir / "file1.txt"
    content = fs_tool.read_file(str(file_path))
    assert content == "hello"


def test_read_file_not_found(fs_tool):
    content = fs_tool.read_file("non_existent_file.txt")
    assert "not found" in content


def test_write_file(fs_tool, tmp_path):
    file_path = tmp_path / "new_file.txt"
    result = fs_tool.write_file(str(file_path), "new content")
    assert "Successfully wrote" in result
    assert file_path.read_text() == "new content"


def test_create_directory(fs_tool, tmp_path):
    new_dir_path = tmp_path / "new_test_dir"
    result = fs_tool.create_directory(str(new_dir_path))
    assert "Successfully created directory" in result
    assert new_dir_path.is_dir()

    # Test creating an already existing directory
    result_existing = fs_tool.create_directory(str(new_dir_path))
    assert (
        "Successfully created directory" in result_existing
    )  # Should still report success due to exist_ok=True
    assert new_dir_path.is_dir()


def test_delete_file(fs_tool, tmp_path):
    file_to_delete = tmp_path / "file_to_delete.txt"
    file_to_delete.write_text("delete me")
    result = fs_tool.delete_file(str(file_to_delete))
    assert "Successfully deleted file" in result
    assert not file_to_delete.exists()

    # Test deleting a non-existent file
    result_non_existent = fs_tool.delete_file(str(tmp_path / "non_existent.txt"))
    assert "not found" in result_non_existent


def test_delete_directory(fs_tool, tmp_path):
    # Test deleting an empty directory
    empty_dir = tmp_path / "empty_dir"
    empty_dir.mkdir()
    result = fs_tool.delete_directory(str(empty_dir))
    assert "Successfully deleted directory" in result
    assert not empty_dir.exists()

    # Test deleting a non-existent directory
    result_non_existent = fs_tool.delete_directory(str(tmp_path / "non_existent_dir"))
    assert "not found" in result_non_existent

    # Test deleting a non-empty directory recursively
    non_empty_dir = tmp_path / "non_empty_dir"
    non_empty_dir.mkdir()
    (non_empty_dir / "file.txt").write_text("content")
    result_recursive = fs_tool.delete_directory(str(non_empty_dir), recursive=True)
    assert "Successfully deleted directory" in result_recursive
    assert not non_empty_dir.exists()


def test_move_item(fs_tool, tmp_path):
    # Test moving and renaming a file
    source_file = tmp_path / "source.txt"
    source_file.write_text("content")
    destination_file = tmp_path / "destination.txt"
    result = fs_tool.move_item(str(source_file), str(destination_file))
    assert "Successfully moved" in result
    assert not source_file.exists()
    assert destination_file.exists()
    assert destination_file.read_text() == "content"

    # Test moving a directory
    source_dir = tmp_path / "source_dir"
    source_dir.mkdir()
    destination_dir = tmp_path / "destination_dir"
    result_dir = fs_tool.move_item(str(source_dir), str(destination_dir))
    assert "Successfully moved" in result_dir
    assert not source_dir.exists()
    assert destination_dir.is_dir()

    # Test moving a non-existent source
    result_non_existent = fs_tool.move_item(
        str(tmp_path / "non_existent_source"), str(tmp_path / "any_destination")
    )
    assert "not found" in result_non_existent


def test_copy_file(fs_tool, tmp_path):
    source_file = tmp_path / "source_copy.txt"
    source_file.write_text("original content")
    destination_file = tmp_path / "destination_copy.txt"
    result = fs_tool.copy_file(str(source_file), str(destination_file))
    assert "Successfully copied" in result
    assert destination_file.exists()
    assert destination_file.read_text() == "original content"

    # Test copying a non-existent source file
    result_non_existent = fs_tool.copy_file(
        str(tmp_path / "non_existent_source_copy.txt"),
        str(tmp_path / "any_destination_copy.txt"),
    )
    assert "not found" in result_non_existent


def test_replace_in_file_success(fs_tool, tmp_path):
    file_path = tmp_path / "test_replace_success.txt"
    file_path.write_text("hello world\nthis is a unique line")
    result = fs_tool.replace_in_file(
        str(file_path), "this is a unique line", "this is a replaced line"
    )
    assert "Replacement successful in file" in result
    assert file_path.read_text() == "hello world\nthis is a replaced line"


def test_replace_in_file_not_found(fs_tool, tmp_path):
    file_path = tmp_path / "test_replace_not_found.txt"
    file_path.write_text("hello world")
    result = fs_tool.replace_in_file(str(file_path), "goodbye", "bye")
    assert "not found" in result
    assert file_path.read_text() == "hello world"


def test_replace_in_file_multiple_occurrences(fs_tool, tmp_path):
    file_path = tmp_path / "test_replace_multiple.txt"
    file_path.write_text("hello world\nhello world")
    result = fs_tool.replace_in_file(str(file_path), "hello world", "hi world")
    assert "2 occurrences found" in result and "requires a unique match" in result
    assert file_path.read_text() == "hello world\nhello world"


def test_replace_in_file_file_not_found(fs_tool):
    result = fs_tool.replace_in_file("non_existent.txt", "a", "b")
    assert "File 'non_existent.txt' not found" in result


def test_evaluate_path_on_patterns(fs_tool):
    # Test with include patterns only
    fs_tool.include_patterns = ["*.txt"]
    assert fs_tool.is_path_valid("test.txt") is True
    assert fs_tool.is_path_valid("test.py") is False

    # Test with exclude patterns only
    fs_tool.include_patterns = ["*.tmp"]
    assert fs_tool.is_path_valid("test.txt") is False
    assert fs_tool.is_path_valid("test.tmp") is True

    # Test with both include and exclude patterns
    fs_tool.include_patterns = ["*.txt"]
    assert fs_tool.is_path_valid("test.txt") is True
    assert fs_tool.is_path_valid("temp.txt") is True
    assert fs_tool.is_path_valid("test.py") is False

    # Test with regex patterns
    fs_tool.include_patterns = [r".*\.txt$"]
    assert fs_tool.is_path_valid("test.txt") is True
    assert fs_tool.is_path_valid("test.py") is False

    # Test with complex patterns
    fs_tool.include_patterns = ["*.txt", "*.md"]
    assert fs_tool.is_path_valid("test.txt") is True
    assert fs_tool.is_path_valid("doc.md") is True
    assert fs_tool.is_path_valid("temp.txt") is True
    assert fs_tool.is_path_valid("backup.md") is True
    assert fs_tool.is_path_valid("test.py") is False

    fs_tool.include_patterns = ["*.txt", "*.md"]
    assert fs_tool.is_path_valid("specific.txt") is True
    assert fs_tool.is_path_valid("/dir/specific.txt") is True
    assert fs_tool.is_path_valid("/dir/specific.py") is False

    fs_tool.include_patterns = ["/specific/dir/*.txt"]
    fs_tool.exclude_patterns = ["*/secrets.txt"]
    assert fs_tool.is_path_valid("/specific/dir/script.py") is False
    assert fs_tool.is_path_valid("/specific/dir/secrets.txt") is False
    assert fs_tool.is_path_valid("/specific/dir/script.txt") is True
    assert fs_tool.is_path_valid("/other/dir/script.txt") is False



================================================
FILE: datapizza-ai-tools/SQLDatabase/README.md
================================================
<div align="center">
<img src="https://github.com/datapizza-labs/datapizza-ai/raw/main/docs/assets/logo_bg_dark.png" alt="Datapizza AI Logo" width="200" height="200">

# Datapizza AI - SQLDatabase Tool

**A tool for Datapizza AI that allows agents to interact with SQL databases using SQLAlchemy.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

</div>

---

This tool provides a robust and easy-to-use interface for connecting `datapizza-ai` agents to any SQL database supported by SQLAlchemy (including SQLite, PostgreSQL, MySQL, and more).

Agents equipped with this tool can inspect database schemas, list tables, and execute SQL queries to answer questions based on structured data.

> **âš ï¸ Warning: Risk of Data Modification**
>
> Using queries like `INSERT`, `UPDATE`, and `DELETE` will permanently modify the data in your database. Exercise extreme caution. Before performing write operations in a production environment, consider the following:
> - Test queries in a development or staging environment.
> - Use a "query-writing" agent to generate and validate the SQL before execution.
> - Ensure you have recent backups of your database.

## âš™ï¸ How it Works

The `SQLDatabase` tool is a class that, once initialized with a database URI, exposes three distinct functionalities to an agent:

1.  `list_tables()`: Lists all tables available in the database. Returns a newline-separated string of table names.
2.  `get_table_schema(table_name: str)`: Retrieves the schema for a specified table. Returns a formatted string describing the columns and their data types.
3.  `run_sql_query(query: str)`: Executes a given SQL query. For `SELECT` statements, it returns a JSON-formatted string of the results. For other operations (e.g., `INSERT`, `UPDATE`), it returns a success message.

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install the core framework
pip install datapizza-ai

# Install the SQLDatabase tool
pip install datapizza-ai-tools-sqldatabase
```

> **Note on Database Drivers:**
>
> This tool uses SQLAlchemy, which requires specific DB-API drivers to connect to different databases. For example, if you want to connect to PostgreSQL or MySQL, you'll need to install their respective drivers:
>
> ```bash
> # For PostgreSQL
> pip install psycopg2-binary
>
> # For MySQL
> pip install mysql-connector-python
> ```
>
> Please refer to the [SQLAlchemy documentation](https://docs.sqlalchemy.org/en/20/dialects/) for a full list of supported databases and their required drivers.

### 2. Example: Creating a Database Expert Agent

In this example, we'll create an agent that can answer questions about a simple SQLite database.

```python
import sqlite3
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.SQLDatabase import SQLDatabase

# ---
# Setup: Create a dummy database for the example
# In a real scenario, you would connect to your existing database.
# ---
db_uri = "sqlite:///company.db"

# Clean up previous runs if the file exists
try:
    import os
    os.remove("company.db")
except OSError:
    pass

# Create and populate the database
conn = sqlite3.connect('company.db')
cursor = conn.cursor()
cursor.execute("CREATE TABLE employees (id INTEGER PRIMARY KEY, name TEXT, department TEXT, salary INTEGER)")
cursor.execute("INSERT INTO employees (name, department, salary) VALUES ('Alice', 'Engineering', 80000)")
cursor.execute("INSERT INTO employees (name, department, salary) VALUES ('Bob', 'HR', 65000)")
cursor.execute("INSERT INTO employees (name, department, salary) VALUES ('Charlie', 'Engineering', 95000)")
conn.commit()
conn.close()
# ---
# End of Setup
# ---


# 1. Initialize the SQLDatabase tool with the database URI
db_tools = SQLDatabase(db_uri=db_uri)

# 2. Initialize a client (e.g., OpenAI)
client = OpenAIClient(api_key="YOUR_API_KEY")

# 3. Create an agent and provide it with the database tools
agent = Agent(
    name="database_expert",
    client=client,
    system_prompt="""You are an expert and careful SQL database assistant. Your primary goal is to answer questions about the database by executing queries.

Follow these steps:
1.  Use `list_tables` to identify the relevant tables.
2.  Use `get_table_schema` to understand the columns and data types of those tables before writing a query.
3.  Construct an efficient SQL query to answer the user's question.
4.  Execute the query using `run_sql_query`.
5.  Analyze the results and provide a clear, human-readable answer to the user.

**Important:** Be extra cautious with `UPDATE` and `DELETE` operations. Ensure you understand the request correctly before modifying data.""",
    tools=[
        db_tools.list_tables,
        db_tools.get_table_schema,
        db_tools.run_sql_query
    ]
)

# 4. Run the agent to answer questions
print("--- Query 1: What are the available tables? ---")
response = agent.run("What tables are in the database?")
print(f"Agent Response: {response.text}\n")

print("--- Query 2: How many employees are in the Engineering department? ---")
response = agent.run("How many employees work in the Engineering department?")
print(f"Agent Response: {response.text}\n")

print("--- Query 3: Who is the highest-paid employee? ---")
response = agent.run("Who is the highest-paid employee and what is their salary?")
print(f"Agent Response: {response.text}\n")

print("--- Query 4: Update Bob's salary ---")
response = agent.run("Update Bob's salary to 70000.")
print(f"Agent Response: {response.text}\n")

print("--- Query 5: Add a new employee ---")
response = agent.run("Add a new employee named 'David' in the 'Sales' department with a salary of 75000.")
print(f"Agent Response: {response.text}\n")

print("--- Query 6: Delete an employee ---")
response = agent.run("Remove the employee named 'Bob' from the database.")
print(f"Agent Response: {response.text}\n")

```

### Expected Output:

```
--- Query 1: What are the available tables? ---
Agent Response: The database contains the following table:

- **employees**

--- Query 2: How many employees are in the Engineering department? ---
Agent Response: There are 2 employees working in the Engineering department.

--- Query 3: Who is the highest-paid employee? ---
Agent Response: The highest-paid employee is Charlie, with a salary of $95,000.

--- Query 4: Update Bob's salary ---
Agent Response: Bob's salary has been successfully updated to 70000.

--- Query 5: Add a new employee ---
Agent Response: I have successfully added David to the employees table.

--- Query 6: Delete an employee ---
Agent Response: The employee named Bob has been removed from the database.
```



================================================
FILE: datapizza-ai-tools/SQLDatabase/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-tools-sqldatabase"
version = "0.0.2"
description = "A tool to interact with SQL databases for the datapizza-ai framework."
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Nilthon Jhon Rojas Apumayta", email = "oneill.jhon97@gmail.com"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "sqlalchemy>=2.0.44",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-tools/SQLDatabase/datapizza/tools/SQLDatabase/__init__.py
================================================
from .base import SQLDatabase

__all__ = ["SQLDatabase"]



================================================
FILE: datapizza-ai-tools/SQLDatabase/datapizza/tools/SQLDatabase/base.py
================================================
import json

from sqlalchemy import create_engine, inspect, text

from datapizza.tools import tool


class SQLDatabase:
    """
    A collection of tools to interact with a SQL database using SQLAlchemy.
    This class is a container for methods that are exposed as tools.
    """

    def __init__(self, db_uri: str):
        """
        Initializes the SQLDatabase tool container.

        Args:
            db_uri (str): The database URI for connection (e.g., "sqlite:///my_database.db").
        """
        self.engine = create_engine(db_uri)

    @tool
    def list_tables(self) -> str:
        """
        Returns a newline-separated string of available table names in the database.
        """
        inspector = inspect(self.engine)
        return "\n".join(inspector.get_table_names())

    @tool
    def get_table_schema(self, table_name: str) -> str:
        """
        Returns the schema of a specific table in a human-readable format.

        Args:
            table_name: The name of the table to inspect.
        """
        inspector = inspect(self.engine)
        columns = inspector.get_columns(table_name)
        schema_description = f"Schema for table '{table_name}':\n"
        for col in columns:
            schema_description += f"  - {col['name']} ({col['type']})\n"
        return schema_description.strip()

    @tool
    def run_sql_query(self, query: str) -> str:
        """
        Executes a SQL query and returns the result.
        For SELECT statements, it returns a JSON string of the rows.
        For other statements (INSERT, UPDATE, DELETE), it returns a success message.

        Args:
            query: The SQL query to execute.
        """
        with self.engine.connect() as connection:
            trans = connection.begin()
            try:
                result = connection.execute(text(query))
                if result.returns_rows:
                    rows = [dict(row._mapping) for row in result.fetchall()]
                    trans.commit()
                    return json.dumps(rows, indent=2)
                else:
                    trans.commit()
                    return f"Query '{query}' executed successfully. {result.rowcount} rows affected."
            except Exception as e:
                trans.rollback()
                return f"Error executing query: {e}"



================================================
FILE: datapizza-ai-tools/SQLDatabase/tests/test_sql_database_tool.py
================================================
import json

import pytest

from datapizza.tools.SQLDatabase.base import SQLDatabase


@pytest.fixture
def db_tool() -> SQLDatabase:
    """Provides a SQLDatabase instance connected to an in-memory SQLite database."""
    db = SQLDatabase(db_uri="sqlite:///:memory:")
    setup_queries = [
        "CREATE TABLE users (id INTEGER PRIMARY KEY, name VARCHAR(50), age INTEGER);",
        "INSERT INTO users (id, name, age) VALUES (1, 'Alice', 30);",
        "INSERT INTO users (id, name, age) VALUES (2, 'Bob', 25);",
    ]
    for query in setup_queries:
        db.run_sql_query(query)
    return db


def test_list_tables(db_tool: SQLDatabase):
    """Tests that list_tables returns the correct table names."""
    tables = db_tool.list_tables()
    assert isinstance(tables, str)
    assert "users" in tables.split("\n")


def test_get_table_schema(db_tool: SQLDatabase):
    """Tests that get_table_schema returns the correct schema information."""
    schema = db_tool.get_table_schema("users")
    assert isinstance(schema, str)
    assert "Schema for table 'users':" in schema
    assert "id (INTEGER)" in schema
    assert "name (VARCHAR(50))" in schema
    assert "age (INTEGER)" in schema


def test_run_select_query(db_tool: SQLDatabase):
    """Tests that run executes a SELECT query and returns correct data."""
    result = db_tool.run_sql_query("SELECT name, age FROM users WHERE id = 1;")

    data = json.loads(result)

    assert isinstance(data, list)
    assert len(data) == 1
    assert data[0] == {"name": "Alice", "age": 30}


def test_run_insert_query(db_tool: SQLDatabase):
    """Tests that run executes an INSERT statement and data is added."""
    insert_query = "INSERT INTO users (id, name, age) VALUES (3, 'Charlie', 35);"
    result = db_tool.run_sql_query(insert_query)
    assert "1 rows affected" in result

    select_result = db_tool.run_sql_query("SELECT * FROM users WHERE id = 3;")
    data = json.loads(select_result)
    assert len(data) == 1
    assert data[0]["name"] == "Charlie"


def test_run_query_error(db_tool: SQLDatabase):
    """Tests that a malformed query returns an error message."""
    result = db_tool.run_sql_query("SELECT * FROM non_existent_table;")
    assert isinstance(result, str)
    assert "Error executing query:" in result



================================================
FILE: datapizza-ai-tools/web_fetch/README.md
================================================
<div align="center">
<img src="https://github.com/datapizza-labs/datapizza-ai/raw/main/docs/assets/logo_bg_dark.png" alt="Datapizza AI Logo" width="200" height="200">

# Datapizza AI - Web Fetch Tool

**A tool for Datapizza AI that allows agents to fetch and process content from web pages.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

</div>

---

This tool provides a simple and effective way for `datapizza-ai` agents to access information from the internet. Agents equipped with this tool can be instructed to fetch the content of a URL, which they can then use for summarization, data extraction, or to answer questions.

## âš™ï¸ How it Works

The `WebFetchTool` is a callable class that, once instantiated, can be passed directly to an agent's tool list. The agent will invoke the tool using its registered name, `web_fetch`. It uses the `httpx` library to make a GET request to the given URL and returns the content as a string.

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install the core framework
pip install datapizza-ai

# Install the WebFetch tool
pip install datapizza-ai-tools-web-fetch
```

### 2. Example: Creating a Web Research Agent

In this example, we'll create an agent that can summarize the content of a web page.

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.web_fetch import WebFetchTool

# 1. Initialize a client (e.g., OpenAI)
client = OpenAIClient(api_key="YOUR_API_KEY")

# 2. Initialize the WebFetchTool
web_tool = WebFetchTool()

# 3. Create an agent and provide it with the web fetch tool
agent = Agent(
    name="WebFetchAgent",
    client=client,
    system_prompt="""You are a helpful research assistant.
Your goal is to answer user questions by fetching information from web pages.

Follow these steps:
1.  Receive a user question that includes a URL.
2.  Use the `web_fetch` tool to get the content of the URL.
3.  Analyze the content and provide a concise summary or answer to the user's question.
""",
    tools=[web_tool]
)

# 4. Run the agent to answer a question
question = "Summarize the main points of the article at https://loremipsum.io/"
print(f"--- Running agent for: '{question}' ---")
response = agent.run(question)
print(f"Agent Response: {response.text}\n")

# Example with a different website
# For this example, we'll stick to example.com to ensure it runs.
question_2 = "What is the title of the page at http://example.com?"
print(f"--- Running agent for: '{question_2}' ---")
response_2 = agent.run(question_2)
print(f"Agent Response: {response_2.text}\n")

```

### Expected Output

The output will vary depending on the live content of the URL. For `https://loremipsum.io/`, it might look something like this:

```
--- Running agent for: 'Summarize the main points of the article at https://loremipsum.io/' ---
Agent Response: The article on **loremipsum.io** provides a comprehensive overview of "Lorem Ipsum," which is a placeholder text commonly used in the graphic, print, and publishing industries. Here are the main points:
```



================================================
FILE: datapizza-ai-tools/web_fetch/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-tools-web-fetch"
version = "0.0.2"
description = "A tool for fetching web content in the Datapizza AI framework."
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Nilthon Jhon Rojas Apumayta", email = "oneill.jhon97@gmail.com"}
]
requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "datapizza-ai-core>=0.0.1,<0.1.0",
    "httpx>=0.28.1",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
    "pytest-httpx>=0.32.0"
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["/datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["/datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-tools/web_fetch/datapizza/tools/web_fetch/__init__.py
================================================
from .base import WebFetchTool

__all__ = ["WebFetchTool"]



================================================
FILE: datapizza-ai-tools/web_fetch/datapizza/tools/web_fetch/base.py
================================================
import httpx

from datapizza.tools import Tool


class WebFetchTool(Tool):
    """
    The Web Fetch tool.
    It allows you to fetch the content of a given URL with configurable timeouts
    and specific error handling.
    """

    DEFAULT_TIMEOUT = 10.0
    DEFAULT_USER_AGENT = "DataPizza-AI-Tool/1.0"

    def __init__(self, timeout: float | None = None, user_agent: str | None = None):
        """Initializes the WebFetchTool.

        Args:
            timeout: The timeout for the request in seconds.
            user_agent: The User-Agent header to use for the request.
        """
        super().__init__(
            name="web_fetch",
            description="Fetches the content of a given URL.",
            func=self.__call__,
        )
        self.timeout = timeout if timeout is not None else self.DEFAULT_TIMEOUT
        self.user_agent = (
            user_agent if user_agent is not None else self.DEFAULT_USER_AGENT
        )

    def __call__(self, url: str) -> str:
        """Invoke the tool."""
        headers = {"User-Agent": self.user_agent}

        try:
            with httpx.Client(headers=headers) as client:
                response = client.get(url, timeout=self.timeout)
                response.raise_for_status()
                return response.text
        except httpx.TimeoutException as e:
            return f"Request timed out while requesting {e.request.url!r}."
        except httpx.RequestError as e:
            return f"An error occurred while requesting {e.request.url!r}: {e}"
        except httpx.HTTPStatusError as e:
            status_code = e.response.status_code
            url_str = str(e.request.url)
            if status_code == 404:
                return f"Error: Resource not found at {url_str!r} (404)."
            elif status_code in (401, 403):
                return f"Error: Not authorized to access {url_str!r} ({status_code})."
            elif 400 <= status_code < 500:
                return (
                    f"Error: Client error {status_code} while requesting {url_str!r}."
                )
            elif 500 <= status_code < 600:
                return (
                    f"Error: Server error {status_code} while requesting {url_str!r}."
                )
            else:
                return f"Error response {status_code} while requesting {url_str!r}."



================================================
FILE: datapizza-ai-tools/web_fetch/tests/test_web_fetch.py
================================================
import httpx
import pytest
from httpx import Request

from datapizza.tools.web_fetch import WebFetchTool


@pytest.fixture
def tool():
    """Provides a WebFetchTool instance."""
    return WebFetchTool()


def test_web_fetch_tool_success(tool, httpx_mock):
    """Test a successful web fetch."""
    url = "https://example.com"
    httpx_mock.add_response(url=url, text="Hello, world!")

    result = tool(url)

    assert result == "Hello, world!"
    request = httpx_mock.get_requests()[0]
    assert "user-agent" in request.headers


def test_web_fetch_tool_timeout(tool, httpx_mock):
    """Test the tool's timeout handling."""
    url = "https://example.com/timeout"
    httpx_mock.add_exception(
        httpx.TimeoutException("Timeout!", request=Request("GET", url))
    )

    result = tool(url)

    assert "Request timed out" in result


def test_web_fetch_tool_not_found_error(tool, httpx_mock):
    """Test the 404 Not Found error handling."""
    url = "https://example.com/404"
    httpx_mock.add_response(url=url, status_code=404)

    result = tool(url)

    assert "Resource not found" in result


def test_web_fetch_tool_server_error(tool, httpx_mock):
    """Test a 500 server error."""
    url = "https://example.com/server-error"
    httpx_mock.add_response(url=url, status_code=500)

    result = tool(url)

    assert "Server error 500" in result


def test_web_fetch_tool_request_error(tool, httpx_mock):
    """Test a generic request error."""
    url = "https://example.com/request-error"
    httpx_mock.add_exception(
        httpx.RequestError("Some error", request=Request("GET", url))
    )

    result = tool(url)

    assert "An error occurred" in result


def test_web_fetch_tool_custom_user_agent(httpx_mock):
    """Test that a custom user agent is correctly passed."""
    url = "https://example.com/custom-ua"
    custom_ua = "MyTestAgent/1.0"
    tool = WebFetchTool(user_agent=custom_ua)
    httpx_mock.add_response(url=url, text="Success")

    tool(url)

    request = httpx_mock.get_requests()[0]
    assert request.headers["user-agent"] == custom_ua


def test_web_fetch_tool_custom_timeout(httpx_mock):
    """Test that a custom timeout is correctly passed."""
    url = "https://example.com/custom-timeout"
    custom_timeout = 5.0
    tool = WebFetchTool(timeout=custom_timeout)
    httpx_mock.add_response(url=url, text="Success")

    tool(url)

    request = httpx_mock.get_requests()[0]
    assert request.extensions["timeout"]["connect"] == custom_timeout



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus/README.md
================================================

Milvus implementation for datapizza-ai framework



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-vectorstores-milvus"
version = "0.0.1"
description = "Milvus vectorstore for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]
dependencies = [
    "datapizza-ai-core>=0.0.6,<0.1.0",
    "pymilvus>=2.5.4",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus/datapizza/vectorstores/milvus/__init__.py
================================================
from .milvus_vectorstore import MilvusVectorstore

__all__ = ["MilvusVectorstore"]



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus/datapizza/vectorstores/milvus/milvus_vectorstore.py
================================================
import logging
from collections.abc import Generator
from typing import Any

from datapizza.core.vectorstore import VectorConfig, Vectorstore
from datapizza.type import (
    Chunk,
    DenseEmbedding,
    Embedding,
    EmbeddingFormat,
    SparseEmbedding,
)
from pymilvus import (
    AsyncMilvusClient,
    CollectionSchema,
    DataType,
    FieldSchema,
    MilvusClient,
    MilvusException,
)
from pymilvus.milvus_client.index import IndexParams

log = logging.getLogger(__name__)


class MilvusVectorstore(Vectorstore):
    """
    Milvus Vectorstore
    """

    def __init__(
        self,
        # You can pass either `uri="http://localhost:19530"` | "./milvus.db" (Milvus Lite)
        # or host/port(+secure/user/password) via **connection_args for flexibility.
        uri: str | None = None,
        host: str | None = None,
        port: int | None = None,
        user: str | None = None,
        password: str | None = None,
        secure: bool | None = None,
        **connection_args: Any,
    ):
        self.conn_kwargs: dict[str, Any] = {}
        if uri:
            self.conn_kwargs["uri"] = uri
        if host:
            self.conn_kwargs["host"] = host
        if port:
            self.conn_kwargs["port"] = port
        if user:
            self.conn_kwargs["user"] = user
        if password:
            self.conn_kwargs["password"] = password
        if secure is not None:
            self.conn_kwargs["secure"] = secure

        # Allow extra MilvusClient kwargs (e.g. token for Zilliz)
        self.conn_kwargs.update(connection_args or {})
        self.client: MilvusClient
        self.a_client: AsyncMilvusClient
        self.batch_size: int = 100

    def get_client(self) -> MilvusClient:
        if not hasattr(self, "client"):
            self._init_client()
        return self.client

    def _get_a_client(self) -> AsyncMilvusClient:
        if not hasattr(self, "a_client"):
            self._init_a_client()
        return self.a_client

    def _init_client(self):
        self.client = MilvusClient(**self.conn_kwargs)

    def _init_a_client(self):
        self.a_client = AsyncMilvusClient(**self.conn_kwargs)

    @staticmethod
    def _chunk_to_row(chunk: Chunk) -> dict[str, Any]:
        def _sparse_to_dict(se: SparseEmbedding) -> dict[int, float]:
            return {
                int(i): float(v) for i, v in zip(se.indices, se.values, strict=False)
            }

        if not chunk.embeddings:
            raise ValueError("Chunk must have an embedding")

        row: dict[str, Any] = {
            "id": chunk.id,
            "text": chunk.text or "",
        }
        if chunk.metadata:
            row.update(chunk.metadata)

        if len(chunk.embeddings) == 1:
            e = chunk.embeddings[0]
            if isinstance(e, DenseEmbedding):
                row[e.name] = e.vector
            elif isinstance(e, SparseEmbedding):
                row[e.name] = _sparse_to_dict(e)
            else:
                raise ValueError(f"Unsupported embedding type: {type(e)}")
        else:
            for e in chunk.embeddings:
                if isinstance(e, DenseEmbedding):
                    fname = e.name
                    row[fname] = e.vector
                elif isinstance(e, SparseEmbedding):
                    fname = e.name
                    row[fname] = _sparse_to_dict(e)
                else:
                    raise ValueError(f"Unsupported embedding type: {type(e)}")
        return row

    @staticmethod
    def _entity_to_chunk(entity: dict[str, Any]) -> Chunk:
        """
        Convert a Milvus entity dict into a Chunk.
        - Dense vectors: list[float] -> DenseEmbedding(name=<field>)
        - Sparse vectors: dict[index->value] -> SparseEmbedding(name=<field>)
        - Everything else (except id/text) -> metadata
        """

        embeddings: list[Embedding] = []
        metadata: dict[str, Any] = {}

        for key, val in entity.items():
            if key in {"id", "text"}:
                continue

            if isinstance(val, list) and all(isinstance(x, float) for x in val):
                embeddings.append(DenseEmbedding(name=key, vector=list(val)))
                continue

            if isinstance(val, dict) and val:
                try:
                    items = sorted(
                        ((int(k), float(v)) for k, v in val.items()), key=lambda t: t[0]
                    )
                    indices = [i for i, _ in items]
                    values = [v for _, v in items]
                    embeddings.append(
                        SparseEmbedding(name=key, indices=indices, values=values)
                    )
                    continue
                except Exception:
                    # fall through to metadata if it isn't a proper sparse map
                    pass

            # Non-vector -> metadata
            metadata[key] = val

        return Chunk(
            id=entity["id"],
            text=entity["text"],
            embeddings=embeddings,
            metadata=metadata,
        )

    @staticmethod
    def _metric_from_config(cfg: VectorConfig) -> str:
        """Map your VectorConfig.distance to Milvus metric string."""
        v = (getattr(cfg.distance, "value", str(cfg.distance)) or "").upper()
        if "COS" in v:
            return "COSINE"
        return "L2"

    @staticmethod
    def _sparse_embedding_to_milvus_format(
        sparse_emb: SparseEmbedding,
    ) -> dict[int, float]:
        """
        Convert a SparseEmbedding instance to a Milvus-supported sparse vector format (dict).

        Milvus accepts: {dimension_index: value, ...}
        """
        return dict(zip(sparse_emb.indices, sparse_emb.values, strict=False))

    def add(
        self,
        chunk: Chunk | list[Chunk],
        collection_name: str | None = None,
        batch_size: int = 100,
    ):
        client = self.get_client()

        chunks = chunk if isinstance(chunk, list) else [chunk]
        total = len(chunks)

        try:
            for i in range(0, total, batch_size):
                batch = chunks[i : i + batch_size]
                rows = [self._chunk_to_row(c) for c in batch]
                client.insert(collection_name=collection_name, data=rows)

        except MilvusException as e:
            log.error(f"Failed to batch insert into '{collection_name}': {e!s}")
            raise

    async def a_add(
        self,
        chunk: Chunk | list[Chunk],
        collection_name: str | None = None,
        batch_size: int = 100,
    ):
        client = self._get_a_client()

        chunks = chunk if isinstance(chunk, list) else [chunk]
        total = len(chunks)

        try:
            for i in range(0, total, batch_size):
                batch = chunks[i : i + batch_size]
                rows = [self._chunk_to_row(c) for c in batch]
                await client.insert(collection_name=collection_name, data=rows)

        except MilvusException as e:
            log.error(f"Failed to batch insert into '{collection_name}': {e!s}")
            raise

    def retrieve(
        self,
        collection_name: str,
        ids: list[str],
        **kwargs,
    ) -> list[Chunk]:
        """
        Retrieve chunks from a collection by their IDs.

        Args:
            collection_name (str): Name of the collection to query.
            ids (list[str]): IDs to retrieve.
            **kwargs: Extra arguments to Milvus client query().

        Returns:
            list[Chunk]: Retrieved chunks.
        """

        client = self.get_client()
        try:
            res = client.query(
                collection_name=collection_name,
                ids=ids,
                **kwargs,
            )
        except Exception as e:
            log.error(f"Failed to retrieve from collection '{collection_name}': {e!s}")
            raise

        return [self._entity_to_chunk(r) for r in (res or [])]

    def remove(self, collection_name: str, ids: list[str], **kwargs):
        client = self.get_client()
        client.delete(collection_name=collection_name, ids=ids, **kwargs)

    def update(self, collection_name: str, chunk: Chunk | list[Chunk], **kwargs):
        """
        Upsert one or more Chunk objects into a Milvus collection.

        In Milvus, an upsert operation combines both insert and update behavior:
          - If an entity with the same primary key already exists in the collection,
            it will be overwritten with the new data.
          - If the primary key does not exist, a new entity will be inserted.

        Args:
            collection_name (str): Name of the Milvus collection.
            chunk (Chunk | list[Chunk]): One or more Chunk objects to upsert.
            **kwargs: Additional parameters passed to the Milvus client.

        """
        client = self.get_client()
        chunks = [chunk] if isinstance(chunk, Chunk) else chunk
        data = [self._chunk_to_row(c) for c in chunks]
        client.upsert(collection_name=collection_name, data=data, **kwargs)
        client.flush(collection_name=collection_name)

    @staticmethod
    def _merge_output_fields(user_fields: list[str] | None) -> list[str]:
        required = ["id", "text"]
        if user_fields is None:
            return required[:]
        if not isinstance(user_fields, list) or not all(
            isinstance(f, str) for f in user_fields
        ):
            raise TypeError("output_fields must be a list[str]")
        # ensure required fields are included
        return required[:] + [f for f in user_fields if f not in required]

    @staticmethod
    def _is_multi_query(vec) -> bool:
        return (
            isinstance(vec, list)
            and vec
            and isinstance(vec[0], (list, DenseEmbedding, SparseEmbedding))
        )

    def _normalize_query(
        self,
        v: list[float] | DenseEmbedding | SparseEmbedding,
        vector_name: str | None,
    ) -> tuple[list[float] | dict[int, float], str | None]:
        if isinstance(v, (DenseEmbedding, SparseEmbedding)) and not vector_name:
            vector_name = v.name
        if isinstance(v, SparseEmbedding):
            return self._sparse_embedding_to_milvus_format(v), vector_name
        if isinstance(v, DenseEmbedding):
            return v.vector, vector_name
        if not isinstance(v, list) or (v and not isinstance(v[0], (int, float))):
            raise TypeError(
                "query_vector must be list[float], DenseEmbedding, or SparseEmbedding"
            )
        return v, vector_name

    def _prepare_search_args(
        self,
        *,
        query_vector: list[float] | DenseEmbedding | SparseEmbedding,
        vector_name: str | None,
        k: int,
        kwargs: dict,
    ) -> dict:
        # guard against accidental multi-vector input
        if self._is_multi_query(query_vector):
            raise TypeError(
                "Single-vector search only: got a list of vectors. Pass exactly one vector."
            )

        # normalize
        vector, vector_name = self._normalize_query(query_vector, vector_name)
        if not vector_name:
            raise ValueError(
                "vector_name must be provided (or embedded in the Embedding.name)."
            )

        # fields
        user_fields = kwargs.pop("output_fields", None)
        # ensure at least id and text are included
        output_fields = self._merge_output_fields(user_fields)

        # final param bundle for Milvus .search()
        return {
            "data": [vector],
            "anns_field": vector_name,
            "limit": k,
            "output_fields": output_fields,
            **kwargs,
        }

    def _chunks_from_results(self, res) -> list[Chunk]:
        hits = res[0] if res else []
        out: list[Chunk] = []
        for h in hits:
            entity = h.get("entity", None)
            if entity is None:
                entity = {k: v for k, v in h.items() if k not in {"score", "distance"}}
            out.append(self._entity_to_chunk(entity))
        return out

    def search(
        self,
        collection_name: str,
        query_vector: list[float] | DenseEmbedding | SparseEmbedding,
        k: int = 10,
        vector_name: str | None = None,
        **kwargs,
    ) -> list[Chunk]:
        """
        Perform a single-vector similarity search on a Milvus collection.

        Args:
            collection_name (str): Name of the Milvus collection to search.
            query_vector (list[float] | DenseEmbedding | SparseEmbedding):
                The query vector or embedding to search for.
            k (int, optional): Number of nearest results to return. Defaults to 10.
            vector_name (str, optional): Name of the vector field to search on.
                If not provided, inferred from the embedding (if applicable).
            **kwargs: Additional parameters passed directly to the Milvus client's
                `search()` method (e.g., filter expressions, consistency level).

        Returns:
            list[Chunk]: A list of retrieved `Chunk` objects representing the top
            `k` most similar results.
        """
        client = self.get_client()
        params = self._prepare_search_args(
            query_vector=query_vector,
            vector_name=vector_name,
            k=k,
            kwargs=kwargs,
        )
        res = client.search(collection_name=collection_name, **params)
        return self._chunks_from_results(res)

    # ---------- async API ----------

    async def a_search(
        self,
        collection_name: str,
        query_vector: list[float] | DenseEmbedding | SparseEmbedding,
        k: int = 10,
        vector_name: str | None = None,
        **kwargs,
    ) -> list[Chunk]:
        """
        Perform an asynchronous single-vector similarity search on a Milvus collection.

        Args:
            collection_name (str): Name of the Milvus collection to search.
            query_vector (list[float] | DenseEmbedding | SparseEmbedding):
                The query vector or embedding to search for.
            k (int, optional): Number of nearest results to return. Defaults to 10.
            vector_name (str, optional): Name of the vector field to search on.
                If not provided, inferred from the embedding (if applicable).
            **kwargs: Additional parameters passed directly to the Milvus client's
                `search()` method (e.g., filter expressions, consistency level).

        Returns:
            list[Chunk]: A list of retrieved `Chunk` objects representing the top
            `k` most similar results.
        """
        client = self._get_a_client()
        params = self._prepare_search_args(
            query_vector=query_vector,
            vector_name=vector_name,
            k=k,
            kwargs=kwargs,
        )
        res = await client.search(collection_name=collection_name, **params)
        return self._chunks_from_results(res)

    def get_collections(self):
        client = self.get_client()
        return client.list_collections()

    def create_collection(
        self,
        collection_name: str,
        vector_config: list[VectorConfig],
        index_params: IndexParams = None,
        **kwargs,
    ):
        """
        Create a collection with:
          - id (VARCHAR primary key), text (VARCHAR)
          - one or more vector fields from `vector_config`
          - dynamic fields enabled (so `chunk.metadata` will be stored in $meta)
          - per-field indexes with metrics derived from `VectorConfig.distance`
        """

        client = self.get_client()
        if client.has_collection(collection_name):
            log.warning(
                f"Collection {collection_name} already exists, skipping creation"
            )
            return

        fields: list[FieldSchema] = [
            # Fixed 36-char UUIDv4 string (includes hyphens), used as primary key
            FieldSchema(
                name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=36
            ),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        ]

        # Build vector fields
        vector_names = set()
        for cfg in vector_config:
            if cfg.format == EmbeddingFormat.DENSE:
                fname = cfg.name
                if fname in vector_names:
                    raise ValueError(
                        "Each VectorConfig must have a unique 'name' for multi-vector collections"
                    )
                vector_names.add(fname)
                fields.append(
                    FieldSchema(
                        name=fname, dtype=DataType.FLOAT_VECTOR, dim=cfg.dimensions
                    )
                )
            elif cfg.format == EmbeddingFormat.SPARSE:
                fname = cfg.name
                if fname in vector_names:
                    raise ValueError(
                        "Each VectorConfig must have a unique 'name' for multi-vector collections"
                    )
                vector_names.add(fname)
                fields.append(
                    FieldSchema(
                        name=fname,
                        dtype=DataType.SPARSE_FLOAT_VECTOR,
                    )
                )
            else:
                raise ValueError(f"Unsupported embedding format: {cfg.format}")

        # Create the schema for the collection
        # TODO: add support for additional kwargs
        #  to pass to schema e.g. partition key, Milvus built in functions
        schema = CollectionSchema(
            fields,
            # default true for flexibility, Chunk's dynamic metadata will be stored into $meta
            # https://milvus.io/docs/enable-dynamic-field.md#Dynamic-Field
            enable_dynamic_field=True,
        )

        idx_params = client.prepare_index_params()
        if index_params is not None:
            idx_params = index_params
        # Add indexes
        if index_params is None:
            for cfg in vector_config:
                if cfg.format == EmbeddingFormat.DENSE:
                    fname = cfg.name
                    default_index_params = {
                        "field_name": fname,
                        "index_type": "AUTOINDEX",
                        "metric_type": self._metric_from_config(cfg),
                        "params": {},
                    }
                    idx_params.add_index(**default_index_params)
                elif cfg.format == EmbeddingFormat.SPARSE:
                    fname = cfg.name
                    default_index_params = {
                        "field_name": fname,
                        "index_type": "SPARSE_INVERTED_INDEX",
                        "metric_type": "IP",  # cosine not supported
                        "params": {},
                    }
                    idx_params.add_index(**default_index_params)

        # Create Collection
        client.create_collection(
            collection_name=collection_name,
            schema=schema,
            index_params=idx_params,
            **kwargs,
        )
        # load the collection by default
        client.load_collection(collection_name)

    def delete_collection(self, collection_name: str, **kwargs):
        client = self.get_client()
        if client.has_collection(collection_name):
            client.drop_collection(collection_name, **kwargs)

    def dump_collection(
        self,
        collection_name: str,
        page_size: int = 100,
    ) -> Generator[Chunk, None, None]:
        """
        Iterate all rows via offset paging.
        """
        client = self.get_client()
        offset = 0
        while True:
            res = client.query(
                collection_name=collection_name,
                filter="",
                output_fields=["*"],
                limit=page_size,
                offset=offset,
            )
            if not res:
                break
            for r in res:
                yield self._entity_to_chunk(r)
            if len(res) < page_size:
                break
            offset += page_size

    def prepare_index_params(self, **kwargs) -> IndexParams:
        client = self.get_client()
        return client.prepare_index_params(**kwargs)



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-milvus/datapizza/vectorstores/milvus/tests/test_milvus_vectorstore.py
================================================
import contextlib
import uuid

import pytest
from datapizza.core.vectorstore import VectorConfig
from datapizza.type import (
    Chunk,
    EmbeddingFormat,
    SparseEmbedding,
)

from datapizza.vectorstores.milvus import MilvusVectorstore

MILVUS_URI = "./milvus.db"


def unique_coll(prefix="itest"):
    return f"{prefix}_{uuid.uuid4().hex[:8]}"


def make_chunk(cid: str, text: str = "hello", meta: bool = True) -> Chunk:
    metadata = {"source": "itest", "idx": cid} if meta else None
    return Chunk(
        id=cid,
        text=text,
        embeddings=[
            SparseEmbedding(
                name="sparse_vec",
                indices=[1, 5, 9],
                values=[0.5, 0.2, 0.1],
            )
        ],
        metadata=metadata,
    )


@pytest.fixture(scope="session")
def store():
    s = MilvusVectorstore(uri=MILVUS_URI)
    try:
        client = s.get_client()
        client.list_collections()
    except Exception as e:
        pytest.skip(f"Cannot connect to Milvus at {MILVUS_URI}: {e}")
    return s


@pytest.fixture
def vector_cfg():
    return [
        VectorConfig(
            name="sparse_vec",
            format=EmbeddingFormat.SPARSE,
            dimensions=1536,
            distance="Cosine",
        )
    ]


@pytest.fixture
def collection(store, vector_cfg):
    name = unique_coll()
    store.create_collection(
        collection_name=name,
        vector_config=vector_cfg,
    )
    yield name
    # Cleanup
    with contextlib.suppress(Exception):
        store.delete_collection(name)


def test_metric_from_config_sparse_cosine(store, vector_cfg):
    assert store._metric_from_config(vector_cfg[0]) == "COSINE"


def test_create_and_list_collections(store, vector_cfg):
    name = unique_coll()
    try:
        store.create_collection(name, vector_cfg)
        cols = store.get_collections()
        assert name in cols
    finally:
        store.delete_collection(name)


def test_add_and_retrieve_roundtrip(store, collection):
    c = make_chunk("id-1", text="alpha")
    store.add(c, collection_name=collection)
    # flush collection
    client = store.get_client()
    client.flush(collection_name=collection)
    out = store.retrieve(collection, ids=["id-1"])
    assert len(out) == 1
    got = out[0]
    assert got.id == "id-1"
    assert got.text == "alpha"
    assert got.metadata.get("source") == "itest"
    emb_names = {e.name for e in got.embeddings}
    assert "sparse_vec" in emb_names


def test_batch_add(store, collection):
    batch = [make_chunk(f"id-{i}") for i in range(6)]
    store.add(batch, collection_name=collection, batch_size=2)
    # flush collection
    client = store.get_client()
    client.flush(collection_name=collection)
    out = store.retrieve(collection, ids=[f"id-{i}" for i in range(6)])
    assert len(out) == 6


def test_update_upsert(store, collection):
    store.add(make_chunk("u1", text="v1"), collection_name=collection)
    store.update(collection, make_chunk("u1", text="v2"))
    got = store.retrieve(collection, ids=["u1"])[0]
    assert got.text == "v2"


def test_remove(store, collection):
    store.add(make_chunk("del-1"), collection_name=collection)
    store.remove(collection, ids=["del-1"])
    # flush collection
    client = store.get_client()
    client.flush(collection_name=collection)
    out = store.retrieve(collection, ids=["del-1"])
    assert out == []


def test_search_single_sparse(store, collection):
    store.add([make_chunk(f"s{i}") for i in range(5)], collection_name=collection)
    # flush collection
    client = store.get_client()
    client.flush(collection_name=collection)
    q = SparseEmbedding(name="sparse_vec", indices=[1, 2], values=[0.3, 0.7])
    res = store.search(collection, q, k=3)
    assert isinstance(res, list)
    assert len(res) == 3
    from datapizza.type import Chunk as _C

    assert all(isinstance(c, _C) for c in res)


def test_dump_collection_paged(store, collection):
    for i in range(7):
        store.add(make_chunk(f"d{i}"), collection_name=collection)
    # flush collection
    client = store.get_client()
    client.flush(collection_name=collection)
    dumped = list(store.dump_collection(collection, page_size=3))
    assert len(dumped) >= 7
    ids = {c.id for c in dumped}
    assert "d0" in ids and "d6" in ids



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-qdrant/README.md
================================================

Qdrant implementation for datapizza-ai framework



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-qdrant/pyproject.toml
================================================
# Build system configuration
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Project metadata
[project]
name = "datapizza-ai-vectorstores-qdrant"
version = "0.0.7"
description = "Qdrant vectorstore for the datapizza-ai framework"
readme = "README.md"
license = {text = "MIT"}

requires-python = ">=3.10.0,<4"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]
dependencies = [
    "datapizza-ai-core>=0.0.6,<0.1.0",
    "qdrant-client>=1.16.0,<2.0.0",
]

# Development dependencies
[dependency-groups]
dev = [
    "deptry>=0.23.0",
    "pytest",
    "ruff>=0.11.5",
]

# Hatch build configuration
[tool.hatch.build.targets.sdist]
include = ["datapizza"]
exclude = ["**/BUILD"]

[tool.hatch.build.targets.wheel]
include = ["datapizza"]
exclude = ["**/BUILD"]

# Ruff configuration
[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    # "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "B",   # flake8-bugbear
    "I",   # isort
    "UP",  # pyupgrade
    "SIM", # flake8-simplify
    "RUF", # Ruff-specific rules
    "C4",  # flake8-comprehensions
]



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-qdrant/datapizza/vectorstores/qdrant/__init__.py
================================================
from .qdrant_vectorstore import QdrantVectorstore

__all__ = ["QdrantVectorstore"]



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-qdrant/datapizza/vectorstores/qdrant/qdrant_vectorstore.py
================================================
import logging
from collections.abc import Generator
from typing import Any

from datapizza.core.vectorstore import VectorConfig, Vectorstore
from datapizza.type import (
    Chunk,
    DenseEmbedding,
    Embedding,
    EmbeddingFormat,
    SparseEmbedding,
)
from pydantic.types import StrictStr
from qdrant_client import AsyncQdrantClient, QdrantClient, models

log = logging.getLogger(__name__)


class QdrantVectorstore(Vectorstore):
    """
    datapizza-ai implementation of a Qdrant vectorstore.
    """

    def __init__(
        self,
        host: str | None = None,
        port: int = 6333,
        api_key: str | None = None,
        **kwargs,
    ):
        """
        Initialize the QdrantVectorstore.

        Args:
            host (str, optional): The host to use for the Qdrant client. Defaults to None.
            port (int, optional): The port to use for the Qdrant client. Defaults to 6333.
            api_key (str, optional): The API key to use for the Qdrant client. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the Qdrant client.
        """
        if host is None and "location" not in kwargs:
            raise ValueError("Either host or location must be provided")

        self.client: QdrantClient
        self.a_client: AsyncQdrantClient
        self.batch_size: int = 100
        self.host: str | None = host
        self.port: int = port
        self.api_key: str | None = api_key
        self.kwargs: dict[str, Any] = kwargs

    def get_client(self) -> QdrantClient:
        if not hasattr(self, "client"):
            self._init_client()
        return self.client

    def _get_a_client(self) -> AsyncQdrantClient:
        if not hasattr(self, "a_client"):
            self._init_a_client()
        return self.a_client

    def _init_client(self):
        self.client = QdrantClient(
            host=self.host, port=self.port, api_key=self.api_key, **self.kwargs
        )

    def _init_a_client(self):
        self.a_client = AsyncQdrantClient(
            host=self.host, port=self.port, api_key=self.api_key, **self.kwargs
        )

    def add(self, chunk: Chunk | list[Chunk], collection_name: str | None = None):
        """Add a single chunk or list of chunks to the vectorstore.
        Args:
            chunk (Chunk | list[Chunk]): The chunk or list of chunks to add.
            collection_name (str, optional): The name of the collection to add the chunks to. Defaults to None.
        """
        client = self.get_client()

        if not collection_name:
            raise ValueError("Collection name must be set")

        chunks = [chunk] if isinstance(chunk, Chunk) else chunk
        points = []

        for chunk in chunks:
            points.append(self._process_chunk(chunk))

        # TODO: Process in batches
        for p in points:
            try:
                client.upsert(collection_name=collection_name, points=[p], wait=True)
            except Exception as e:
                log.error(f"Failed to add points to Qdrant: {e!s}")
                raise e

    async def a_add(
        self, chunk: Chunk | list[Chunk], collection_name: str | None = None
    ):
        client = self._get_a_client()

        if not collection_name:
            raise ValueError("Collection name must be set")

        chunks = [chunk] if isinstance(chunk, Chunk) else chunk
        points = []

        for chunk in chunks:
            points.append(self._process_chunk(chunk))

        # TODO: Process in batches
        for p in points:
            try:
                await client.upsert(
                    collection_name=collection_name, points=[p], wait=True
                )
            except Exception as e:
                log.error(f"Failed to add points to Qdrant: {e!s}")
                raise e

    def _process_chunk(self, chunk: Chunk) -> models.PointStruct:
        """Process a chunk into a Qdrant point."""
        if not chunk.embeddings:
            raise ValueError("Chunk must have an embedding")

        vector = {}
        for v in chunk.embeddings:
            if isinstance(v, DenseEmbedding):
                if v.name is None:
                    if len(chunk.embeddings) > 1:
                        raise ValueError(
                            "There is at least one unnamed vector, even though the chunk has more than one vector"
                        )
                    vector = v.vector
                else:
                    vector[v.name] = v.vector

            elif isinstance(v, SparseEmbedding):
                vector[v.name] = models.SparseVector(values=v.values, indices=v.indices)
            else:
                raise ValueError(f"Unsupported embedding type: {type(v)}")

        return models.PointStruct(
            id=str(chunk.id),
            payload={
                "text": chunk.text,
                **chunk.metadata,
            },
            vector=vector,  # type: ignore
        )

    def update(self, collection_name: str, payload: dict, points: list[int], **kwargs):
        client = self.get_client()
        client.overwrite_payload(
            collection_name=collection_name,
            payload=payload,
            points=points,  # type: ignore
            **kwargs,
        )

    def retrieve(self, collection_name: str, ids: list[str], **kwargs) -> list[Chunk]:
        """Retrieve chunks from a collection by their IDs.
        Args:
            collection_name (str): The name of the collection to retrieve the chunks from.
            ids (list[str]): The IDs of the chunks to retrieve.
            **kwargs: Additional keyword arguments to pass to the Qdrant client.
        Returns:
            list[Chunk]: The list of chunks retrieved from the collection.
        """
        client = self.get_client()
        return self._point_to_chunk(
            client.retrieve(
                collection_name=collection_name,
                ids=ids,
                **kwargs,
            )
        )

    def remove(self, collection_name: str, ids: list[str], **kwargs):
        """Remove chunks from a collection by their IDs.
        Args:
            collection_name (str): The name of the collection to remove the chunks from.
            ids (list[str]): The IDs of the chunks to remove.
            **kwargs: Additional keyword arguments to pass to the Qdrant client.
        """
        client = self.get_client()
        client.delete(
            collection_name=collection_name,
            points_selector=models.PointIdsList(
                points=ids,  # type: ignore
            ),
            **kwargs,
        )

    def search(
        self,
        collection_name: str,
        query_vector: list[float] | SparseEmbedding | dict,
        k: int = 10,
        vector_name: str | None = None,
        **kwargs,
    ) -> list[Chunk]:
        """
        Search for chunks in a collection by their query vector.

        Args:
            collection_name (str): The name of the collection to search in.
            query_vector (list[float]): The query vector to search for.
            k (int, optional): The number of results to return. Defaults to 10.
            vector_name (str, optional): The name of the vector to search for. Defaults to None.
            **kwargs: Additional keyword arguments to pass to the Qdrant client.

        Returns:
            list[Chunk]: The list of chunks found in the collection.
        """
        client = self.get_client()
        using = None

        if isinstance(query_vector, list) and all(
            isinstance(v, float) for v in query_vector
        ):
            if not vector_name:
                collection = client.get_collection(collection_name)
                vectors = collection.config.params.vectors
                if vectors and isinstance(vectors, dict):
                    vectors_config = list[StrictStr](vectors)
                    if vectors_config and len(vectors_config) > 1:
                        raise ValueError(
                            f"Vector name not specified and multiple dense vectors are configured. Available vector names: {vectors_config}"
                        )
                    vector_name = str(vectors_config[0])
            using = vector_name
            qry = query_vector

        elif isinstance(query_vector, dict):
            indices = query_vector.get("indices", [])
            values = query_vector.get("values", [])
            qry = (models.SparseVector(indices=indices, values=values),)
            using = vector_name or "default"

        elif isinstance(query_vector, SparseEmbedding):
            using = query_vector.name
            qry = models.SparseVector(
                indices=query_vector.indices, values=query_vector.values
            )
        else:
            raise ValueError(f"Unsupported query vector type: {type(query_vector)}")

        hits = client.query_points(
            collection_name=collection_name,
            query=qry,
            using=using,
            limit=k,  # Return k closest points
            **kwargs,
        )
        return self._point_to_chunk(hits.points)

    async def a_search(
        self,
        collection_name: str,
        query_vector: list[float],
        k: int = 10,
        vector_name: str | None = None,
        **kwargs,
    ) -> list[Chunk]:
        """Search for chunks in a collection by their query vector."""
        client = self._get_a_client()
        using = None

        if isinstance(query_vector, list) and all(
            isinstance(v, float) for v in query_vector
        ):
            if not vector_name:
                collection = await client.get_collection(collection_name)
                vectors = collection.config.params.vectors
                if vectors and isinstance(vectors, dict):
                    vectors_config = list[StrictStr](vectors)
                    if vectors_config and len(vectors_config) > 1:
                        raise ValueError(
                            f"Vector name not specified and multiple dense vectors are configured. Available vector names: {vectors_config}"
                        )
                    vector_name = str(vectors_config[0])
            using = vector_name
            qry = query_vector

        elif isinstance(query_vector, dict):
            indices = query_vector.get("indices", [])
            values = query_vector.get("values", [])
            qry = (models.SparseVector(indices=indices, values=values),)
            using = vector_name or "default"

        elif isinstance(query_vector, SparseEmbedding):
            using = query_vector.name
            qry = models.SparseVector(
                indices=query_vector.indices, values=query_vector.values
            )
        else:
            raise ValueError(f"Unsupported query vector type: {type(query_vector)}")

        hits = await client.query_points(
            collection_name=collection_name,
            query=qry,
            using=using,
            limit=k,  # Return k closest points
            **kwargs,
        )
        return self._point_to_chunk(hits.points)

    def get_collections(self):
        """Get all collections in Qdrant."""
        client = self.get_client()
        return client.get_collections()

    async def a_get_collections(self):
        """Get all collections in Qdrant."""
        client = self._get_a_client()
        return await client.get_collections()

    def create_collection(
        self, collection_name: str, vector_config: list[VectorConfig], **kwargs
    ):
        """Create a new collection in Qdrant if it doesn't exist with the specified vector configurations

        Args:
            collection_name: Name of the collection to create
            vector_config: List of vector configurations specifying dimensions and distance metrics
            **kwargs: Additional arguments to pass to Qdrant's create_collection
        """

        client = self.get_client()

        if client.collection_exists(collection_name):
            log.warning(
                f"Collection {collection_name} already exists, skipping creation"
            )
            return

        sparse_config: (
            dict[str, models.SparseVectorParams] | models.SparseVectorParams | None
        ) = None
        config = None
        try:
            config = {
                v.name: models.VectorParams(
                    size=v.dimensions,  # type: ignore
                    distance=v.distance.value,  # type: ignore
                )
                for v in vector_config
                if v.format == EmbeddingFormat.DENSE
            }
            sparse_config = {
                v.name: models.SparseVectorParams()
                for v in vector_config
                if v.format == EmbeddingFormat.SPARSE
            }

            client.create_collection(
                collection_name=collection_name,
                vectors_config=config,
                sparse_vectors_config=sparse_config,
                **kwargs,
            )
        except Exception as e:
            log.error(f"Failed to create collection {collection_name}: {e!s}")
            raise e

    def delete_collection(self, collection_name: str, **kwargs):
        """Delete a collection in Qdrant."""
        client = self.get_client()
        client.delete_collection(collection_name=collection_name, **kwargs)

    def dump_collection(
        self,
        collection_name: str,
        page_size: int = 100,
        with_vectors: bool = False,
    ) -> Generator[Chunk, None, None]:
        """
        Dumps all points from a collection in a chunk-wise manner.

        Args:
            collection_name: Name of the collection to dump.
            page_size: Number of points to retrieve per batch.
            with_vectors: Whether to include vectors in the dumped chunks.

        Yields:
            Chunk: A chunk object from the collection.
        """
        client = self.get_client()
        next_page_offset = None

        while True:
            points, next_page_offset = client.scroll(
                collection_name=collection_name,
                limit=page_size,
                offset=next_page_offset,
                with_payload=True,
                with_vectors=with_vectors,
            )

            if not points:
                break

            yield from self._point_to_chunk(points)

            if next_page_offset is None:
                break

    def _point_to_chunk(self, points) -> list[Chunk]:
        """
        Convert Qdrant points to Chunk objects.

        Args:
            points: List of Qdrant point objects

        Returns:
            List of Chunk objects with appropriate embeddings
        """
        chunks = []

        for point in points:
            vector = point.vector
            embeddings: list[Embedding] = []

            # Handle dictionary of named vectors
            if isinstance(vector, dict):
                for name, vec in vector.items():
                    if isinstance(vec, models.SparseVector):
                        embeddings.append(
                            SparseEmbedding(
                                name=name,
                                values=vec.values,
                                indices=vec.indices,
                            )
                        )
                    elif isinstance(vec, list):
                        embeddings.append(DenseEmbedding(name=name, vector=vec))
            # Handle single dense vector (list)
            elif isinstance(vector, list):
                embeddings.append(DenseEmbedding(name="dense", vector=vector))
            # Handle single sparse vector
            elif isinstance(vector, models.SparseVector):
                embeddings.append(
                    SparseEmbedding(
                        name="sparse", values=vector.values, indices=vector.indices
                    )
                )
            elif vector is None:
                embeddings = []
            else:
                raise ValueError(f"Unsupported vector type: {type(vector)}")

            chunks.append(
                Chunk(
                    id=point.id,
                    metadata=point.payload,
                    text=point.payload["text"],
                    embeddings=embeddings,
                )
            )

        return chunks



================================================
FILE: datapizza-ai-vectorstores/datapizza-ai-vectorstores-qdrant/datapizza/vectorstores/qdrant/tests/test_qdrant_vectorstore.py
================================================
import uuid

import pytest
from datapizza.core.vectorstore import VectorConfig
from datapizza.type import EmbeddingFormat
from datapizza.type.type import Chunk, DenseEmbedding, SparseEmbedding

from datapizza.vectorstores.qdrant import QdrantVectorstore


@pytest.fixture
def vectorstore() -> QdrantVectorstore:
    vectorstore = QdrantVectorstore(location=":memory:")
    vectorstore.create_collection(
        collection_name="test",
        vector_config=[VectorConfig(dimensions=1536, name="dense_emb_name")],
    )

    return vectorstore


def test_qdrant_vectorstore_init():
    vectorstore = QdrantVectorstore(location=":memory:")
    assert vectorstore is not None


def test_qdrant_vectorstore_add(vectorstore):
    chunks = [
        Chunk(
            id=str(uuid.uuid4()),
            text="Hello world",
            embeddings=[DenseEmbedding(name="dense_emb_name", vector=[0.0] * 1536)],
        )
    ]
    vectorstore.add(chunks, collection_name="test")

    res = vectorstore.search(collection_name="test", query_vector=[0.0] * 1536)
    assert len(res) == 1

    res = vectorstore.search(
        collection_name="test", query_vector=[0.0] * 1536, vector_name="dense_emb_name"
    )
    assert len(res) == 1


def test_qdrant_vectorstore_create_collection(vectorstore):
    vectorstore.create_collection(
        collection_name="test2",
        vector_config=[VectorConfig(dimensions=1536, name="test2")],
    )

    colls = vectorstore.get_collections()

    assert len(colls.collections) == 2


def test_delete_collection(vectorstore):
    vectorstore.create_collection(
        collection_name="deleteme",
        vector_config=[VectorConfig(dimensions=1536, name="test2")],
    )

    colls = vectorstore.get_collections()
    assert len(colls.collections) == 2
    vectorstore.delete_collection(collection_name="deleteme")

    colls = vectorstore.get_collections()
    assert len(colls.collections) == 1


def test_qdrant_create_collection_with_sparse_vector(vectorstore):
    vectorstore.create_collection(
        collection_name="test3",
        vector_config=[
            VectorConfig(dimensions=1536, name="test3", format=EmbeddingFormat.SPARSE)
        ],
    )

    dense = vectorstore.get_client().get_collection("test3").config.params.vectors
    sparse = (
        vectorstore.get_client().get_collection("test3").config.params.sparse_vectors
    )
    assert sparse is not None
    assert len(sparse) == 1
    assert len(dense) == 0


def test_qdrant_search_sparse_vector(vectorstore):
    vectorstore.create_collection(
        collection_name="sparse_test",
        vector_config=[VectorConfig(name="sparse", format=EmbeddingFormat.SPARSE)],
    )

    vectorstore.add(
        chunk=[
            Chunk(
                id=str(uuid.uuid4()),
                text="Hello world",
                embeddings=[SparseEmbedding(name="sparse", values=[0.1], indices=[1])],
            )
        ],
        collection_name="sparse_test",
    )

    results = vectorstore.search(
        collection_name="sparse_test",
        query_vector=SparseEmbedding(name="sparse", values=[0.1], indices=[1]),
    )
    assert len(results) == 1


def test_collection_with_multiple_vectors(vectorstore):
    vectorstore.create_collection(
        collection_name="multi_vector_test",
        vector_config=[
            VectorConfig(dimensions=1536, name="dense_emb_name"),
            VectorConfig(name="sparse", format=EmbeddingFormat.SPARSE),
        ],
    )

    vectorstore.add(
        chunk=[
            Chunk(
                id=str(uuid.uuid4()),
                text="Hello world",
                embeddings=[
                    DenseEmbedding(name="dense_emb_name", vector=[0.0] * 1536),
                    SparseEmbedding(name="sparse", values=[0.1], indices=[1]),
                ],
            )
        ],
        collection_name="multi_vector_test",
    )

    res_no_name = vectorstore.search(
        collection_name="multi_vector_test", query_vector=[0.0] * 1536
    )
    assert len(res_no_name) == 1

    res_dense_name = vectorstore.search(
        collection_name="multi_vector_test",
        query_vector=[0.0] * 1536,
        vector_name="dense_emb_name",
    )
    assert len(res_dense_name) == 1

    res_sparse_name = vectorstore.search(
        collection_name="multi_vector_test",
        query_vector=SparseEmbedding(name="sparse", values=[0.1], indices=[1]),
        vector_name="sparse",
    )
    assert len(res_sparse_name) == 1

    res_sparse_no_name = vectorstore.search(
        collection_name="multi_vector_test",
        query_vector=SparseEmbedding(name="sparse", values=[0.1], indices=[1]),
    )
    assert len(res_sparse_no_name) == 1


def test_search_with_dense_vector(vectorstore):
    vectorstore.create_collection(
        collection_name="dense_test",
        vector_config=[VectorConfig(dimensions=1536, name="dense_emb_name")],
    )

    vectorstore.add(
        chunk=[
            Chunk(
                id=str(uuid.uuid4()),
                text="Hello world",
                embeddings=[DenseEmbedding(name="dense_emb_name", vector=[0.0] * 1536)],
            )
        ],
        collection_name="dense_test",
    )

    results = vectorstore.search(
        collection_name="dense_test",
        query_vector=[0.0] * 1536,
        vector_name="dense_emb_name",
    )
    assert len(results) == 1

    res_no_name = vectorstore.search(
        collection_name="dense_test",
        query_vector=[0.0] * 1536,
    )
    assert len(res_no_name) == 1


def test_search_with_multiple_dense_vector(vectorstore):
    vectorstore.create_collection(
        collection_name="dense_test_multiple",
        vector_config=[
            VectorConfig(dimensions=1536, name="dense_emb_name_1"),
            VectorConfig(dimensions=1536, name="dense_emb_name_2"),
        ],
    )

    vectorstore.add(
        chunk=[
            Chunk(
                id=str(uuid.uuid4()),
                text="Hello world",
                embeddings=[
                    DenseEmbedding(name="dense_emb_name_1", vector=[0.0] * 1536),
                    DenseEmbedding(name="dense_emb_name_2", vector=[0.0] * 1536),
                ],
            )
        ],
        collection_name="dense_test_multiple",
    )

    results = vectorstore.search(
        collection_name="dense_test_multiple",
        query_vector=[0.0] * 1536,
        vector_name="dense_emb_name_1",
    )
    assert len(results) == 1



================================================
FILE: docs/index.md
================================================
# datapizza-ai

**Build reliable Gen AI solutions without overhead**

`datapizza-ai` provides clear interfaces and predictable behavior for agents and RAG. End-to-end visibility and reliable orchestration keep engineers in control from PoC to scale

## Installation

Install the library using pip:

```bash
pip install datapizza-ai
```

## Key Features

- **Integration with AI Providers**: Seamlessly connect with AI services like OpenAI and Google VertexAI.
- **Complex workflows, minimal code.**: Design, automate, and scale powerful agent workflows without the overhead of boilerplate.
- **Retrieval-Augmented Generation (RAG)**: Enhance AI responses with document retrieval.
- **Faster delivery, easier onboarding for new engineers**: Rebuild a RAG + tools agent without multi-class plumbing; parity with simpler, typed interfaces.
- **Up to 40% less debugging time**: Trace and log every LLM/tool call with inputs/outputs

## Quick Start

To get started with `datapizza-ai`, ensure you have Python `>=3.10.0,<3.13.0` installed.

Here's a basic example demonstrating how to use agents in `datapizza-ai`:

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools import tool

@tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

client = OpenAIClient(api_key="YOUR_API_KEY")
agent = Agent(name="assistant", client=client, tools = [get_weather])

response = agent.run("What is the weather in Rome?")
# output: The weather in Rome is sunny
```



================================================
FILE: docs/.pages
================================================
nav:
    - Home: index.md
    - Guides
    - API Reference



================================================
FILE: docs/API Reference/index.md
================================================
# API Reference

Here you can find the API reference for `datapizza-ai`. Most of what you find here is work in progress! âš ï¸

Use the navigation on the left or search to find the classes you are interested in.



================================================
FILE: docs/API Reference/memory.md
================================================


<!-- prettier-ignore -->
::: datapizza.memory.memory.Memory
    options:
        show_source: false



================================================
FILE: docs/API Reference/.pages
================================================
nav:
    - Clients
    - Agents
    - Embedders
    - Vectorstore
    - memory.md
    - Type
    - Pipelines
    - Modules
    - Tools



================================================
FILE: docs/API Reference/Agents/agent.md
================================================
# Agent
<!-- prettier-ignore -->
::: datapizza.agents.agent.Agent



================================================
FILE: docs/API Reference/Clients/cache.md
================================================


<!-- prettier-ignore -->
::: datapizza.core.cache.cache.Cache
    options:
        show_source: false
        show_signature: true
        members: true



================================================
FILE: docs/API Reference/Clients/client_factory.md
================================================
# Client Factory

The ClientFactory provides a convenient way to create LLM clients for different providers without having to import and instantiate each client type individually.


<!-- prettier-ignore -->
::: datapizza.clients.factory.ClientFactory
    options:
        show_source: false



## Example Usage

```python
from datapizza.clients.factory import ClientFactory, Provider

# Create an OpenAI client
openai_client = ClientFactory.create(
    provider=Provider.OPENAI,
    api_key="OPENAI_API_KEY",
    model="gpt-4",
    system_prompt="You are a helpful assistant.",
    temperature=0.7
)

# Create a Google client using string provider
google_client = ClientFactory.create(
    provider="google",
    api_key="GOOGLE_API_KEY",
    model="gemini-pro",
    system_prompt="You are a helpful assistant.",
    temperature=0.5
)

# Create an Anthropic client with custom parameters
anthropic_client = ClientFactory.create(
    provider=Provider.ANTHROPIC,
    api_key="ANTHROPIC_API_KEY",
    model="claude-3-sonnet-20240229",
    system_prompt="You are a helpful assistant.",
    temperature=0.3,
)

# Use the client
response = openai_client.invoke("What is the capital of France?")
print(response.content)
```

## Supported Providers

- `openai` - OpenAI GPT models
- `google` - Google Gemini models
- `anthropic` - Anthropic Claude models
- `mistral` - Mistral AI models



================================================
FILE: docs/API Reference/Clients/clients.md
================================================
# Clients

<!-- prettier-ignore -->
::: datapizza.core.clients.client.Client
    options:
        show_source: false



================================================
FILE: docs/API Reference/Clients/models.md
================================================
# Response

<!-- prettier-ignore -->
::: datapizza.core.clients.ClientResponse
    options:
        show_source: false



================================================
FILE: docs/API Reference/Clients/.pages
================================================

nav:
    - clients.md
    - client_factory.md
    - models.md
    - cache.md
    - Avaiable_Clients



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/anthropic.md
================================================
# Anthropic


```bash
pip install datapizza-ai-clients-anthropic
```
<!-- prettier-ignore -->
::: datapizza.clients.anthropic.AnthropicClient
    options:
        show_source: false


## Usage example


```python

from datapizza.clients.anthropic import AnthropicClient

client = AnthropicClient(
    api_key="YOUR_API_KEY"
    model="claude-3-5-sonnet-20240620",
)
resposne = client.invoke("hi")
print(response.text)

```

## Show thinking

```python
import os

from datapizza.clients.anthropic import AnthropicClient
from dotenv import load_dotenv

load_dotenv()

client = AnthropicClient(
    api_key=os.getenv("ANTHROPIC_API_KEY"),
    model="claude-sonnet-4-0",
)

response = client.invoke("Hi", thinking =  {"type": "enabled", "budget_tokens": 1024})
print(response)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/AzureOpenai.md
================================================


# Azure Openai

As mentioned in [Microsoft Docs](https://learn.microsoft.com/it-it/azure/ai-foundry/openai/how-to/switching-endpoints?view=foundry-classic), Azure is compatible with the Openai Client


```sh
pip install datapizza-ai-clients-openai
```

## Usage example

```python

from datapizza.clients.openai import OpenAIClient

load_dotenv()
client = OpenAIClient(
    base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"
    api_key= "AZURE_OPENAI_API_KEY",
    model="gpt-4o-mini",
)
response = client.invoke("Hello!")
print(response.text)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/google.md
================================================



# Google


```sh
pip install datapizza-ai-clients-google
```

<!-- prettier-ignore -->
::: datapizza.clients.google.GoogleClient
    options:
        show_source: false


## Usage example

```python
from datapizza.clients.google import GoogleClient

client = GoogleClient(
    api_key="YOUR_API_KEY",
    model="gemini-2.5-flash",
    system_prompt="You are a helpful assistant.",
    temperature=0.7,
)

response = client.invoke("What is the photoelectric effect?")
print(response.text)
```

## Thinking Configuration

Gemini models support explicit reasoning via `thinking_config`. Use `include_thoughts=True` to receive `ThoughtBlock` in the response.

### Gemini 2.5 â€“ Budget-based thinking

```python
response = client.invoke(
    input="Explain step by step why the sky is blue.",
    thinking_config={
        "thinking_budget": 1024,
        "include_thoughts": True,
    },
)

print("Thoughts:", response.thoughts)
print("Answer:", response.text)
```

### Gemini 3 â€“ Level-based thinking

```python
response = client_3.invoke(
    input="Design a simple movie recommendation algorithm.",
    thinking_config={
        "thinking_level": "high",  # "low" | "high"
        "include_thoughts": True,
    },
)

print("Thoughts:", response.thoughts)
print("Answer:", response.text)
```

## Native Tools

The `GoogleClient` supports Gemini's native tools: **Google Search**, **URL Context**, **Google Maps** and **Code Execution**.

### Google Search

```python
from google.genai import types

response = client.invoke(
    input="What are the latest news about the James Webb telescope?",
    tools=[{"google_search": types.GoogleSearch()}],
)
print(response.text)
```

### URL Context

```python
response = client.invoke(
    input="Summarize this page: https://example.com/article",
    tools=[{"url_context": {}}],
)
print(response.text)
```

### Google Maps

```python
response = client.invoke(
    input="List the best restaurants in Rome",
    tools=[{"google_maps": types.GoogleMaps()}],
)
print(response.text)
```

### Code Execution

```python
from google.genai import types

response = client.invoke(
    input="Calculate the sum of the first 50 prime numbers.",
    tools=[{"code_execution": types.ToolCodeExecution()}],
)
print(response.text)
```

## Media Resolution

Control the quality/cost/latency trade-off for multimedia inputs (images, PDFs) with `media_resolution` (Gemini 3 models only).

```python
from google.genai import types

response = client.invoke(
    input="Analyze this technical diagram in detail.",
    media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH,
)
```

Available values: `MEDIA_RESOLUTION_LOW`, `MEDIA_RESOLUTION_MEDIUM`, `MEDIA_RESOLUTION_HIGH`.

## Image Generation

```python
from google.genai import types
from datapizza.type import MediaBlock
import base64

client_img = GoogleClient(
    api_key="YOUR_API_KEY",
    model="gemini-2.5-flash-image",
)

response = client_img.invoke(
    input="Create an image of a cat baking a pizza",
    image_config=types.ImageConfig(aspect_ratio="4:3", image_size="1K"), # Gemini 3 only
)
print(response.text)

# response.content could contain both TextBlock and MediaBlock
for block in response.content:
    if isinstance(block, MediaBlock):
        image_bytes = base64.b64decode(block.media.source)
        with open(f"generated.{block.media.extension}", "wb") as f:
            f.write(image_bytes)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/mistral.md
================================================

# Mistral


```
pip install datapizza-ai-clients-mistral
```
<!-- prettier-ignore -->
::: datapizza.clients.mistral.MistralClient
    options:
        show_source: false


# Usage Example

```python
from datapizza.clients.mistral import MistralClient

client = MistralClient(
    api_key=os.getenv("MISTRAL_API_KEY"),
    model="mistral-small-latest",
    system_prompt="You are a helpful assistant that responds short and concise.",
)
response = client.invoke("hi")
print(response.text)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/openai-like.md
================================================



# Openai like


```sh
pip install datapizza-ai-clients-openai-like
```
<!-- prettier-ignore -->
::: datapizza.clients.openai_like.OpenAILikeClient
    options:
        show_source: false

## Key Differences from OpenAIClient

The main difference between OpenAILikeClient and OpenAIClient is the API endpoint they use:

- OpenAILikeClient uses the chat completions API
- OpenAIClient uses the responses API

This makes OpenAILikeClient compatible with services that implement the OpenAI-compatible completions API, such as local models served through Ollama or other providers that follow the OpenAI API specification but only support the completions endpoint.


## Usage example

```python
from datapizza.clients.openai_like import OpenAILikeClient

client = OpenAILikeClient(
    api_key="OPENAI_API_KEY",
    system_prompt="You are a helpful assistant.",
)

response = client.invoke("What is the capital of France?")
print(response.content)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/openai.md
================================================


# Openai


```sh
pip install datapizza-ai-clients-openai
```
<!-- prettier-ignore -->
::: datapizza.clients.openai.OpenAIClient
    options:
        show_source: false



## Usage example

```python

from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(
    api_key="YOUR_API_KEY",
    model="gpt-4o-mini",
)
response = client.invoke("Hello!")
print(response.text)
```


## Include thinking
```python
from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(
    model= "gpt-5",
    api_key="YOUR_API_KEY",
)

response = client.invoke("Hi",reasoning={
        "effort": "low",
        "summary": "auto"
    }
)
print(response)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/watsonx.md
================================================

# IBM WatsonX

```sh
pip install datapizza-ai-clients-watsonx
```

## Usage example

```python
from datapizza.clients.watsonx import WatsonXClient

client = WatsonXClient(
    api_key=os.getenv("IBM_API_KEY"),
    url = os.getenv("IBM_ENDPOINT_URL"),
    project_id = os.getenv("IBM_PROJECT_ID"),
    model="meta-llama/llama-3-2-90b-vision-instruct",
)

response = client.invoke("Hello!")
print(response.text)
```



================================================
FILE: docs/API Reference/Clients/Avaiable_Clients/.pages
================================================
nav:
    - openai.md
    - google.md
    - anthropic.md
    - AzureOpenai.md
    - mistral.md
    - openai-like.md
    - watsonx.md



================================================
FILE: docs/API Reference/Embedders/chunk_embedder.md
================================================
# ChunkEmbedder

<!-- prettier-ignore -->
::: datapizza.embedders.ChunkEmbedder
    options:
        show_source: false


## Usage

```python
from datapizza.embedders import ChunkEmbedder
from datapizza.core.clients import Client

# Initialize with any compatible client
client = Client(...)  # Your client instance
embedder = ChunkEmbedder(
    client=client,
    model_name="text-embedding-ada-002",  # Optional model override
    embedding_name="my_embeddings",       # Optional custom embedding name
    batch_size=100                        # Optional batch size for processing
)

# Embed chunks - adds embeddings to chunk objects
embedded_chunks = embedder.embed(chunks)
```

## Features

- Specialized for embedding lists of Chunk objects
- Batch processing with configurable batch size
- Adds embeddings directly to Chunk objects
- Preserves original chunk structure and metadata
- Async embedding support with `a_embed()`
- Memory efficient batch processing
- Works with any compatible LLM client

## Examples

### Basic Chunk Embedding

```python
import os

from datapizza.embedders import ChunkEmbedder
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.type import Chunk
from dotenv import load_dotenv

load_dotenv()

# Create client and embedder
client = OpenAIEmbedder(api_key=os.getenv("OPENAI_API_KEY"))
embedder = ChunkEmbedder(
    client=client,
    model_name="text-embedding-ada-002",
    batch_size=50
)

# Create sample chunks
chunks = [
    Chunk(id="1", text="First chunk of text", metadata={"source": "doc1"}),
    Chunk(id="2", text="Second chunk of text", metadata={"source": "doc2"}),
    Chunk(id="3", text="Third chunk of text", metadata={"source": "doc3"})
]

# Embed chunks (modifies chunks in-place)
embedded_chunks = embedder.embed(chunks)

# Check embeddings were added
for i, chunk in enumerate(embedded_chunks):
    print(f"Chunk {i+1}:")
    print(f"  Text: {chunk.text[:50]}...")
    print(f"  Embeddings: {len(chunk.embeddings)}")
    if chunk.embeddings:
        print(f"  Embedding name: {chunk.embeddings[0].name}")
        print(f"  Vector size: {len(chunk.embeddings[0].vector)}")
```



================================================
FILE: docs/API Reference/Embedders/cohere_embedder.md
================================================
# CohereEmbedder



```python
pip install datapizza-ai-embedders-cohere
```

<!-- prettier-ignore -->
::: datapizza.embedders.cohere.CohereEmbedder
    options:
        show_source: false


## Usage

```python
from datapizza.embedders.cohere import CohereEmbedder

embedder = CohereEmbedder(
    api_key="your-cohere-api-key",
    base_url="https://api.cohere.ai/v1",
    input_type="search_document"  # or "search_query"
)

# Embed a single text
embedding = embedder.embed("Hello world", model_name="embed-english-v3.0")

# Embed multiple texts
embeddings = embedder.embed(
    ["Hello world", "Another text"],
    model_name="embed-english-v3.0"
)
```

## Features

- Supports Cohere's embedding models
- Configurable input type for search documents or queries
- Handles both single text and batch text embedding
- Async embedding support with `a_embed()`
- Custom endpoint support for compatible APIs
- Uses Cohere's ClientV2 for optimal performance

## Examples

### Basic Text Embedding

```python
from datapizza.embedders.cohere import CohereEmbedder

embedder = CohereEmbedder(
    api_key="your-cohere-api-key",
    base_url="https://api.cohere.ai/v1",
    input_type="search_document"
)

# Single text embedding
text = "This is a sample document for embedding."
embedding = embedder.embed(text, model_name="embed-english-v3.0")

print(f"Embedding dimensions: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
```

### Search Query Embedding

```python
from datapizza.embedders.cohere import CohereEmbedder

# Configure for search queries
embedder = CohereEmbedder(
    api_key="your-cohere-api-key",
    base_url="https://api.cohere.ai/v1",
    input_type="search_query"
)

query = "What is machine learning?"
embedding = embedder.embed(query, model_name="embed-english-v3.0")

print(f"Query embedding size: {len(embedding)}")
```

### Batch Text Embedding

```python
from datapizza.embedders.cohere import CohereEmbedder

embedder = CohereEmbedder(
    api_key="your-cohere-api-key",
    base_url="https://api.cohere.ai/v1"
)

texts = [
    "First document to embed",
    "Second document to embed",
    "Third document to embed"
]

embeddings = embedder.embed(texts, model_name="embed-english-v3.0")

for i, emb in enumerate(embeddings):
    print(f"Document {i+1} embedding size: {len(emb)}")
```

### Async Embedding

```python
import asyncio
from datapizza.embedders.cohere import CohereEmbedder

async def embed_async():
    embedder = CohereEmbedder(
        api_key="your-cohere-api-key",
        base_url="https://api.cohere.ai/v1"
    )

    text = "Async embedding example"
    embedding = await embedder.a_embed(text, model_name="embed-english-v3.0")

    return embedding

# Run async function
embedding = asyncio.run(embed_async())
```



================================================
FILE: docs/API Reference/Embedders/fast_embedder.md
================================================
# FastEmbedder


```python
pip install datapizza-ai-embedders-fastembedder
```

<!-- prettier-ignore -->
::: datapizza.embedders.fastembedder.FastEmbedder
    options:
        show_source: false


## Usage

```python
from datapizza.embedders.fastembedder import FastEmbedder

embedder = FastEmbedder(
    model_name="Qdrant/bm25",
    embedding_name="bm25_embeddings",
)

# Embed text (returns sparse embeddings)
embeddings = embedder.embed(["Hello world", "Another text"])
print(embeddings)
```

## Features

- Uses FastEmbed for efficient sparse text embeddings
- Local model execution (no API calls required)
- Configurable model caching directory
- Custom embedding naming
- Sparse embedding format for memory efficiency
- Both sync and async embedding support



================================================
FILE: docs/API Reference/Embedders/google_embedder.md
================================================
# GoogleEmbedder

```python
pip install datapizza-ai-embedders-google
```

<!-- prettier-ignore -->
::: datapizza.embedders.google.GoogleEmbedder
    options:
        show_source: false


## Usage

```python
from datapizza.embedders.google import GoogleEmbedder

embedder = GoogleEmbedder(
    api_key="your-google-api-key"
)

# Embed a single text
embedding = embedder.embed("Hello world", model_name="models/embedding-001")

# Embed multiple texts
embeddings = embedder.embed(
    ["Hello world", "Another text"],
    model_name="models/embedding-001"
)
```

## Features

- Supports Google's Gemini embedding models
- Handles both single text and batch text embedding
- Async embedding support with `a_embed()`
- Automatic client initialization and management
- Uses Google's Generative AI SDK

## Examples

### Basic Text Embedding

```python
from datapizza.embedders.google import GoogleEmbedder

embedder = GoogleEmbedder(api_key="your-google-api-key")

# Single text embedding
text = "This is a sample document for embedding."
embedding = embedder.embed(text, model_name="models/embedding-001")

print(f"Embedding dimensions: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
```

### Async Embedding

```python
import asyncio
from datapizza.embedders.google import GoogleEmbedder

async def embed_async():
    embedder = GoogleEmbedder(api_key="your-google-api-key")

    text = "Async embedding example"
    embedding = await embedder.a_embed(text, model_name="models/embedding-001")

    return embedding

# Run async function
embedding = asyncio.run(embed_async())
```



================================================
FILE: docs/API Reference/Embedders/mistral_embedder.md
================================================
# MistralEmbedder

```python
pip install datapizza-ai-embedders-mistral
```


## Usage

```python
from datapizza.embedders.mistral import MistralEmbedder

embedder = MistralEmbedder(
    api_key=os.getenv("MISTRAL_API_KEY"),
    model_name="mistral-embed",
)

embedding = mistral_embedder.embed(text)

# Embed multiple texts
embeddings = embedder.embed(
    ["Hello world", "Another text"],
)
```

### Async Embedding

```python
import asyncio
from datapizza.embedders.mistral import MistralEmbedder

async def embed_async():
    embedder = MistralEmbedder(
        api_key=os.getenv("MISTRAL_API_KEY"),
        model_name="mistral-embed",
    )

    text = "Async embedding example"
    embedding = await embedder.a_embed(text)

    return embedding

# Run async function
embedding = asyncio.run(embed_async())
```

## Ingestion

Set the batch size to a conservative value (eg: 150) to avoid the error `Too many inputs in request`:


```python
                ingestion_pipeline = IngestionPipeline(
                    modules=[
                        # some modules
                        ChunkEmbedder(
                            client=MistralEmbedder(
                                api_key="some_key",
                                model_name="mistral-embed",
                            ),
                            batch_size=150  # Mistral API limit (conservative)
                        ),  
                    ],
                )
```

### The full error
```json
{
  "object": "error",
  "message": "Too many inputs in request, split into more batches.",
  "type": "invalid_request_prompt",
  "param": null,
  "code": "3210"
}
```



================================================
FILE: docs/API Reference/Embedders/ollama_embedder.md
================================================



# OllamaEmbedder

Ollama embedders are OpenAI-compatible, which means you can use the OpenAI embedder to generate embeddings with Ollama models. Simply configure the OpenAI embedder with Ollama's base URL and leave the API key empty.

## Usage


```python
from datapizza.embedders.openai import OpenAIEmbedder

embedder = OpenAIEmbedder(
    api_key="",
    base_url="http://localhost:11434/v1",
)

# Embed a single text
embedding = embedder.embed("Hello world", model_name="nomic-embed-text")

print(embedding)

# Embed multiple texts
embeddings = embedder.embed(
    ["Hello world", "Another text"], model_name="nomic-embed-text"
)

print(embeddings)
```



================================================
FILE: docs/API Reference/Embedders/openai_embedder.md
================================================
# OpenAIEmbedder

<!-- prettier-ignore -->
::: datapizza.embedders.openai.OpenAIEmbedder
    options:
        show_source: false


## Usage

```python
from datapizza.embedders.openai import OpenAIEmbedder

embedder = OpenAIEmbedder(
    api_key="your-openai-api-key",
    base_url="https://api.openai.com/v1"  # Optional custom base URL
)

# Embed a single text
embedding = embedder.embed("Hello world", model_name="text-embedding-ada-002")

# Embed multiple texts
embeddings = embedder.embed(
    ["Hello world", "Another text"],
    model_name="text-embedding-ada-002"
)
```

## Features

- Supports OpenAI's embedding models
- Handles both single text and batch text embedding
- Async embedding support with `a_embed()`
- Custom base URL support for compatible APIs
- Automatic client initialization and management

## Examples

### Basic Text Embedding

```python
from datapizza.embedders.openai import OpenAIEmbedder

embedder = OpenAIEmbedder(api_key="your-api-key")

# Single text embedding
text = "This is a sample document for embedding."
embedding = embedder.embed(text, model_name="text-embedding-ada-002")

print(f"Embedding dimensions: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
```

### Async Embedding

```python
import asyncio
from datapizza.embedders.openai import OpenAIEmbedder

async def embed_async():
    embedder = OpenAIEmbedder(api_key="your-api-key")

    text = "Async embedding example"
    embedding = await embedder.a_embed(text, model_name="text-embedding-ada-002")

    return embedding

# Run async function
embedding = asyncio.run(embed_async())
```



================================================
FILE: docs/API Reference/Modules/captioners.md
================================================
# Captioners

Captioners are pipeline components that generate captions and descriptions for media content such as images, figures, and tables. They use LLM clients to analyze visual content and produce descriptive text.

### LLMCaptioner

<!-- prettier-ignore -->
::: datapizza.modules.captioners.LLMCaptioner
    options:
        show_source: false


A captioner that uses language models to generate captions for media nodes (figures and tables) within document hierarchies.

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.modules.captioners import LLMCaptioner
from datapizza.type import ROLE, Media, MediaNode, NodeType

client = OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4o")
captioner = LLMCaptioner(
    client=client,
    max_workers=3,
    system_prompt_table="Describe this table in detail.",
    system_prompt_figure="Describe this figure/image in detail."
)

document_node = MediaNode( node_type=NodeType.FIGURE, children=[], metadata={}, media=Media(source_type="path", source="gogole.png", extension="png", media_type="image"))
captioned_document = captioner(document_node)
print(captioned_document)
```

**Parameters:**

- `client` (Client): The LLM client to use for caption generation
- `max_workers` (int): Maximum number of concurrent workers for parallel processing (default: 3)
- `system_prompt_table` (str, optional): System prompt for table captioning
- `system_prompt_figure` (str, optional): System prompt for figure captioning

**Features:**

- Automatically finds all media nodes (figures and tables) in a document hierarchy
- Generates captions using configurable system prompts
- Supports concurrent processing for better performance
- Creates new paragraph nodes containing the original content plus generated captions
- Preserves original node metadata and structure
- Supports both sync and async processing

**Supported Node Types:**

- `FIGURE`: Images and visual figures
- `TABLE`: Tables and tabular data

**Output Format:**

The captioner creates new paragraph nodes with content in the format:
```
{original_content} <{node_type}> [{generated_caption}]
```



================================================
FILE: docs/API Reference/Modules/index.md
================================================
# Modules

This section contains API reference documentation for all datapizza-ai modules. Modules are organized by functionality and include both core modules (included by default) and optional modules that require separate installation.

## Core Modules (Included by Default)

These modules are included with `datapizza-ai-core` and are available without additional installation:

- [Parsers](Parsers/index.md) - Convert documents into structured Node representations
- [Captioners](captioners.md) - Generate captions and descriptions for content
- [Metatagger](metatagger.md) - Add metadata tags to content
- [Prompt](./Prompt/ChatPromptTemplate.md) - Manage prompts and prompt templates
- [Rewriters](rewriters.md) - Transform and rewrite content
- [Splitters](Splitters/index.md) - Split content into smaller chunks
- [Treebuilder](treebuilder.md) - Build hierarchical tree structures from content

## Optional Modules (Separate Installation Required)

These modules require separate installation via pip:

- [Rerankers](./Rerankers/index.md) - Rerank and score content relevance

Each module page includes installation instructions and usage examples.



================================================
FILE: docs/API Reference/Modules/metatagger.md
================================================
# Metatagger

Metataggers are pipeline components that add metadata tags to content chunks using language models. They analyze text content and generate relevant keywords, tags, or other metadata to enhance content discoverability and organization.

<!-- prettier-ignore -->
::: datapizza.modules.metatagger.KeywordMetatagger
    options:
        show_source: false



A metatagger that uses language models to generate keywords and metadata for text chunks.

```python
from datapizza.modules.metatagger import KeywordMetatagger
from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(api_key="your-api-key")
metatagger = KeywordMetatagger(
    client=client,
    max_workers=3,
    system_prompt="Generate relevant keywords for the given text.",
    user_prompt="Extract 5-10 keywords from this text:",
    keyword_name="keywords"
)

# Process chunks
tagged_chunks = metatagger.tag(chunks)
```

**Features:**

- Processes chunks in parallel for better performance
- Configurable prompts for different keyword extraction strategies
- Adds generated keywords to chunk metadata
- Supports custom metadata field naming
- Handles both individual chunks and lists of chunks
- Uses memory-based conversation for consistent prompting

**Input/Output:**

- Input: `Chunk` objects or lists of `Chunk` objects
- Output: Same `Chunk` objects with additional metadata containing generated keywords

## Usage Examples

### Basic Keyword Extraction
```python
import uuid

from datapizza.clients.openai import OpenAIClient
from datapizza.modules.metatagger import KeywordMetatagger
from datapizza.type import Chunk

# Initialize client and metatagger
client = OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4o")
metatagger = KeywordMetatagger(
    client=client,
    system_prompt="You are a keyword extraction expert. Generate relevant, concise keywords.",
    user_prompt="Extract 5-8 important keywords from this text:",
    keyword_name="keywords"
)

# Process chunks
chunks = [
    Chunk(id=str(uuid.uuid4()), text="Machine learning algorithms are transforming healthcare diagnostics."),
    Chunk(id=str(uuid.uuid4()), text="Climate change impacts ocean temperatures and marine ecosystems.")
]

tagged_chunks = metatagger.tag(chunks)

# Access generated keywords
for chunk in tagged_chunks:
    print(f"Content: {chunk.text}")
    print(f"Keywords: {chunk.metadata.get('keywords', [])}")
```



================================================
FILE: docs/API Reference/Modules/rewriters.md
================================================
# Rewriters

Rewriters are pipeline components that transform and rewrite text content using language models. They can modify content style, format, tone, or structure while preserving meaning and important information.

<!-- prettier-ignore -->
::: datapizza.modules.rewriters.ToolRewriter
    options:
        show_source: false




A rewriter that uses language models to transform text content with specific instructions and tools.

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.modules.rewriters import ToolRewriter

client = OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4o")

# Create a simplification rewriter
simplifier = ToolRewriter(
    client=client,
    system_prompt="You are an expert at simplifying complex text for general audiences.",
)

# Simplify technical content
technical_text = """
The algorithmic implementation utilizes a recursive binary search methodology
to optimize computational complexity in logarithmic time scenarios.
"""

simplified_text = simplifier(technical_text)
print(simplified_text)
# Output: recursive binary search
```

**Features:**

- Flexible content transformation with custom instructions
- Support for various rewriting tasks (summarization, style changes, format conversion)
- Integration with tool calling for enhanced capabilities
- Preserves important information while transforming presentation
- Supports both sync and async processing
- Configurable prompting for different rewriting strategies



================================================
FILE: docs/API Reference/Modules/treebuilder.md
================================================
# Treebuilder

Treebuilders are pipeline components that construct hierarchical tree structures (Node objects) from various types of content. They convert flat or unstructured content into organized, nested representations that facilitate better processing and understanding.

<!-- prettier-ignore -->
::: datapizza.modules.treebuilder.LLMTreeBuilder
    options:
        show_source: false



A treebuilder that uses language models to analyze content and create hierarchical structures based on semantic understanding.

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.modules.treebuilder import LLMTreeBuilder

client = OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4o")
treebuilder = LLMTreeBuilder(client=client)

flat_content = "This is a flat piece of content. It should be converted into a hierarchical structure."

structured_document = treebuilder.parse(flat_content)

print(structured_document)
```

**Features:**

- Semantic understanding of content organization
- Configurable tree depth and structure rules
- Support for various content types (articles, reports, manuals, etc.)
- Preserves original content while adding hierarchical organization
- Metadata extraction and tagging during structure creation
- Supports both sync and async processing

## Usage Examples

### Basic Tree Structure Creation
```python
from datapizza.modules.treebuilder import LLMTreeBuilder
from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(api_key="your-openai-key")

# Create basic treebuilder
treebuilder = LLMTreeBuilder(client=client)

# Unstructured content
flat_content = """
Introduction to Machine Learning

Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.

Supervised Learning
In supervised learning, algorithms learn from labeled training data. The goal is to predict outcomes for new data based on patterns learned from the training set. Common examples include classification and regression tasks.

Classification algorithms predict discrete categories or classes. For example, email spam detection classifies emails as either spam or not spam.

Regression algorithms predict continuous numerical values. For instance, predicting house prices based on features like location and size.

Unsupervised Learning
Unsupervised learning finds patterns in data without labeled examples. The algorithm identifies hidden structures in input data.

Clustering groups similar data points together. Customer segmentation is a common application.

Dimensionality reduction reduces the number of features while preserving important information.

Reinforcement Learning
This approach learns through interaction with an environment, receiving rewards or penalties for actions taken.
"""

# Build hierarchical structure
structured_document = treebuilder.parse(flat_content)

# Navigate the structure
def print_structure(node, depth=0):
    indent = "  " * depth
    print(f"{indent}{node.node_type.value}: {node.content[:50]}...")
    for child in node.children:
        print_structure(child, depth + 1)

print_structure(structured_document)
```



================================================
FILE: docs/API Reference/Modules/.pages
================================================
title: Modules
nav:
  - index.md
  - Parsers
  - treebuilder.md
  - captioners.md
  - Splitters
  - metatagger.md

  #- embedders.md

  - rewriters.md
  - Rerankers
  - Prompt



================================================
FILE: docs/API Reference/Modules/Parsers/azure_parser.md
================================================
# AzureParser

A document parser that uses Azure AI Document Intelligence to extract structured content from PDFs and other documents.

## Installation

```bash
pip install datapizza-ai-parsers-azure
```

<!-- prettier-ignore -->
::: datapizza.modules.parsers.azure.AzureParser
    options:
        show_source: false




## Usage

```python
from datapizza.modules.parsers.azure import AzureParser

parser = AzureParser(
    api_key="your-azure-key",
    endpoint="https://your-endpoint.cognitiveservices.azure.com/",
    result_type="text"
)

document_node = parser.parse("document.pdf")
```

## Parameters

- `api_key` (str): Azure AI Document Intelligence API key
- `endpoint` (str): Azure service endpoint URL
- `result_type` (str): Output format - "text" or "markdown" (default: "text")

## Features

- Creates hierarchical document structure: document â†’ sections â†’ paragraphs/tables/figures
- Extracts bounding regions and spatial layout information
- Handles tables, figures, and complex document layouts
- Preserves metadata including page numbers and coordinates
- Supports both sync and async processing
- Converts media elements to base64 images with coordinates

## Node Types Created

- `DOCUMENT`: Root document container
- `SECTION`: Document sections
- `PARAGRAPH`: Text paragraphs with content
- `TABLE`: Tables with markdown representation
- `FIGURE`: Images and figures with media data

## Examples

### Basic Document Processing

```python
from datapizza.modules.parsers.azure import AzureParser
import os

parser = AzureParser(
    api_key=os.getenv("AZURE_DOC_INTELLIGENCE_KEY"),
    endpoint=os.getenv("AZURE_DOC_INTELLIGENCE_ENDPOINT"),
    result_type="markdown"
)

# Parse document
document = parser.parse("complex_document.pdf")

# Access hierarchical structure
for section in document.children:
    for paragraph in section.children:
        print(f"Content: {paragraph.content}")
        print(f"Bounding regions: {paragraph.metadata.get('boundingRegions', [])}")
```

### Async Processing

```python
async def process_document():
    document = await parser.a_run("document.pdf")
    return document

# Usage in async context
document = await process_document()
```



================================================
FILE: docs/API Reference/Modules/Parsers/docling_parser.md
================================================
# DoclingParser

A document parser that uses Docling to convert PDF files into structured hierarchical Node representations with preserved layout information and media extraction.

## Installation

```bash
pip install datapizza-ai-parsers-docling
```

<!--
::: datapizza.modules.parsers.docling.DoclingParser
    options:
        show_source: false


 -->



## Usage

```python
from datapizza.modules.parsers.docling import DoclingParser

# Basic usage
parser = DoclingParser()
document_node = parser.parse("sample.pdf")

print(document_node)
```

## Parameters

- `json_output_dir` (str, optional): Directory to save intermediate Docling JSON results for debugging and inspection

## Features

- **PDF Processing**: Converts PDF files using Docling's DocumentConverter with OCR and table structure detection
- **Hierarchical Structure**: Creates logical document hierarchy (document â†’ sections â†’ paragraphs/tables/figures)
- **Media Extraction**: Extracts images and tables as base64-encoded media with bounding box coordinates
- **Layout Preservation**: Maintains spatial layout information including page numbers and bounding regions
- **Markdown Generation**: Converts tables to markdown format and handles list structures
- **Metadata Rich**: Preserves full Docling metadata in `docling_raw` with convenience fields

## Configuration

The parser automatically configures Docling with:

- Table structure detection enabled
- Full page OCR with EasyOCR
- PyPdfium backend for PDF processing

## Examples

### Basic Document Processing

```python
from datapizza.modules.parsers.docling import DoclingParser

parser = DoclingParser()
document = parser.parse("research_paper.pdf")

# Access hierarchical structure
for section in document.children:
    print(f"Section: {section.metadata.get('docling_label', 'Unknown')}")
    for child in section.children:
        if child.node_type.name == "PARAGRAPH":
            print(f"  Paragraph: {child.content[:100]}...")
        elif child.node_type.name == "TABLE":
            print(f"  Table with {len(child.children)} rows")
        elif child.node_type.name == "FIGURE":
            print(f"  Figure: {child.metadata.get('docling_label', 'Image')}")
```

### Configured OCR Document Processing

```python
from datapizza.modules.parsers.docling import DoclingParser
from datapizza.modules.parsers.docling.ocr_options import OCROptions, OCREngine

# Configure parser with EasyOCR (default, backward compatible)
parser = DoclingParser()
document = parser.parse("document.pdf")

# Configure parser with Tesseract OCR for Italian language
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["ita"],
)
parser = DoclingParser(ocr_options=ocr_config)
document = parser.parse("italian_document.pdf")

# Configure parser with Tesseract for multiple languages
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["eng", "fra"],  # English and French
)
parser = DoclingParser(ocr_options=ocr_config)
document = parser.parse("multilingual_document.pdf")

# Configure parser with Tesseract for Italian and English
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["ita", "eng"],
)
parser = DoclingParser(ocr_options=ocr_config)
document = parser.parse("italian_english_document.pdf")

# Configure parser with automatic language detection
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["auto"],
)
parser = DoclingParser(ocr_options=ocr_config)
document = parser.parse("mixed_language_document.pdf")

# Disable OCR completely (for documents without text that needs OCR)
ocr_config = OCROptions(engine=OCREngine.NONE)
parser = DoclingParser(ocr_options=ocr_config)
document = parser.parse("native_text_document.pdf")

# Enable custom EasyOCR configuration
ocr_config = OCROptions(
    engine=OCREngine.EASY_OCR,
    easy_ocr_force_full_page=False,  # Process only text-light regions
)
parser = DoclingParser(ocr_options=ocr_config)
document = parser.parse("document.pdf")

# Parse with JSON output for debugging
ocr_config = OCROptions(engine=OCREngine.TESSERACT, tesseract_lang=["ita", "eng"])
parser = DoclingParser(
    json_output_dir="./docling_debug",
    ocr_options=ocr_config,
)
document = parser.parse("document.pdf")
# Intermediate Docling JSON will be saved to ./docling_debug/document.json
```

### Tesseract Language Options

When using Tesseract OCR, you can specify languages in the `tesseract_lang` parameter as a list:

**Single Language:**
```python
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["eng"],  # English
)
```

**Multiple Languages:**
```python
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["eng", "ita", "fra"],  # English, Italian, French
)
```

**Automatic Language Detection:**
```python
ocr_config = OCROptions(
    engine=OCREngine.TESSERACT,
    tesseract_lang=["auto"],  # Auto-detect language
)
```

**Common Language Codes:**
- `"eng"` - English
- `"ita"` - Italian
- `"fra"` - French
- `"deu"` - German
- `"spa"` - Spanish
- `"por"` - Portuguese
- `"chi_sim"` - Simplified Chinese
- `"chi_tra"` - Traditional Chinese
- `"jpn"` - Japanese
- `"auto"` - Automatic detection

For a complete list of supported languages, refer to [Tesseract documentation](https://github.com/UB-Mannheim/tesseract/wiki).



================================================
FILE: docs/API Reference/Modules/Parsers/index.md
================================================
# Parsers

Parsers are pipeline components that convert documents into structured hierarchical Node representations. They extract text, layout information, and metadata from various document formats to create tree-like data structures for further processing.

Each parser should return a [Node](../../Type/node.md) object, which is a hierarchical representation of the document content.

If you write a custom parser that returns a different type of object (for example, the plain text of the document content), you must use a [TreeBuilder](../treebuilder.md) to convert it into a Node.

## Available Parsers

### Core Parsers (Included by Default)

- [TextParser](text_parser.md) - Simple text parser for plain text content

### Optional Parsers (Separate Installation Required)

- [AzureParser](azure_parser.md) - Azure AI Document Intelligence parser for PDFs and documents
- [DoclingParser](docling_parser.md) - Docling-based parser for PDFs with layout preservation and media extraction

## Common Usage Patterns

### Basic Text Processing
```python
from datapizza.modules.parsers.text_parser import parse_text

# Process plain text
document = parse_text("Your text content here")
```

### Document Processing Pipeline
```python
from datapizza.modules.parsers import TextParser
from datapizza.modules.splitters import RecursiveSplitter

# Create processing pipeline
parser = TextParser()
splitter = RecursiveSplitter(chunk_size=1000)

# Process document
document = parser.parse(text_content)
chunks = splitter(document.content)
```



================================================
FILE: docs/API Reference/Modules/Parsers/text_parser.md
================================================
# TextParser


<!-- prettier-ignore -->
::: datapizza.modules.parsers.TextParser
    options:
        show_source: false

## Usage

```python
from datapizza.modules.parsers.text_parser import TextParser, parse_text

# Using the class
parser = TextParser()
document_node = parser.parse("Your text content here", metadata={"source": "example"})

# Using the convenience function
document_node = parse_text("Your text content here")
```

## Parameters

The TextParser class takes no initialization parameters.

The `parse` method accepts:
- `text` (str): The text content to parse
- `metadata` (dict, optional): Additional metadata to attach to the document

## Features

- Splits text into paragraphs based on double newlines
- Breaks paragraphs into sentences using regex patterns
- Creates three-level hierarchy: document â†’ paragraphs â†’ sentences
- Preserves original text content in sentence nodes
- Adds index metadata for paragraphs and sentences

## Node Types Created

- `DOCUMENT`: Root document container
- `PARAGRAPH`: Text paragraphs
- `SENTENCE`: Individual sentences with content

## Examples

### Basic Usage

```python
from datapizza.modules.parsers.text_parser import parse_text

text_content = """
This is the first paragraph.
It contains multiple sentences.

This is the second paragraph.
It also has content.
"""

document = parse_text(text_content, metadata={"source": "user_input"})

# Navigate structure
for i, paragraph in enumerate(document.children):
    print(f"Paragraph {i}:")
    for j, sentence in enumerate(paragraph.children):
        print(f"  Sentence {j}: {sentence.content}")
```

### Class-Based Usage

```python
from datapizza.modules.parsers.text_parser import TextParser

parser = TextParser()

# Parse with custom metadata
document = parser.parse(
    text="Sample text content here.",
    metadata={
        "source": "api_input",
        "timestamp": "2024-01-01",
        "language": "en"
    }
)

# Access document metadata
print(f"Source: {document.metadata['source']}")
print(f"Number of paragraphs: {len(document.children)}")
```

### Pipeline Integration

```python
from datapizza.modules.parsers.text_parser import TextParser
from datapizza.modules.splitters import RecursiveSplitter

# Create processing pipeline
parser = TextParser()
splitter = RecursiveSplitter(chunk_size=500)

def process_text_document(text):
    # Parse into hierarchical structure
    document = parser.parse(text)

    # Convert back to flat text for splitting
    full_text = document.content

    # Split into chunks
    chunks = splitter(full_text)

    return document, chunks

# Process document
structured_doc, chunks = process_text_document(long_text)
```

## Best Practices

1. **Use for Simple Text**: Best suited for plain text content without complex formatting
2. **Preprocessing**: Clean text of unwanted characters before parsing if needed
3. **Metadata**: Add relevant metadata during parsing for downstream processing
4. **Pipeline Integration**: Combine with other modules for complete text processing workflows



================================================
FILE: docs/API Reference/Modules/Parsers/.pages
================================================
title: Parsers
nav:
  - text_parser.md
  - docling_parser.md
  - azure_parser.md



================================================
FILE: docs/API Reference/Modules/Prompt/ChatPromptTemplate.md
================================================
# ChatPromptTemplate

The ChatPromptTemplate class provides utilities for managing prompts, prompt templates, and conversation memory for AI interactions. It helps structure and format prompts for various AI tasks and maintain conversation context.

<!-- prettier-ignore -->
::: datapizza.modules.prompt.ChatPromptTemplate
    options:
        show_source: false

## Overview

The ChatPromptTemplate module provides utilities for managing prompts and prompt templates in AI pipelines.

```python
from datapizza.modules.prompt import prompt
```

**Features:**

- Prompt template management and formatting
- Context-aware prompt construction
- Integration with memory systems for conversation history
- Structured prompt formatting for different AI tasks

## Usage Examples

### Basic Prompt Management
```python
import uuid

from datapizza.modules.prompt import ChatPromptTemplate
from datapizza.type import Chunk

# Create structured prompts for different tasks
system_prompt = ChatPromptTemplate(
    user_prompt_template="You are helping with data analysis tasks, this is the user prompt: {{ user_prompt }}",
    retrieval_prompt_template="Retrieved content:\n{% for chunk in chunks %}{{ chunk.text }}\n{% endfor %}"
)

print(system_prompt.format(user_prompt="Hello, how are you?", chunks=[Chunk(id=str(uuid.uuid4()), text="This is a chunk"), Chunk(id=str(uuid.uuid4()), text="This is another chunk")]))

```



================================================
FILE: docs/API Reference/Modules/Prompt/ImageRAGPrompt.md
================================================
# ImageRAGPrompt

Specialized prompting utilities for Retrieval-Augmented Generation (RAG) with image content. The ImageRAGPrompt class provides multimodal content integration for vision-language models.

<!-- prettier-ignore -->
::: datapizza.modules.prompt.ImageRAGPrompt
    options:
        show_source: false


## Overview

```python
from datapizza.modules.prompt.image_rag import ImageRAGPrompt

# Initialize image RAG prompt handler
image_rag = ImageRAGPrompt()
```

**Features:**

- Image-aware RAG prompt construction
- Multimodal content integration
- Context preservation for image-text interactions
- Optimized prompting for vision-language models

## Usage Examples

### Basic Image RAG Usage
```python
from datapizza.modules.prompt.image_rag import ImageRAGPrompt
from datapizza.type import Media

# Initialize image RAG prompt
image_rag = ImageRAGPrompt()

# Create multimodal RAG prompt
media_content = Media(data=image_data, media_type="image/png")
rag_prompt = image_rag.create_rag_prompt(
    query="What does this chart show?",
    retrieved_context=text_context,
    images=[media_content],
    instructions="Analyze both the text context and image content"
)
```



================================================
FILE: docs/API Reference/Modules/Prompt/.pages
================================================
title: Prompt
nav:
  - ChatPromptTemplate.md



================================================
FILE: docs/API Reference/Modules/Rerankers/cohere_reranker.md
================================================
# CohereReranker

A reranker that uses Cohere's reranking API to score and reorder documents based on query relevance.

## Installation

```bash
pip install datapizza-ai-rerankers-cohere
```

<!-- prettier-ignore -->
::: datapizza.modules.rerankers.cohere.CohereReranker
    options:
        show_source: false


## Usage

```python
from datapizza.modules.rerankers.cohere import CohereReranker

reranker = CohereReranker(
    api_key="your-cohere-api-key",
    endpoint="https://api.cohere.ai/v1",
    top_n=10,
    threshold=0.5,
    model="rerank-v3.5",
)

# Rerank chunks based on query
query = "What are the benefits of machine learning?"
reranked_chunks = reranker.rerank(query, chunks)
```

## Features

- High-quality semantic reranking using Cohere's models
- Configurable result count and score thresholds
- Support for both sync and async processing
- Automatic relevance scoring for retrieved content
- Integration with Cohere's latest reranking models
- Flexible endpoint configuration for different Cohere services

## Examples

### Basic Usage

```python
import uuid

from datapizza.modules.rerankers.cohere import CohereReranker
from datapizza.type import Chunk

# Initialize reranker
reranker = CohereReranker(
    api_key="COHERE_API_KEY",
    endpoint="https://api.cohere.ai/v1",
    top_n=5,
    threshold=0.6,
    model="rerank-v3.5",
)

# Sample retrieved chunks
chunks = [
    Chunk(id=str(uuid.uuid4()), text="Machine learning enables computers to learn from data..."),
    Chunk(id=str(uuid.uuid4()), text="Deep learning is a subset of machine learning..."),
    Chunk(id=str(uuid.uuid4()), text="Neural networks consist of interconnected nodes..."),
    Chunk(id=str(uuid.uuid4()), text="Supervised learning uses labeled training data..."),
    Chunk(id=str(uuid.uuid4()), text="The weather forecast shows rain tomorrow...")
]

query = "What is deep learning and how does it work?"

# Rerank based on relevance to query
reranked_chunks = reranker.rerank(query, chunks)

# Display results with scores
for i, chunk in enumerate(reranked_chunks):
    score = chunk.metadata.get('relevance_score', 'N/A')
    print(f"Rank {i+1} (Score: {score}): {chunk.text[:80]}...")
```



================================================
FILE: docs/API Reference/Modules/Rerankers/index.md
================================================
# Rerankers

Rerankers are pipeline components that reorder and score retrieved content based on relevance to a query. They improve retrieval quality by applying more sophisticated ranking algorithms after initial retrieval, helping surface the most relevant content for user queries.

## Installation

All rerankers require separate installation via pip and are not included by default with `datapizza-ai-core`.

## Available Rerankers

### Optional Rerankers (Separate Installation Required)

- [CohereReranker](cohere_reranker.md) - Uses Cohere's reranking API for high-quality semantic reranking
- [TogetherReranker](together_reranker.md) - Uses Together AI's API with various model options

## Common Features

- High-quality semantic reranking using specialized models
- Configurable result count and score thresholds
- Support for both sync and async processing
- Automatic relevance scoring for retrieved content
- Integration with various reranking model providers

## Usage Patterns

### Basic Reranking Pipeline
```python
from datapizza.modules.rerankers.cohere import CohereReranker

reranker = CohereReranker(
    api_key="your-cohere-key",
    endpoint="https://api.cohere.ai/v1",
    top_n=5,
    threshold=0.6
)

query = "What is deep learning?"
reranked_chunks = reranker(query, chunks)
```

### RAG Pipeline Integration
```python
from datapizza.modules.rerankers.together import TogetherReranker
from datapizza.vectorstores import QdrantVectorStore

# Initial broad retrieval
vectorstore = QdrantVectorStore(collection_name="documents")
initial_results = vectorstore.similarity_search(query, k=20)

# Rerank for better relevance
reranker = TogetherReranker(api_key="together-key", model="rerank-model")
reranked_results = reranker(query, initial_results)
```

## Best Practices

1. **Choose the Right Model**: Select reranker models based on your domain and language requirements
2. **Tune Thresholds**: Experiment with relevance score thresholds to balance precision and recall
3. **Initial Retrieval Size**: Retrieve more documents initially (k=20-50) before reranking to improve final quality
4. **Performance Considerations**: Use async processing for high-throughput applications
5. **Cost Management**: Monitor API usage, especially for high-volume applications
6. **Evaluation**: Test different rerankers on your specific data to find the best performance



================================================
FILE: docs/API Reference/Modules/Rerankers/together_reranker.md
================================================
# TogetherReranker

A reranker that uses Together AI's API for document reranking with various model options.

## Installation

```bash
pip install datapizza-ai-rerankers-together
```
<!-- prettier-ignore -->
::: datapizza.modules.rerankers.together.TogetherReranker
    options:
        show_source: false



## Usage

```python
from datapizza.modules.rerankers.together import TogetherReranker

reranker = TogetherReranker(
    api_key="your-together-api-key",
    model="sentence-transformers/msmarco-bert-base-dot-v5",
    top_n=15,
    threshold=0.3
)

# Rerank documents
query = "How to implement neural networks?"
reranked_results = reranker.rerank(query, document_chunks)
```

## Features

- Access to multiple reranking model options
- Flexible model selection for different use cases
- Score-based filtering with configurable thresholds
- Support for various domain-specific models
- Integration with Together AI's model ecosystem
- Automatic model initialization and management

## Available Models

Common reranking models available through Together AI:

- `sentence-transformers/msmarco-bert-base-dot-v5`
- `sentence-transformers/all-MiniLM-L6-v2`
- `sentence-transformers/all-mpnet-base-v2`
- Custom fine-tuned models for specific domains

## Examples

### Basic Usage

```python
import uuid

from datapizza.modules.rerankers.together import TogetherReranker
from datapizza.type import Chunk

# Initialize with specific model
reranker = TogetherReranker(
    api_key="TOGETHER_API_KEY",
    model="Salesforce/Llama-Rank-V1",
    top_n=10,
    threshold=0.4
)

# Sample chunks
chunks = [
    Chunk(id=str(uuid.uuid4()), text="Neural networks are computational models inspired by biological brains..."),
    Chunk(id=str(uuid.uuid4()), text="Deep learning uses multiple layers to learn complex patterns..."),
    Chunk(id=str(uuid.uuid4()), text="Backpropagation is the algorithm used to train neural networks..."),
    Chunk(id=str(uuid.uuid4()), text="The weather is sunny today with mild temperatures..."),
    Chunk(id=str(uuid.uuid4()), text="Convolutional neural networks excel at image recognition tasks...")
]

query = "How do neural networks learn?"

# Rerank based on relevance
reranked_results = reranker.rerank(query, chunks)

# Display results
for i, chunk in enumerate(reranked_results):
    score = chunk.metadata.get('relevance_score', 'N/A')
    print(f"Rank {i+1} (Score: {score}): {chunk.text[:70]}...")
```



================================================
FILE: docs/API Reference/Modules/Rerankers/.pages
================================================
title: Rerankers
nav:
  - cohere_reranker.md
  - together_reranker.md



================================================
FILE: docs/API Reference/Modules/Splitters/index.md
================================================
# Splitters

Splitters are pipeline components that divide large text content into smaller, manageable chunks. They help optimize content for processing, storage, and retrieval in AI applications by creating appropriately sized segments while preserving context and meaning.

## Installation

All splitters are included with `datapizza-ai-core` and require no additional installation.

## Available Splitters

### Core Splitters (Included by Default)

- [RecursiveSplitter](recursive_splitter.md) - Recursively divides text using multiple splitting strategies
- [TextSplitter](text_splitter.md) - Basic text splitter for general-purpose chunking
- [NodeSplitter](node_splitter.md) - Splitter for Node objects preserving hierarchical structure
- [PDFImageSplitter](pdf_image_splitter.md) - Specialized splitter for PDF content with images

## Common Features

- Multiple splitting strategies for different content types
- Configurable chunk sizes and overlap
- Context preservation through overlapping
- Support for structured content (nodes, PDFs, etc.)
- Metadata preservation during splitting
- Spatial layout awareness for document content

## Usage Patterns

### Basic Text Splitting
```python
from datapizza.modules.splitters import RecursiveSplitter

splitter = RecursiveSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter(long_text_content)
```

### Document Processing Pipeline
```python
from datapizza.modules.parsers import TextParser
from datapizza.modules.splitters import NodeSplitter

parser = TextParser()
splitter = NodeSplitter(max_char = 4000)

document = parser.parse(text_content)
structured_chunks = splitter(document)
```

### Choosing the Right Splitter

- **RecursiveSplitter**: Best for general text content, articles, and most use cases
- **TextSplitter**: Simple splitting for basic text without complex requirements
- **NodeSplitter**: When working with structured Node objects from parsers
- **PDFImageSplitter**: Specifically for PDF content with images and complex layouts
- **BBoxMerger**: Utility for processing documents with spatial layout information



================================================
FILE: docs/API Reference/Modules/Splitters/node_splitter.md
================================================
# NodeSplitter

<!-- prettier-ignore -->
::: datapizza.modules.splitters.NodeSplitter
    options:
        show_source: false


## Usage

```python
from datapizza.modules.splitters import NodeSplitter

splitter = NodeSplitter(
    max_char=800,
)

node_chunks = splitter.split(document_node)
```

## Features

- Maintains Node object structure and hierarchy
- Preserves metadata from original nodes
- Respects node boundaries when possible
- Supports both structure-preserving and flattened chunking
- Handles nested node relationships intelligently

## Examples

### Basic Node Splitting

```python
from datapizza.modules.parsers import TextParser
from datapizza.modules.splitters import NodeSplitter

# Parse text into nodes
parser = TextParser()
document = parser.parse("""
This is the first section of the document.
It contains important information about the topic.

This is the second section with more details.
It provides additional context and examples.

The final section concludes the document.
It summarizes the key points discussed.
""")

splitter = NodeSplitter(
    max_char=150,
)

chunks = splitter.split(document)

# Examine the structured chunks
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}:")
    print(f"  Content length: {len(chunk.text)}")
    print(f"  Content preview: {chunk.text[:80]}...")
    print("---")
```



================================================
FILE: docs/API Reference/Modules/Splitters/pdf_image_splitter.md
================================================
# PDFImageSplitter

<!-- prettier-ignore -->
::: datapizza.modules.splitters.PDFImageSplitter
    options:
        show_source: false


## Usage

```python
from datapizza.modules.splitters import PDFImageSplitter

splitter = PDFImageSplitter()

pdf_chunks = splitter("pdf_path")
```

## Features

- Specialized handling of PDF document structure
- Preserves image data and visual elements
- Maintains spatial layout information
- Includes page-level metadata and coordinates
- Handles complex document layouts with mixed content
- Optimized for PDF content from document intelligence services

## Examples

### Basic PDF Content Splitting

```python
from datapizza.modules.splitters import PDFImageSplitter

# Split while preserving images and layout
pdf_splitter = PDFImageSplitter()

pdf_chunks = pdf_splitter("pdf_path")

# Examine chunks with visual content
for i, chunk in enumerate(pdf_chunks):
    print(f"Chunk {i+1}:")
    print(f"  Content length: {len(chunk.content)}")
    print(f"  Page: {chunk.metadata.get('page_number', 'unknown')}")

    if hasattr(chunk, 'media') and chunk.media:
        print(f"  Media elements: {len(chunk.media)}")
        for media in chunk.media:
            print(f"    Type: {media.media_type}")

    if 'boundingRegions' in chunk.metadata:
        print(f"  Bounding regions: {len(chunk.metadata['boundingRegions'])}")

    print("---")
```



================================================
FILE: docs/API Reference/Modules/Splitters/recursive_splitter.md
================================================
# RecursiveSplitter


<!-- prettier-ignore -->
::: datapizza.modules.splitters.RecursiveSplitter
    options:
        show_source: false




## Usage

```python
from datapizza.modules.parsers import TextParser
from datapizza.modules.splitters import RecursiveSplitter

splitter = RecursiveSplitter(
    max_char=10,
    overlap=1,
)

# Parse text into nodes because RecursiveSplitter need Node
parser = TextParser()
document = parser.parse("""
This is the first section of the document.
It contains important information about the topic.

This is the second section with more details.
It provides additional context and examples.

The final section concludes the document.
It summarizes the key points discussed.
""")

chunks = splitter.split(document)
print(chunks)
```

## Features

- Uses multiple separator strategies in order of preference
- Recursive approach ensures optimal chunk boundaries
- Configurable chunk size and overlap for context preservation
- Handles various content types with appropriate separator selection
- Preserves content structure while maintaining size limits



================================================
FILE: docs/API Reference/Modules/Splitters/text_splitter.md
================================================
# TextSplitter


<!-- prettier-ignore -->
::: datapizza.modules.splitters.TextSplitter
    options:
        show_source: false

## Usage

```python
from datapizza.modules.splitters import TextSplitter

splitter = TextSplitter(
    max_char=500,
    overlap=50
)

chunks = splitter.split(text_content)
```

## Features

- Simple, straightforward text splitting algorithm
- Configurable chunk size and overlap
- Lightweight implementation for basic splitting needs
- Preserves character-level accuracy in chunk boundaries
- Minimal overhead for high-performance applications

## Examples

### Basic Usage

```python
from datapizza.modules.splitters import TextSplitter

splitter = TextSplitter(max_char=50, overlap=5)

text = """
This is a sample text that we want to split into smaller chunks.
The TextSplitter will divide this content based on the specified
chunk size and overlap parameters. This ensures that information
is preserved while creating manageable pieces of content.
"""

chunks = splitter.split(text)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {len(chunk.text)} chars")
    print(f"Content: {chunk.text}")
    print("---")
```



================================================
FILE: docs/API Reference/Modules/Splitters/.pages
================================================
title: Splitters
nav:
  - recursive_splitter.md
  - text_splitter.md
  - node_splitter.md
  - pdf_image_splitter.md



================================================
FILE: docs/API Reference/Pipelines/dag.md
================================================


<!-- prettier-ignore -->
::: datapizza.pipeline.dag_pipeline.DagPipeline
    options:
        show_source: false
        show_signature: true



================================================
FILE: docs/API Reference/Pipelines/functional.md
================================================


<!-- prettier-ignore -->
::: datapizza.pipeline.functional_pipeline.FunctionalPipeline
    options:
        show_source: false
        show_signature: true



================================================
FILE: docs/API Reference/Pipelines/ingestion.md
================================================


<!-- prettier-ignore -->
::: datapizza.pipeline.pipeline.IngestionPipeline
    options:
        show_source: false
        show_signature: true



================================================
FILE: docs/API Reference/Tools/duckduckgo.md
================================================
# DuckDuckGo

```bash
pip install datapizza-ai-tools-duckduckgo
```

<!-- prettier-ignore -->
::: datapizza.tools.duckduckgo.DuckDuckGoSearchTool
    options:
        show_source: false

## Overview

The DuckDuckGoSearchTool provides web search capabilities using the DuckDuckGo search engine. This tool enables AI models to search for real-time information from the web, making it particularly useful for grounding model responses with current data.

## Features

- **Web Search**: Search the web using DuckDuckGo's search engine
- **Privacy-focused**: Uses DuckDuckGo which doesn't track users
- **Simple Integration**: Easy to integrate with AI agents and pipelines
- **Real-time Results**: Get current web search results

## Usage Example

```python
from datapizza.tools.duckduckgo import DuckDuckGoSearchTool

# Initialize the tool
search_tool = DuckDuckGoSearchTool()

# Perform a search
results = search_tool.search("latest AI developments 2024")

# Process results
for result in results:
    print(f"Title: {result.get('title', 'N/A')}")
    print(f"URL: {result.get('href', 'N/A')}")
    print(f"Body: {result.get('body', 'N/A')}")
    print("---")
```

## Integration with Agents

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.duckduckgo import DuckDuckGoSearchTool

agent = Agent(
    name="agent",
    tools=[DuckDuckGoSearchTool()],
    client=OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4.1"),
)

response = agent.run("What is datapizza? and who are the founders?", tool_choice="required_first")
print(response)

```



================================================
FILE: docs/API Reference/Tools/filesystem.md
================================================
# FileSystem

```bash
pip install datapizza-ai-tools-filesystem
```


## Overview

This tool provides a robust and easy-to-use interface for `datapizza-ai` agents to perform various operations on the local file system. This includes listing, reading, writing, creating, deleting, moving, copying, and precisely replacing content within files.

> **âš ï¸ Warning: Risk of Data Loss and System Modification**
>
> Operations performed by this tool directly affect your local file system. Using functions like `delete_file`, `delete_directory`, and `write_file` can lead to permanent data loss or unintended system modifications if not used carefully. Exercise extreme caution.

## Features

- **List directories**: `list_directory(path: str)`
- **Read files**: `read_file(file_path: str)`
- **Write files**: `write_file(file_path: str, content: str)`
- **Create directories**: `create_directory(path: str)`
- **Delete files**: `delete_file(file_path: str)`
- **Delete directories**: `delete_directory(path: str, recursive: bool = False)`
- **Move or rename**: `move_item(source_path: str, destination_path: str)`
- **Copy files**: `copy_file(source_path: str, destination_path: str)`
- **Replace with precision**: `replace_in_file(file_path: str, old_string: str, new_string: str)` - Replaces a block of text only if it appears exactly once, requiring context in `old_string` for safety.

## Usage Example

```python
import os
import tempfile
import shutil
from datapizza.tools.filesystem import FileSystem

# Initialize the tool
fs_tool = FileSystem()

# Create a temporary directory for the example
temp_dir_path = tempfile.mkdtemp()
print(f"Working in temporary directory: {temp_dir_path}")

# 1. Create a directory
fs_tool.create_directory(os.path.join(temp_dir_path, "my_folder"))

# 2. Write a file
fs_tool.write_file(os.path.join(temp_dir_path, "my_folder", "my_file.txt"), "Hello, world!\nAnother line.")

# 3. Replace content precisely
# The 'old_string' should be unique to avoid errors.
fs_tool.replace_in_file(
    os.path.join(temp_dir_path, "my_folder", "my_file.txt"),
    old_string="Hello, world!",
    new_string="Goodbye, world!"
)

# 4. Read the file to see the change
content = fs_tool.read_file(os.path.join(temp_dir_path, "my_folder", "my_file.txt"))
print(f"File content: {content}")

# Clean up the temporary directory
shutil.rmtree(temp_dir_path)
```

## Integration with Agents

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.filesystem import FileSystem

# 1. Initialize the FileSystem tool
fs_tool = FileSystem()

# 2. Create an agent and provide it with the file system tools
agent = Agent(
    name="filesystem_manager",
    client=OpenAIClient(api_key="YOUR_API_KEY"),
    system_prompt="You are an expert and careful file system manager.",
    tools=[
        fs_tool.list_directory,
        fs_tool.read_file,
        fs_tool.write_file,
        fs_tool.create_directory,
        fs_tool.delete_file,
        fs_tool.delete_directory,
        fs_tool.move_item,
        fs_tool.copy_file,
        fs_tool.replace_in_file,
    ]
)

# 3. Run the agent
# The agent will first read the file, then use 'replace_in_file' with enough context.
response = agent.run("In the file 'test.txt', replace the line 'Hello!' with 'Hello, precisely!'")
print(response)
```



================================================
FILE: docs/API Reference/Tools/mcp.md
================================================

# MCPClient


<!-- prettier-ignore -->
::: datapizza.tools.mcp_client.MCPClient
    options:
        show_source: false



================================================
FILE: docs/API Reference/Tools/SQLDatabase.md
================================================
# SQLDatabase

```bash
pip install datapizza-ai-tools-sqldatabase
```

<!-- prettier-ignore -->
::: datapizza.tools.SQLDatabase.SQLDatabase
    options:
        show_source: false

## Overview

The SQLDatabase tool provides a powerful interface for AI agents to interact with any SQL database supported by SQLAlchemy. This allows models to query structured, relational data to answer questions, providing more accurate and fact-based responses.

## Features

- **Broad Database Support**: Connect to any database with a SQLAlchemy driver (SQLite, PostgreSQL, MySQL, etc.).
- **Schema Inspection**: Allows the agent to view table schemas to understand data structure before querying.
- **Table Listing**: Lets the agent list all available tables to get context of the database.

## Integration with Agents

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.SQLDatabase import SQLDatabase

db_uri = "sqlite:///company.db"

# 1. Initialize the SQLDatabase tool
db_tool = SQLDatabase(db_uri=db_uri)

# 2. Create an agent and provide it with the database tool's methods
agent = Agent(
    name="database_expert",
    client=OpenAIClient(api_key="YOUR_API_KEY"),
    system_prompt="You are a database expert. Use the available tools to answer questions about the database.",
    tools=[
        db_tool.list_tables,
        db_tool.get_table_schema,
        db_tool.run_sql_query
    ]
)

# 3. Run the agent
response = agent.run("How many people work in the Engineering department?")
print(response)
```



================================================
FILE: docs/API Reference/Tools/web_fetch.md
================================================
# WebFetch

```bash
pip install datapizza-ai-tools-web-fetch
```

<!-- prettier-ignore -->
::: datapizza.tools.web_fetch.base.WebFetchTool
    options:
        show_source: false

## Overview

The WebFetch tool provides a simple and robust way for AI agents to retrieve content from a given URL. It allows models to access live information from the internet, which is crucial for tasks requiring up-to-date data.

## Features

- **Live Web Access**: Fetches content from any public URL.
- **Error Handling**: Gracefully handles common HTTP errors (e.g., timeouts, 404, 503) and reports them clearly.
- **Configurable**: Allows setting custom timeouts and User-Agent strings.
- **Simple Integration**: As a callable tool, it integrates seamlessly with `datapizza-ai` agents.

## Usage Example

```python
from datapizza.tools.web_fetch import WebFetchTool

# Initialize the tool
fetch_tool = WebFetchTool()

# Fetch content from a URL by calling the tool instance
content = fetch_tool("https://example.com")

print(content)
```

## Integration with Agents

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools.web_fetch import WebFetchTool

# 1. Initialize the WebFetchTool, optionally with a custom timeout
web_tool = WebFetchTool(timeout=15.0)

# 2. Create an agent and provide it with the tool
agent = Agent(
    name="web_researcher",
    client=OpenAIClient(api_key="YOUR_API_KEY"),
    system_prompt="You are a research assistant. Use the web_fetch tool to get information from URLs to answer questions.",
    tools=[web_tool]
)

# 3. Run the agent to summarize a web page
response = agent.run("Please summarize the content of https://loremipsum.io/")
print(response.text)
```



================================================
FILE: docs/API Reference/Tools/.pages
================================================
title: Tools
nav:
  - mcp.md
  - duckduckgo.md
  - filesystem.md
  - SQLDatabase.md
  - web_fetch.md



================================================
FILE: docs/API Reference/Type/block.md
================================================

# Blocks

<!-- prettier-ignore -->
::: datapizza.type.Block
    options:
        show_source: false


<!-- prettier-ignore -->
::: datapizza.type.TextBlock
    options:
        show_source: false


<!-- prettier-ignore -->
::: datapizza.type.MediaBlock
    options:
        show_source: false


<!-- prettier-ignore -->
::: datapizza.type.ThoughtBlock
    options:
        show_source: false


<!-- prettier-ignore -->
::: datapizza.type.FunctionCallBlock
    options:
        show_source: false

<!-- prettier-ignore -->
::: datapizza.type.FunctionCallResultBlock
    options:
        show_source: false


<!-- prettier-ignore -->
::: datapizza.type.StructuredBlock
    options:
        show_source: false



================================================
FILE: docs/API Reference/Type/chunk.md
================================================
# Chunk

<!-- prettier-ignore -->
::: datapizza.type.Chunk
    options:
        show_source: false

## Overview

The `Chunk` class represents a unit of text content that has been segmented from a larger document. It's a fundamental data structure in datapizza-ai used throughout the RAG pipeline for text processing, embedding, and retrieval operations.
 **Serializable**: Can be easily stored and retrieved from databases



================================================
FILE: docs/API Reference/Type/media.md
================================================

<!-- prettier-ignore -->
::: datapizza.type.Media
    options:
        show_source: false



================================================
FILE: docs/API Reference/Type/node.md
================================================

<!-- prettier-ignore -->
::: datapizza.type.Node
    options:
        show_source: false



<!-- prettier-ignore -->
::: datapizza.type.MediaNode
    options:
        show_source: false



================================================
FILE: docs/API Reference/Type/tool.md
================================================

<!-- prettier-ignore -->
::: datapizza.tools.Tool
    options:
        show_source: false



================================================
FILE: docs/API Reference/Vectorstore/milvus_vectorstore.md
================================================
# Milvus



```bash
pip install datapizza-ai-vectorstores-milvus
```

<!-- prettier-ignore
::: datapizza.vectorstores.milvus.MilvusVectorstore
    options:
        show_source: false

 -->


## Usage
```python
from datapizza.vectorstores.milvus import MilvusVectorstore

# Option A) Milvus Server / Zilliz Cloud
vectorstore = MilvusVectorstore(
    host="localhost",
    port=19530,
    # user="username",            # Optional
    # password="password",        # Optional
    # secure=True,                # Optional (TLS)
    # token="zilliz_token",       # Optional (Zilliz)
)

# Option B) Single-URI style (works with Milvus, Zilliz, or Milvus Lite)
vectorstore = MilvusVectorstore(uri="./milvus.db")  # Milvus Lite

```

## Features

- Connect via `host/port` or a single `uri` (supports Milvus Server, Zilliz Cloud, and Milvus Lite).
- Works with **dense** and **sparse** embeddings in the *same* collection.
- Named vector fields for **multi-vector** collections.
- Batch/async operations: `add` / `a_add`, `search` / `a_search`.
- Collection management: `create_collection`, `delete_collection`, `get_collections`.
- Entities ops: `retrieve`, `update` (upsert), `remove`.
- Flexible indexing (defaults provided; accepts custom `IndexParams`).
- Dynamic metadata via Milvusâ€™ `$meta` (stored from `Chunk.metadata`).

---

## Examples

### Basic Setup & Collection Creation

```python
import uuid
from datapizza.core.vectorstore import Distance, VectorConfig
from datapizza.type import EmbeddingFormat
from datapizza.vectorstores.milvus import MilvusVectorstore


# Local Lite (file) for quick dev
vectorstore = MilvusVectorstore(uri="./milvus.db")

# Declare your vector fields (can be multiple)
vector_config = [
    VectorConfig(
        name="text_embeddings",
        dimensions=3,
        format=EmbeddingFormat.DENSE,
        distance=Distance.COSINE,
    )
]

# Create (id, text, and above vectors; dynamic metadata enabled)
vectorstore.create_collection(
    collection_name="documents",
    vector_config=vector_config
)

```



### Add Chunks & Search

```python
from datapizza.type import Chunk, DenseEmbedding

chunks = [
    Chunk(
        id=str(uuid.uuid4()),
        text="First document content",
        metadata={"source": "doc1.txt"},
        embeddings=[DenseEmbedding(name="text_embeddings", vector=[0.1, 0.2, 0.3])]
    ),

    Chunk(
        id=str(uuid.uuid4()),
        text="Second document content",
        metadata={"source": "doc2.txt"},
        embeddings=[DenseEmbedding(name="text_embeddings", vector=[0.4, 0.5, 0.6])]
    ),
]

vectorstore.add(chunks, collection_name="documents")

# IMPORTANT: provide the vector field name if not using an Embedding instance
results = vectorstore.search(
    collection_name="documents",
    query_vector=[0.1, 0.2, 0.3],
    vector_name="text_embeddings",
    k=5
)

for chunk in results:
    print(chunk.id, chunk.text, chunk.metadata)

```



### Retrieve, Update (Upsert), Remove

```python
# Suppose you kept an id from earlier
target_id = chunks[0].id

# Retrieve by ids
found = vectorstore.retrieve(collection_name="documents", ids=[target_id])
print("Retrieved:", found[0].text)

updated = Chunk(
    id=target_id,
    text="First document content (updated)",
    metadata={"source": "doc1.txt", "version": 2},
    embeddings=[DenseEmbedding(name="text_embeddings", vector=[0.11, 0.19, 0.31])],
)

vectorstore.update(collection_name="documents", chunk=updated)

vectorstore.remove(collection_name="documents", ids=[target_id])

```



### Async API

```python
import asyncio
from datapizza.type import DenseEmbedding

async def main():
    vs = MilvusVectorstore(uri="./milvus.db")
    more = [
        Chunk(
            id=str(uuid.uuid4()),
            text="Async doc",
            embeddings=[DenseEmbedding(name="text_embeddings", vector=[0.2, 0.1, 0.9])]
        )
    ]
    await vs.a_add(more, collection_name="documents")

    hits = await vs.a_search(
        collection_name="documents",
        query_vector=DenseEmbedding(name="text_embeddings", vector=[0.2, 0.1, 0.9]),
        k=3,
    )
    print([h.text for h in hits])

asyncio.run(main())
```

### Multi-Vector (Dense + Sparse)

```python
from datapizza.core.vectorstore import VectorConfig
from datapizza.type import Chunk, DenseEmbedding, SparseEmbedding, EmbeddingFormat
from datapizza.vectorstores.milvus import MilvusVectorstore
import uuid

vectorstore = MilvusVectorstore()
# You can also specify advanced IndexParams
index_params = vectorstore.prepare_index_params()

index_params.add_index(
    field_name="dense_vector", # Name of the vector field to be indexed
    index_type="GPU_IVF_FLAT", # Type of the index to create
    index_name="dense_vector_index", # Name of the index to create
    metric_type="L2", # Metric type used to measure similarity
    params={
        "nlist": 1024, # Number of clusters for the index
    } # Index building params
)

index_params.add_index(
    field_name="sparse_vector", # Name of the vector field to be indexed
    index_type="SPARSE_INVERTED_INDEX", # Type of the index to create
    index_name="sparse_vector_index", # Name of the index to create
    metric_type="IP", # Metric type used to measure similarity
    params={"inverted_index_algo": "DAAT_MAXSCORE"}, # Algorithm used for building and querying the index
)
# Create with dense + sparse vector fields

vectorstore.create_collection(
    collection_name="hybrid_docs",
    vector_config=[
        VectorConfig(
            name="dense_vector",
            dimensions=1024,
            format=EmbeddingFormat.DENSE,
        ),
        VectorConfig(
            name="sparse_vector",
            dimensions=0,  # ignored by Milvus for sparse
            format=EmbeddingFormat.SPARSE,
        ),
    ],
    index_params=index_params  # index params created earlier
)

hybrid = Chunk(
    id=str(uuid.uuid4()),
    text="Hybrid vector example",
    metadata={"lang": "en"},
    embeddings=[
        DenseEmbedding(name="dense_vector", vector=[0.01]*1024),
        SparseEmbedding(name="sparse_vector", indices=[1, 7, 42], values=[0.9, 0.5, 0.2]),
    ],
)

vectorstore.add(hybrid, collection_name="hybrid_docs")

# Dense search
vectorstore.search(
    collection_name="hybrid_docs",
    query_vector=DenseEmbedding(name="dense_vector", vector=[0.01]*1024),
    k=5,
)

# Sparse search
vectorstore.search(
    collection_name="hybrid_docs",
    query_vector=SparseEmbedding(name="sparse_vector", indices=[1,7], values=[1.0,0.6]),
    k=5,
)
```


### Dump a Collection (Pagination Helper

```python
for chunk in vectorstore.dump_collection("documents", page_size=100):
    print(chunk.id, chunk.text, chunk.metadata)
```



================================================
FILE: docs/API Reference/Vectorstore/qdrant_vectorstore.md
================================================
# Qdrant

```python
pip install datapizza-ai-vectorstores-qdrant
```

<!-- prettier-ignore -->
::: datapizza.vectorstores.qdrant.QdrantVectorstore
    options:
        show_source: false


## Usage

```python
from datapizza.vectorstores.qdrant import QdrantVectorstore

# Connect to Qdrant server
vectorstore = QdrantVectorstore(
    host="localhost",
    port=6333,
    api_key="your-api-key"  # Optional
)

# Or use in-memory/file storage
vectorstore = QdrantVectorstore(
    location=":memory:"  # Or path to file
)
```

## Features

- Connect to Qdrant server or use local storage
- Support for both dense and sparse embeddings
- Named vector configurations for multi-vector collections
- Batch operations for efficient processing
- Collection management (create, delete, list)
- Chunk-based operations with metadata preservation
- Async support for all operations
- Point-level operations (add, update, remove, retrieve)

## Examples

### Basic Setup and Collection Creation

```python
from datapizza.core.vectorstore import Distance, VectorConfig
from datapizza.type import EmbeddingFormat
from datapizza.vectorstores.qdrant import QdrantVectorstore

vectorstore = QdrantVectorstore(location=":memory:")

# Create collection with vector configuration
vector_config = [
    VectorConfig(
        name="text_embeddings",
        dimensions=3,
        format=EmbeddingFormat.DENSE,
        distance=Distance.COSINE
    )
]

vectorstore.create_collection(
    collection_name="documents",
    vector_config=vector_config
)

# Add nodes and search

import uuid
from datapizza.type import Chunk, DenseEmbedding
from datapizza.vectorstores.qdrant import QdrantVectorstore

# Create chunks with embeddings
chunks = [
    Chunk(
        id=str(uuid.uuid4()),
        text="First document content",
        metadata={"source": "doc1.txt"},
        embeddings=[DenseEmbedding(name="text_embeddings", vector=[0.1, 0.2, 0.3])]
    ),
    Chunk(
        id=str(uuid.uuid4()),
        text="Second document content",
        metadata={"source": "doc2.txt"},
        embeddings=[DenseEmbedding(name="text_embeddings", vector=[0.4, 0.5, 0.6])]
    )
]

# Add chunks to collection
vectorstore.add(chunks, collection_name="documents")

# Search for similar chunks
query_vector = [0.1, 0.2, 0.3]
results = vectorstore.search(
    collection_name="documents",
    query_vector=query_vector,
    k=5
)

for chunk in results:
    print(f"Text: {chunk.text}")
    print(f"Metadata: {chunk.metadata}")
```



================================================
FILE: docs/Guides/.pages
================================================
nav:
    - Clients
    - Agents
    - RAG
    - Pipeline
    - Monitoring



================================================
FILE: docs/Guides/Agents/agent.md
================================================
# Build your first agent

The `Agent` class is the core component for creating autonomous AI agents in Datapizza AI. It handles task execution, tool management, memory, and planning.

## Basic Usage

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.memory import Memory
from datapizza.tools import tool

agent = Agent(
    name="my_agent",
    system_prompt="You are a helpful assistant",
    client=OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4o-mini"),
    # tools=[],
    # max_steps=10,
    # terminate_on_text=True,  # Terminate execution when the client return a plain text
    # memory=memory,
    # stream=False,
    # planning_interval=0
)

res = agent.run("Hi")
print(res.text)
```


## Use Tools

The above agent is quite basic, so let's make it more functional by adding [**tools**](../../API%20Reference/Type/tool.md).

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.memory import Memory
from datapizza.tools import tool

@tool
def get_weather(location: str, when: str) -> str:
    """Retrieves weather information for a specified location and time."""
    return "25 Â°C"

agent = Agent(name="weather_agent", tools=[get_weather], client=OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4o-mini"))
response = agent.run("What's the weather tomorrow in Milan?")

print(response.text)
# Output:
# Tomorrow in Milan, the temperature will be 25 Â°C.
```


### tool_choice

You can set the parameter `tool_choice` at invoke time.

The accepted values are: `auto`, `required`, `none`, `required_first`, `list["tool_name"]`


```python
res = master_agent.run(
    task_input="what is the weather in milan?", tool_choice="required_first"
)
```

- `auto`: the model will decide if use a tool or not.
- `required_first`: force to use a tool only at the first step, then auto.
- `required`: force to use a tool at every step.
- `none`: force to not use any tool.



## Core Methods


### Sync run

`run(task_input: str, tool_choice = "auto", **kwargs) -> str`
Execute a task and return the final result.

```python
result = agent.run("What's the weather like today?")
print(result.text)  # "The weather is sunny with 25Â°C"
```

### Stream invoke
Stream the agent's execution process, yielding intermediate steps. (Do not stream the single answer)

```python
from datapizza.agents.agent import Agent, StepResult
from datapizza.clients.openai import OpenAIClient
from datapizza.memory import Memory
from datapizza.tools import tool

@tool
def get_weather(location: str, when: str) -> str:
    """Retrieves weather information for a specified location and time."""
    return "25 Â°C"

agent = Agent(name="weather_agent", tools=[get_weather], client=OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4o-mini"))

for step in agent.stream_invoke("What's the weather tomorrow in Milan?"):
    print(f"Step {step.index} starting...")
    print(step.text)
```

### Async run

`a_run(task_input: str, **kwargs) -> str`
Async version of run.

```python
import asyncio

async def main():

    agent = Agent(name="agent", client=OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4o-mini"))
    return await agent.a_run("Process this request")


res = asyncio.run(main())
print(res.text)
```

### Async stream invoke
`a_stream_invoke(task_input: str, **kwargs) -> AsyncGenerator[str | StepResult, None]`
Stream the agent's execution process, yielding intermediate steps. (Do not stream the single answer)

```python
from datapizza.agents.agent import Agent
from datapizza.clients.openai import OpenAIClient
import asyncio

async def get_response():
    client = OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4o-mini")
    agent = Agent(name= "joke_agent",client=client)
    async for step in agent.a_stream_invoke("tell me a joke"):
        print(f"Step {step.index} starting...")
        print(step.text)

asyncio.run(get_response())
```


## Multi-Agent Communication

An agent can call another ones using `can_call` method


```python
from datapizza.agents.agent import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools import tool

client = OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4.1")

@tool
def get_weather(city: str) -> str:
    return f""" Monday's weather in {city} is cloudy.
                Tuesday's weather in {city} is rainy.
                Wednesday's weather in {city} is sunny
                Thursday's weather in {city} is cloudy,
                Friday's weather in {city} is rainy,
                Saturday's weather in {city} is sunny
                and Sunday's weather in {city} is cloudy."""

weather_agent = Agent(
    name="weather_expert",
    client=client,
    system_prompt="You are a weather expert. Provide detailed weather information and forecasts.",
    tools=[get_weather]
)

planner_agent = Agent(
    name="planner",
    client=client,
    system_prompt="You are a trip planner. Use weather and analysis info to make recommendations."
)

planner_agent.can_call(weather_agent)

response = planner_agent.run(
    "I need to plan a hiking trip in Seattle next week. Can you help analyze the weather and make recommendations?"
)
print(response.text)
```

Alternatively, you can define a tool that manually calls the agent.
The two solutions are more or less identical.

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.tools import tool

class MasterAgent(Agent):
    system_prompt="You are a master agent. You can call the weather expert to get weather information."
    name="master_agent"

    @tool
    def call_weather_expert(self, task_to_ask: str) -> str:
        @tool
        def get_weather(city: str) -> str:
            return f""" Monday's weather in {city} is cloudy.
                        Tuesday's weather in {city} is rainy.
                        Wednesday's weather in {city} is sunny
                        Thursday's weather in {city} is cloudy,
                        Friday's weather in {city} is rainy,
                        Saturday's weather in {city} is sunny
                        and Sunday's weather in {city} is cloudy."""

        weather_agent = Agent(
            name="weather_expert",
            client=OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4.1"),
            system_prompt="You are a weather expert. Provide detailed weather information and forecasts.",
            tools=[get_weather]
        )
        res = weather_agent.run(task_to_ask)
        return res.text

master_agent = MasterAgent(
    client=OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4.1"),
)

master_agent.run("What is the weather in Rome?")
```



## Planning System

When `planning_interval > 0`, the agent creates execution plans at regular intervals:

During the planning stages, the agent spends time thinking about what the next steps are to be taken to achieve the task.

```python
agent = Agent(
    name="Agent_with_plan",
    client=client,
    planning_interval=3,  # Plan every 3 steps
)
```

The planning system generates structured plans that help the agent organize complex tasks.


## Stream output response

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient
from datapizza.core.clients import ClientResponse
from datapizza.tools import tool

client = OpenAIClient(api_key="YOUR_API_KEY", model="gpt-4.1")

agent = Agent(
    name="Big_boss",
    client=client,
    system_prompt="You are a helpful assistant that answers questions based on the provided context.",
    stream=True, # With stream=True, the agent will stream the client resposne, not only the intermediate steps

)

for r in agent.stream_invoke("What is the weather in Milan?"):
    if isinstance(r, ClientResponse):
        print(r.delta, end="", flush=True)
```



================================================
FILE: docs/Guides/Agents/mcp.md
================================================
# Model Context Protocol (MCP)

Model Context Protocol (MCP) is an open-source standard that enables AI applications to connect with external systems like databases, APIs, and tools.

Use MCP (Model Context Protocol) tools inside `datapizza-ai` by wrapping them as
regular agent tools. Follow this minimal recipe to get an agent talking to a
remote MCP server in just a few steps.


With MCP, you can build AI agents that:

- **Access your codebase**: Let AI read GitHub repositories, create issues, and manage pull requests
- **Query your database**: Enable natural language queries against PostgreSQL, MySQL, or any database
- **Browse the web**: Give AI the ability to search and extract information from websites
- **Control your tools**: Connect to Slack, Notion, Google Calendar, or any API-based service
- **Analyze your data**: Let AI work with spreadsheets, documents, and business intelligence tools

## Fetch MCP tools

Here an example of [FastMCP](https://gofastmcp.com/getting-started/welcome) tool provided by FastMCP

```python
from datapizza.tools.mcp_client import MCPClient

fastmcp_client = MCPClient(url="https://gofastmcp.com/mcp")
fastmcp_tools = fastmcp_client.list_tools()
```

## Create the agent and run it

```python
from datapizza.agents import Agent
from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4o-mini")

agent = Agent(
    name="mcp_agent",
    client=client,
    tools=fastmcp_tools,
)

result = agent.run("How can I use a FastMCP server over HTTP?")
print(result.text)
```

Thatâ€™s itâ€”you now have an agent that discovers tools from the FastMCP server and
uses them as part of normal `datapizza-ai` reasoning. Swap in any MCP endpoint
or different LLM client to match your project.



================================================
FILE: docs/Guides/Clients/chatbot.md
================================================
# Real example: Chatbot

Learn how to build conversational AI applications using the OpenAI client with memory management, context awareness, and advanced chatbot patterns.

## Basic Chatbot


Clients need memory to maintain context and have meaningful conversations. The Memory class stores and manages conversation history, allowing the AI to reference previous exchanges and maintain coherent dialogue.

Here's a simple example of a chatbot with memory:

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.memory import Memory
from datapizza.type import ROLE, TextBlock

client = OpenAIClient(
    api_key="your-api-key",
    model="gpt-4o-mini",
    system_prompt="You are a helpful assistant"
)

def simple_chatbot():
    """Basic chatbot with conversation memory."""

    memory = Memory()

    print("Chatbot: Hello! I'm here to help. Type 'quit' to exit.")

    while True:
        user_input = input("\nYou: ")

        if user_input.lower() in ['quit', 'exit', 'bye']:
            print("Chatbot: Goodbye!")
            break

        # Get AI response with memory context
        response = client.invoke(user_input, memory=memory)
        print(f"Chatbot: {response.text}")

        # Update conversation memory
        memory.add_turn(TextBlock(content=user_input), role=ROLE.USER)
        memory.add_turn(response.content, role=ROLE.ASSISTANT)

# Run the chatbot
simple_chatbot()
```



================================================
FILE: docs/Guides/Clients/local_model.md
================================================
# Running with Ollama

Datapizza AI supports running with local models through Ollama, providing you with complete control over your AI infrastructure while maintaining privacy and reducing costs.

## Prerequisites

Before getting started, you'll need to have Ollama installed and running on your system.

### Installing Ollama

1. **Download and Install Ollama**
   - Visit [ollama.ai](https://ollama.ai) and download the installer for your operating system
   - Follow the installation instructions for your platform

2. **Start Ollama Service**
   ```bash
   # Ollama typically starts automatically after installation
   # If not, you can start it manually:
   ollama serve
   ```

3. **Pull a Model**
   ```bash
   # Pull the Gemma 2B model (lightweight option)
   ollama pull gemma2:2b

   # Or pull Gemma 7B for better performance
   ollama pull gemma2:7b

   # Or pull Llama 3.1 8B
   ollama pull llama3.1:8b
   ```

## Installation

Install the Datapizza AI OpenAI-like client:

```bash
pip install datapizza-ai-clients-openai-like
```

## Basic Usage

Here's a simple example of how to use Datapizza AI with Ollama:

```python
import os
from datapizza.clients.openai_like import OpenAILikeClient
from dotenv import load_dotenv

load_dotenv()

# Create client for Ollama
client = OpenAILikeClient(
    api_key="",  # Ollama doesn't require an API key
    model="gemma2:2b",  # Use any model you've pulled with Ollama
    system_prompt="You are a helpful assistant.",
    base_url="http://localhost:11434/v1",  # Default Ollama API endpoint
)

# Simple query
response = client.invoke("What is the capital of France?")
print(response.content)
```



================================================
FILE: docs/Guides/Clients/multimodality.md
================================================
# Multimodality

The clients supports various media types including images and PDFs, allowing you to create rich multimodal applications.

## Supported Media Types

| Media Type | Supported Formats | Source Types |
|------------|------------------|--------------|
| Images | PNG, JPEG, GIF, WebP | File path, URL, base64 |
| PDFs | PDF documents | File path, base64 |

## Basic Image Input

### Single Image from File

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.type import Media, MediaBlock, TextBlock

client = OpenAIClient(
    api_key="your-api-key",
    model="gpt-4o"  # Vision models required for images
)

# Create image media object
image = Media(
    media_type="image",
    source_type="path",
    source="image.png", # Use the correct path
    extension="png"
)

# Create media block
media_block = MediaBlock(media=image)
text_block = TextBlock(content="What do you see in this image?")

# Send multimodal input
response = client.invoke(
    input=[text_block, media_block],
    max_tokens=200
)

print(response.text)
```

### Image from URL

```python
# Image from URL
image_url = Media(
    media_type="image",
    source_type="url",
    source="https://example.com/image.png",
    extension="png"
)

response = client.invoke(
    input=[
        TextBlock(content="Describe this image"),
        MediaBlock(media=image_url)
    ]
)
print(response.text)
```

### Image from Base64

```python
import base64

# Read and encode image
with open("image.jpg", "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode('utf-8')

image_b64 = Media(
    media_type="image",
    source_type="base64",
    source=base64_image,
    extension="png"
)

response = client.invoke(
    input=[
        TextBlock(content="Analyze this image"),
        MediaBlock(media=image_b64)
    ]
)
print(response.text)
```

## Multiple Images

Compare or analyze multiple images in a single request:

```python
# Multiple images for comparison
image1 = Media(
    media_type="image",
    source_type="path",
    source="before.png",
    extension="png"
)

image2 = Media(
    media_type="image",
    source_type="path",
    source="after.png",
    extension="png"
)

response = client.invoke(
    input=[
        TextBlock(content="Compare these two images and describe the differences"),
        MediaBlock(media=image1),
        MediaBlock(media=image2)
    ],
    max_tokens=300
)

print(response.text)
```

## Working with PDFs

```python
# PDF from file path
pdf_doc = Media(
    media_type="pdf",
    source_type="path",
    source="document.pdf",
    extension="pdf"
)

response = client.invoke(
    input=[
        TextBlock(content="Summarize the key points from this document"),
        MediaBlock(media=pdf_doc)
    ],
    max_tokens=500
)

print(response.text)
```


## Working with Audio


Google handle audio inline

```sh
pip install datapizza-ai-clients-google
```

```python
from datapizza.clients.google import GoogleClient
from datapizza.type import Media, MediaBlock, TextBlock

client = GoogleClient(
    api_key="YOUR_API_KEY",
    model="gemini-2.0-flash-exp"
)
# PDF from file path
media = Media(
    media_type="audio",
    source_type="path",
    source="sample.mp3",
    extension="mp3"
)

response = client.invoke(
    input=[
        TextBlock(content="Summarize the key points from this audio file"),
        MediaBlock(media=media)
    ],
)

print(response.text)
```



================================================
FILE: docs/Guides/Clients/quick_start.md
================================================
# Quick Start

This guide will help you get started with the `OpenAIClient` in datapizza-ai. For specialized topics, check out our detailed guides on [multimodality](multimodality.md), [streaming](streaming.md) and [building chatbots](chatbot.md).

## Installation

First, make sure you have datapizza-ai installed:

```bash
pip install datapizza-ai
```

## Basic Setup


```python
from datapizza.clients.openai import OpenAIClient

# Initialize the client with your API key
client = OpenAIClient(
    api_key="your-openai-api-key",
    model="gpt-4o-mini",  # Default model
    system_prompt="You are a helpful assistant",  # Optional
    temperature=0.7  # Optional, controls randomness (0-2)
)
```


```python
# Basic text response
response = client.invoke("What is the capital of France?")
print(response.text)
# Output: "The capital of France is Paris."
```

## Core Methods


```python
response = client.invoke(
    input="Explain quantum computing in simple terms",
    temperature=0.5,  # Override default temperature
    max_tokens=200,   # Limit response length
    system_prompt="You are a physics teacher"  # Override system prompt
)

print(response.text)
print(f"Tokens used: {response.completion_tokens_used}")
```


## Async invoke

```python
import asyncio

async def main():
    return await client.a_invoke(
        input="Explain quantum computing in simple terms",
        temperature=0.5,  # Override default temperature
        max_tokens=200,   # Limit response length
        system_prompt="You are a physics teacher"  # Override system prompt
    )

response = asyncio.run(main())

print(response.text)
print(f"Tokens used: {response.completion_tokens_used}")
```
## Working with Memory

Memory allows you to maintain conversation context:

```python
from datapizza.memory import Memory
from datapizza.type import ROLE, TextBlock

memory = Memory()

# First interaction
response1 = client.invoke("My name is Alice", memory=memory)
memory.add_turn(TextBlock(content="My name is Alice"), role=ROLE.USER)
memory.add_turn(response1.content, role=ROLE.ASSISTANT)

# Second interaction - the model remembers Alice
response2 = client.invoke("What's my name?", memory=memory)
print(response2.text)  # Should mention Alice
```

## Token Management
Monitor your usage:

```python
response = client.invoke("Explain AI")
print(f"Tokens used: {response.completion_tokens_used}")
print(f"Prompt token used: {response.prompt_tokens_used}")
print(f"Cached token used: {response.cached_tokens_used}")
```

That's it! You're ready to start building with the OpenAI client. Check out the specialized guides above for advanced features and patterns.



## What's Next?

Now that you know the basics, explore our specialized guides:

### ğŸ“¸ [Multimodality Guide](multimodality.md)
Work with images, PDFs, and other media types for visual AI applications.

### ğŸŒŠ [Streaming Guide](streaming.md)
Build responsive applications with real-time text generation and streaming.

### ğŸ› ï¸ [Tools Guide](tools.md)
Extend AI capabilities by integrating external functions and tools.

### ğŸ“Š [Structured Responses Guide](structured_responses.md)
Work with strongly-typed outputs using JSON schemas and Pydantic models.

### ğŸ¤– [Chatbot Guide](chatbot.md)
Create sophisticated conversational AI with memory and context management.



================================================
FILE: docs/Guides/Clients/streaming.md
================================================
# Streaming

Streaming allows you to receive responses in real-time as they're generated, providing a better user experience for long responses and interactive applications.

## Why Use Streaming?

- **Real-time feedback**: Users see responses as they're generated
- **Better UX**: Reduces perceived latency for long responses
- **Progressive display**: Show partial results immediately
- **Interruptible**: Can stop generation early if needed

## Basic Streaming

### Synchronous Streaming

```python
from datapizza.clients.openai import OpenAIClient

client = OpenAIClient(
    api_key="your-api-key",
    model="gpt-4o-mini"
)

# Basic streaming
for chunk in client.stream_invoke("Write a short story about a robot learning to paint"):
    if chunk.delta:
        print(chunk.delta, end="", flush=True)
print()  # New line when complete
```

### Asynchronous Streaming

```python
import asyncio

async def async_stream_example():
    async for chunk in client.a_stream_invoke("Explain quantum computing in simple terms"):
        if chunk.delta:
            print(chunk.delta, end="", flush=True)
    print()  # New line when complete

# Run the async function
asyncio.run(async_stream_example())
```



================================================
FILE: docs/Guides/Clients/structured_responses.md
================================================
# Structured Responses

Generate structured, typed data from AI responses using Pydantic models. This ensures consistent output format and enables easy data validation.

## Basic Usage

### Simple Model

```python
from datapizza.clients.openai import OpenAIClient
from pydantic import BaseModel

client = OpenAIClient(api_key="your-api-key", model="gpt-4o-mini")

class Person(BaseModel):
    name: str
    age: int
    occupation: str

response = client.structured_response(
    input="Create a profile for a software engineer",
    output_cls=Person
)

person = response.structured_data[0]
print(f"Name: {person.name}")
print(f"Age: {person.age}")
print(f"Occupation: {person.occupation}")
```

### Complex Models

```python
from typing import List
from pydantic import BaseModel, Field

class Product(BaseModel):
    name: str
    price: float = Field(gt=0, description="Price must be positive")
    tags: List[str]
    in_stock: bool

class Store(BaseModel):
    name: str
    location: str
    products: List[Product]

response = client.structured_response(
    input="Create a tech store with 3 products",
    output_cls=Store
)

store = response.structured_data[0]
print(f"Store: {store.name}")
for product in store.products:
    print(f"- {product.name}: ${product.price}")
```

## Data Extraction

### Extract Information from Text

```python
class ContactInfo(BaseModel):
    name: str
    email: str
    phone: str
    company: str

text = """
Hi, I'm John Smith from TechCorp.
You can reach me at john.smith@techcorp.com or call 555-0123.
"""

response = client.structured_response(
    input=f"Extract contact information from this text: {text}",
    output_cls=ContactInfo
)

contact = response.structured_data[0]
print(f"Contact: {contact.name} at {contact.company}")
```

### Analyze and Categorize

```python
from enum import Enum

class Sentiment(str, Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"

class TextAnalysis(BaseModel):
    sentiment: Sentiment
    confidence: float = Field(ge=0, le=1)
    key_topics: List[str]
    summary: str

review = "This product is amazing! Great quality and fast shipping."

response = client.structured_response(
    input=f"Analyze this review: {review}",
    output_cls=TextAnalysis
)

analysis = response.structured_data[0]
print(f"Sentiment: {analysis.sentiment}")
print(f"Confidence: {analysis.confidence}")
print(f"Topics: {', '.join(analysis.key_topics)}")
```



================================================
FILE: docs/Guides/Clients/tools.md
================================================
# Tools

Tools allow AI models to call external functions, enabling them to perform actions, retrieve data, and interact with external systems.

## Basic Tool Usage

### Simple Tool

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.tools import tool

client = OpenAIClient(api_key="your-api-key", model="gpt-4o-mini")

@tool
def get_weather(location: str) -> str:
    """Get the current weather for a location."""
    # Simulate weather API call
    return f"The weather in {location} is sunny and 72Â°F"

# Use the tool
response = client.invoke(
    "What's the weather in New York?",
    tools=[get_weather]
)

# Execute tool calls
for func_call in response.function_calls:
    result = func_call.tool(**func_call.arguments)
    print(f"Tool result: {result}")

print(response.text)
```

### Multiple Tools

```python
@tool
def calculate(expression: str) -> str:
    """Calculate a mathematical expression."""
    try:
        result = eval(expression)  # Note: Use safe evaluation in production
        return str(result)
    except Exception as e:
        return f"Error: {e}"

@tool
def get_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Use multiple tools
response = client.invoke(
    "What time is it and what's 15 * 8?",
    tools=[get_time, calculate]
)

# Execute all tool calls
for func_call in response.function_calls:
    result = func_call.tool(**func_call.arguments)
    print(f"{func_call.name}: {result}")
```

## Tool Choice Control

### Auto (Default)
Let the model decide when to use tools:

```python
response = client.invoke(
    "Hello, how are you?",
    tools=[get_weather],
    tool_choice="auto"  # Model may or may not use tools
)
```

### Required
Force the model to use a tool:

```python
response = client.invoke(
    "Get weather information",
    tools=[get_weather],
    tool_choice="required"  # Model must use a tool
)
```

### None
Disable tool usage:

```python
response = client.invoke(
    "What's the weather like?",
    tools=[get_weather],
    tool_choice="none"  # Model won't use tools
)
```

### Specific Tool
Force a specific tool:


```python
response = client.invoke(
    "Check the weather",
    tools=[get_weather, get_time],
    tool_choice=["get_weather"]  # Only use this specific tool
)
```



================================================
FILE: docs/Guides/Clients/.pages
================================================
nav:
    - quick_start.md
    - multimodality.md
    - structured_responses.md
    - streaming.md
    - tools.md
    - chatbot.md
    - local_model.md



================================================
FILE: docs/Guides/Monitoring/log.md
================================================

# Log level

With the variables `DATAPIZZA_LOG_LEVEL` and `DATAPIZZA_AGENT_LOG_LEVEL` you can change the log levels of the master logger and the agent logger

Allowed values are:

- `DEBUG`
- `INFO`
- `WARN`
- `ERROR`

The default values are:

- `DATAPIZZA_LOG_LEVEL=INFO`
- `DATAPIZZA_AGENT_LOG_LEVEL=INFO`



================================================
FILE: docs/Guides/Monitoring/tracing.md
================================================
# Tracing

The tracing module provides an easy-to-use interface for collecting and displaying OpenTelemetry traces with rich console output. It's designed to help developers monitor performance and understand the execution flow of their applications.

## Features

- **In-memory trace collection** - Stores spans in memory for fast access
- **Context-aware tracking** - Only collects spans for explicitly tracked operations
- **Thread-safe operations** - Safe for use in multi-threaded applications
- **OpenTelemetry integration** - Works with standard OpenTelemetry instrumentation

## Quick Start


The simplest way to use tracing is with the `tracer` context manager:

```python
from datapizza.tracing import ContextTracing


# Basic tracing
with ContextTracing().trace("trace_name"):
    # Your code here
    result = some_datapizza_operations()

# Output will show:
# â•­â”€ Trace Summary of my_operation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•®
# â”‚ Total Spans: 3                                                    â”‚
# â”‚ Duration: 2.45s                                                   â”‚
# â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
# â”‚ â”ƒ Model       â”ƒ Prompt Tokens â”ƒ Completion Tokens â”ƒ Cached Tokens â”ƒ
# â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
# â”‚ â”‚ gpt-4o-mini â”‚ 31            â”‚ 27                â”‚ 0             â”‚
# â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

## Clients trace input/output/memory

If you want to log the input/output and the memory passed to client invoke you should set the env variable

`DATAPIZZA_TRACE_CLIENT_IO=TRUE`

default is `FALSE`



## Manual Span Creation

For more granular control, create spans manually:

```python
from opentelemetry import trace
from datapizza.tracing import ContextTracing

tracer = trace.get_tracer(__name__)

with ContextTracing().trace("trace_name"):
    with tracer.start_as_current_span("database_query"):
        # Database operation
        data = fetch_from_database()

    with tracer.start_as_current_span("data_validation"):
        # Validation logic
        validate_data(data)

    with tracer.start_as_current_span("business_logic"):
        # Core business logic
        result = process_business_rules(data)
```


## Adding External Exporters

The tracing module uses in-memory storage by default, but you can easily add external exporters to send traces to other systems.

### Create the resource

First of all you should set the trace provider


```python
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, SimpleSpanProcessor
from opentelemetry.semconv.resource import ResourceAttributes

resource = Resource.create(
   {
       ResourceAttributes.SERVICE_NAME: "your_service_name",
   }
)
trace.set_tracer_provider(TracerProvider(resource=resource))


```


### Zipkin Integration

Export traces to Zipkin for visualization and analysis:

`pip install opentelemetry-exporter-zipkin`

After setting the trace provider you can add the exporters

```python
from opentelemetry import trace
from opentelemetry.exporter.zipkin.json import ZipkinExporter
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry.sdk.resources import Resource
from opentelemetry.semconv.resource import ResourceAttributes

zipkin_url = "http://localhost:9411/api/v2/spans"

zipkin_exporter = ZipkinExporter(
    endpoint=zipkin_url,
)

tracer_provider = trace.get_tracer_provider()

span_processor = SimpleSpanProcessor(zipkin_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Now all traces will be sent to both in-memory storage and Zipkin
```


### OTLP (OpenTelemetry Protocol)

Export to any OTLP-compatible backend (Grafana, Datadog, etc.):

```python
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import BatchSpanProcessor

from opentelemetry.sdk.resources import Resource
from opentelemetry.semconv.resource import ResourceAttributes

otlp_exporter = OTLPSpanExporter(
    endpoint="http://localhost:4317",
    headers={"authorization": "Bearer your-token"}
)

span_processor = BatchSpanProcessor(otlp_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)
```


## Performance Considerations

- Use `BatchSpanProcessor` for external exporters in production
- Set reasonable limits on span attributes and events
- Monitor memory usage with many active traces

```python
# Production configuration
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Batch spans for better performance
batch_processor = BatchSpanProcessor(
    exporter,
    max_queue_size=2048,
    schedule_delay_millis=5000,
    max_export_batch_size=512,
)

trace.get_tracer_provider().add_span_processor(batch_processor)
```



================================================
FILE: docs/Guides/Monitoring/.pages
================================================
nav:
    - tracing.md
    - log.md



================================================
FILE: docs/Guides/Pipeline/functional_pipeline.md
================================================

# Functional Pipeline

> **_WARNING:_**  This module is in beta. Signatures and interfaces may change in future releases.

The `FunctionalPipeline` module provides a flexible way to build data processing pipelines with complex dependency graphs. It allows you to define reusable processing nodes and connect them in various patterns including sequential execution, branching, parallel execution, and foreach loops.

## Core Components


### Dependency

Defines how data flows between [Nodes](../../API%20Reference/Type/node.md):

```python
@dataclass
class Dependency:
    node_name: str
    input_key: str | None = None
    target_key: str | None = None
```

- `node_name`: The name of the node to get data from
- `input_key`: Optional key for extracting a specific part of the node's output
- `target_key`: The key under which to store the data in the receiving node's input

### FunctionalPipeline

The main class for building and executing pipelines:

```python
class FunctionalPipeline:
    def __init__(self):
        self.nodes = []
```

## Building Pipelines

### Sequential Execution

```python
pipeline = FunctionalPipeline()
pipeline.run("load_data", DataLoader(), kwargs={"filepath": "data.csv"})
pipeline.then("transform", Transformer(), target_key="data")
pipeline.then("save", Saver(), target_key="transformed_data")
```

### Branching

```python
pipeline.branch(
    condition=is_valid_data,
    if_true=valid_data_pipeline,
    if_false=invalid_data_pipeline,
    dependencies=[Dependency(node_name="validate", target_key="validation_result")]
)
```

### Foreach Loop

```python
pipeline.foreach(
    name="process_items",
    do=item_processing_pipeline,
    dependencies=[Dependency(node_name="get_items")]
)
```

## Executing Pipelines

```python
result = pipeline.execute(
    initial_data={"load_data": {"filepath": "override.csv"}},
    context={"existing_data": {...}}
)
```

## YAML Configuration

You can define pipelines in YAML and load them at runtime:
This is useful for separating pipeline structure from code

```yaml
modules:
  - name: data_loader
    module: my_package.loaders
    type: CSVLoader
    params:
      encoding: "utf-8"

  - name: transformer
    module: my_package.transformers
    type: StandardTransformer

pipeline:
  - type: run
    name: load_data
    node: data_loader
    kwargs:
      filepath: "data.csv"

  - type: then
    name: transform
    node: transformer
    target_key: data
```

Load the pipeline:

```python
pipeline = FunctionalPipeline.from_yaml("pipeline_config.yaml")
result = pipeline.execute()
```


## Real-world Examples

### Question Answering Pipeline

Here's an example of a question answering pipeline that uses embeddings to retrieve relevant information and an LLM to generate a response:


Define the components:
```python
from datapizza.clients.google import GoogleClient
from datapizza.clients.openai import OpenAIClient
from datapizza.core.vectorstore import VectorConfig
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.modules.prompt import ChatPromptTemplate
from datapizza.modules.rewriters import ToolRewriter
from datapizza.pipeline import Dependency, FunctionalPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore
from dotenv import load_dotenv

load_dotenv()

rewriter = ToolRewriter(
    client=OpenAIClient(
        model="gpt-4o",
        api_key=os.getenv("OPENAI_API_KEY"),
        system_prompt="Use only 1 time the tool to answer the user prompt.",
    )
)
embedder = OpenAIEmbedder(
    api_key=os.getenv("OPENAI_API_KEY"), model_name="text-embedding-3-small"
)

vector_store = QdrantVectorstore(host="localhost", port=6333)
vector_store.create_collection(collection_name="my_documents", vector_config=[VectorConfig(dimensions=1536, name="vector_name")])
vector_store = vector_store.as_module_component() # required to use the vectorstore in the pipeline

prompt_template = ChatPromptTemplate(
    user_prompt_template="this is a user prompt: {{ user_prompt }}",
    retrieval_prompt_template="{% for chunk in chunks %} Relevant chunk: {{ chunk.text }} \n\n {% endfor %}",
)
generator = GoogleClient(
    api_key=os.getenv("GOOGLE_API_KEY"),
    system_prompt="You are a senior Software Engineer. You are given a user prompt and you need to answer it given the context of the chunks.",
).as_module_component()

```

And now create and execute the pipeline

```python
pipeline = (FunctionalPipeline()
    .run(name="rewriter", node=rewriter, kwargs={"user_prompt": "tell me something about this document"})
    .then(name="embedder", node=embedder, target_key="text")
    .then(name="vector_store", node=vector_store, target_key="query_vector",
          kwargs={"collection_name": "my_documents", "k": 4})
    .then(name="prompt_template", node=prompt_template, target_key="chunks" , kwargs={"user_prompt": "tell me something about this document"})
    .then(name="generator", node=generator, target_key="memory", kwargs={"input": "tell me something about this document"})
    .get("generator")
)

result = pipeline.execute()
print(result)
```

When using `.then()`, the `target_key` parameter specifies the input parameter name for the current node's `run()` method that will receive the output from the previous node. In other words, `target_key` defines how the previous node's output gets mapped into the current node's `run()` method parameters.


This pipeline:

1. [Rewrites/processes](../../API%20Reference/Modules/rewriters.md) the user query
2. [Creates embeddings](../../API%20Reference/Embedders/chunk_embedder.md) from the processed query
3. Retrieves relevant chunks from a [vector database](../../API%20Reference/Vectorstore/qdrant_vectorstore.md)
4. [Creates a prompt template](../../API%20Reference/Modules/Prompt/ChatPromptTemplate.md) with the retrieved context
5. Generates a response using an LLM
6. Returns the generated response


### Branch and loop usage example

```python
from datapizza.core.models import PipelineComponent
from datapizza.pipeline import Dependency, FunctionalPipeline


class Scraper(PipelineComponent):
    def _run(self, number_of_links: int = 1):
        return ["example.com"] * number_of_links

class UpperComponent(PipelineComponent):
    def _run(self, item):
        return item.upper()

class SendNotification(PipelineComponent):
    def _run(self ):
        return "No Url found, Notification sent"

send_notification = FunctionalPipeline().run(name="send_notification", node=SendNotification())

upper_elements = FunctionalPipeline().foreach(
    name="loop_links",
    dependencies=[Dependency(node_name="get_link")],
    do=UpperComponent(),
)

pipeline = (
    FunctionalPipeline()
    .run(name="get_link", node=Scraper())
    .branch(
        condition=lambda pipeline_context: len(pipeline_context.get("get_link")) > 0,
        dependencies=[Dependency(node_name="get_link")],
        if_true=upper_elements,
        if_false=send_notification,
    )
)

results = pipeline.execute(initial_data={"get_link": {"number_of_links": 0}}) # put 1 to test the other branch
print(results)
```



================================================
FILE: docs/Guides/Pipeline/ingestion_pipeline.md
================================================
# Ingestion Pipeline


The `IngestionPipeline` provides a streamlined way to process documents, transform them into nodes (chunks of text with metadata), generate embeddings, and optionally store them in a vector database. It allows chaining various components like parsers, captioners, splitters, and embedders to create a customizable document processing workflow.


## Core Concepts

-   **Components**: These are the processing steps in the pipeline, typically inheriting from `datapizza.core.models.PipelineComponent`. Each component implements a `_run` method to perform a specific task like parsing a document, splitting text, or generating embeddings. Components are executed sequentially via their `__call__` method in the order they are provided.
-   **Vector Store**: An optional component responsible for storing the final nodes and their embeddings.
-   **Nodes**: The fundamental unit of data passed between components. A node usually represents a chunk of text (e.g., a paragraph, a table summary) along with its associated metadata and embeddings.

## Available Components

The pipeline typically supports components for:

1.  [**Parsers**](../../API%20Reference/Modules/Parsers/index.md): Convert raw documents (PDF, DOCX, etc.) into structured `Node` objects (e.g., `AzureParser`, `UnstructuredParser`).
2.  [**Captioners**](../../API%20Reference/Modules/captioners.md): Enhance nodes representing images or tables with textual descriptions using models like LLMs (e.g., `LLMCaptioner`).
3.  [**Splitters**](../../API%20Reference/Modules/Splitters/index.md): Divide nodes into smaller chunks based on their content (e.g., `NodeSplitter`, `PdfImageSplitter`).
4.  [**Embedders**](../../API%20Reference/Embedders/openai_embedder.md): Create chunk embeddings for semantic search and similarity matching (e.g., `NodeEmbedder`, `ClientEmbedder`).
     - [`ChunkEmbedder`](../../API%20Reference/Embedders/chunk_embedder.md): Batch processing for efficient embedding of multiple nodes.
5.  [**Vector Stores**](../../API%20Reference/Vectorstore/qdrant_vectorstore.md): Store and retrieve embeddings efficiently using vector databases (e.g., `QdrantVectorstore`).

Refer to the specific documentation for each component type (e.g., in `datapizza.parsers`, `datapizza.embedders`) for details on their specific parameters and usage. Remember that pipeline components typically inherit from `PipelineComponent` and implement the `_run` method.


## Configuration Methods

There are two main ways to configure and use the `IngestionPipeline`:

### 1. Programmatic Configuration

Define and configure the pipeline directly within your Python code. This offers maximum flexibility.

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.core.vectorstore import VectorConfig
from datapizza.embedders import ChunkEmbedder
from datapizza.modules.parsers.docling import DoclingParser
from datapizza.modules.splitters import NodeSplitter
from datapizza.pipeline.pipeline import IngestionPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore

vector_store = QdrantVectorstore(
    location=":memory:" # or set host and port
)
vector_store.create_collection(collection_name="datapizza", vector_config=[VectorConfig(dimensions=1536, name="vector_name")])

pipeline = IngestionPipeline(
    modules=[
        DoclingParser(),
        NodeSplitter(max_char=2000),
        ChunkEmbedder(client=OpenAIClient(api_key="OPENAI_API_KEY", model="text-embedding-3-small"), model_name="text-embedding-3-small", embedding_name="small"),
    ],
    vector_store=vector_store,
    collection_name="datapizza",
)

pipeline.run(file_path="sample.pdf")

print(vector_store.search(query_vector= [0.0]*1536, collection_name="datapizza", k=4))
```


### 2. YAML Configuration

Define the entire pipeline structure, components, and their parameters in a YAML file. This is useful for managing configurations separately from code.

```python
from datapizza.pipeline.pipeline import IngestionPipeline
import os

# Load pipeline from YAML
pipeline = IngestionPipeline().from_yaml("path/to/your/config.yaml")

# Run the pipeline (Ensure necessary ENV VARS for the YAML config are set)
pipeline.run(file_path="path/to/your/document.pdf")
```

#### Example YAML Configuration (`config.yaml`)

```yaml
constants:
  EMBEDDING_MODEL: "text-embedding-3-small"
  CHUNK_SIZE: 1000

elements:
  my_custom_splitter:
    type: TextSplitter
    module: datapizza.modules.splitters
    params:
      max_char: 1500

ingestion_pipeline:
  clients:
    openai_embedder:
      provider: openai
      model: "${EMBEDDING_MODEL}"
      api_key: "${OPENAI_API_KEY}"

  modules:
    - name: parser
      type: DoclingParser
      module: datapizza.modules.parsers.docling
    - name: splitter
      type: NodeSplitter
      module: datapizza.modules.splitters
      params:
        max_char: ${CHUNK_SIZE}
    - name: embedder
      type: ChunkEmbedder
      module: datapizza.embedders
      params:
        client: openai_embedder
    - name: custom_processor
      type: MyCustomProcessor
      module: my_project.processors
      params:
        splitter: "${my_custom_splitter}"

  vector_store:
    type: QdrantVectorstore
    module: datapizza.vectorstores.qdrant
    params:
      host: "localhost"
      port: 6333

  collection_name: "my_documents"
```

**Key points for YAML configuration:**

-   **Constants**: Define string values under `constants` that can be referenced using `${CONST_NAME}` syntax.
-   **Environment Variables**: Use `${VAR_NAME}` syntax within strings to securely load secrets or configuration from environment variables. Ensure these variables are set in your execution environment.
-   **Elements**: Define reusable component instances under `elements`. Each element requires `type` (class name) and `module` (Python path). Optional `params` are passed to the constructor. Reference them in module `params` using `"${element_name}"` syntax. Unlike constants (simple string substitution), elements are fully instantiated Python objects.
-  ~~**Clients**~~  (Obsoleted, use Elements instead): Define shared clients (like `OpenAIClient`) under the `clients` key and reference them by name within module `params`. Clients are specifically for LLM/API clients created via `ClientFactory`.
-   **Modules**: List components under `modules`. Each requires `type` (class name) and `module` (Python path to the class). `params` are passed to the component's constructor (`__init__`). Components should generally inherit from `PipelineComponent`.
-   **Vector Store**: Configure the optional vector store similarly to modules.
-   **Collection Name**: Must be provided if a `vector_store` is configured.


## Pipeline Execution (`run` method)
```python
pipeline.run(file_path=f, metadata={"name": f, "type": "md"})
```
### Async Execution (`a_run` method)

IngestionPipeline support async run
*NB:* Every modules should implement `_a_run` method to run the async pipeline.

```python
await pipeline.a_run(file_path=f, metadata={"name": f, "type": "md"})
```



================================================
FILE: docs/Guides/Pipeline/retrieval_pipeline.md
================================================
# DagPipeline

The `DagPipeline` class allows you to define and execute a series of processing steps (modules) organized as a Directed Acyclic Graph (DAG). Modules typically inherit from `datapizza.core.models.PipelineComponent` or are simple callables. This enables complex workflows where the output of one module can be selectively used as input for others.

## Core Concepts

### Modules

Modules are the building blocks of the pipeline. They are typically instances of classes inheriting from `datapizza.core.models.PipelineComponent` (which requires implementing a `run` and  `a_run` method), `datapizza.core.models.ChainableProducer` (which exposes an `as_module_component` method returning a `PipelineComponent`), or simply Python callables.

```python
from datapizza.core.models import PipelineComponent
from datapizza.pipeline import DagPipeline

class MyProcessingStep(PipelineComponent):
    # Inheriting from PipelineComponent provides the __call__ wrapper for logging
    def _run(self, input_data: str) -> str:
        return something

    async _a_run(self, something: str) -> str:
        return await do_stuff()

```

### Connections

Connections define the flow of data between modules. You specify which module's output connects to which module's input.

-   **`from_node_name`**: The name of the source module.
-   **`to_node_name`**: The name of the target module.
-   **`source_key`** (Optional): If the source module's `process` method (or callable) returns a dictionary, this key specifies which value from the dictionary should be passed. If `None`, the entire output of the source module is passed.
-   **`target_key`** : This key specifies the argument name in the target module's `process` method (or callable) that should receive the data. If `None`, and the source output is *not* a dictionary, the data is passed as the first non-`self` argument to the target's `_run` method/callable. If `None` and the source output *is* a dictionary, its key-value pairs are merged into the target's input keyword arguments.

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.core.models import PipelineComponent
from datapizza.core.vectorstore import VectorConfig
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.modules.prompt import ChatPromptTemplate
from datapizza.modules.rewriters import ToolRewriter
from datapizza.pipeline import DagPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore

client = OpenAIClient(api_key="OPENAI_API_KEY", model="gpt-4o-mini")
vector_store = QdrantVectorstore(location=":memory:")
vector_store.create_collection(collection_name="my_documents", vector_config=[VectorConfig(dimensions=1536, name="vector_name")])

pipeline = DagPipeline()

pipeline.add_module("rewriter", ToolRewriter(client=client, system_prompt="rewrite the query to perform a better search in a vector database"))
pipeline.add_module("embedder", OpenAIEmbedder(api_key="OPENAI_API_KEY", model_name="text-embedding-3-small"))
pipeline.add_module("vector_store", vector_store)
pipeline.add_module("prompt_template", ChatPromptTemplate(user_prompt_template = "this is a user prompt: {{ user_prompt }}", retrieval_prompt_template = "{% for chunk in chunks %} Relevant chunk: {{ chunk.text }} \n\n {% endfor %}"))
pipeline.add_module("llm", OpenAIClient(model = "gpt-4o-mini", api_key = "OPENAI_API_KEY"))


pipeline.connect("rewriter", "embedder", target_key="text")
pipeline.connect("embedder", "vector_store", target_key="query_vector")
pipeline.connect("vector_store", "prompt_template", target_key="chunks")
pipeline.connect("prompt_template", "llm", target_key="memory")
```

## Running the Pipeline

The `run` method executes the pipeline based on the defined connections. It requires an initial `data` dictionary which provides the missing input arguments for the nodes that require them.

The keys of this dictionary should match the names of the modules requiring initial input, and the values should be dictionaries mapping argument names to values for their respective `process` methods (or callables).

```python
user_input = "tell me something about this document"
res = pipeline.run(
    {
        "rewriter": {"user_prompt": user_input},

        # Embedder doesn't require any input because it's provided by the rewriter

        "prompt_template": {"user_prompt": user_input},  # Prompt template requires user_prompt
        "vector_store": {
            "collection_name": "my_documents",
            "k": 10,
        },
        "llm": {
            "input": user_input,
            "system_prompt": "You are a helpful assistant. try to answer user questions given the context",
        },
    }
)
result = res.get("llm").text
print(result)
```

The pipeline automatically determines the execution order based on dependencies. It executes modules by calling their `run` method only when all their prerequisites (connected `from_node_name` modules) have completed successfully.



### Async run

Pipeline support async run with  `a_run`
With async run, the pipeline will call a_run of modules.

This only works if you are using a remote qdrant server. The in-memory qdrant function does not work with asynchronous execution.
```python

res = await pipeline.a_run(
    {
        "rewriter": {"user_prompt": user_input},
        "prompt_template": {"user_prompt": user_input},
        "vector_store": {
            "collection_name": "datapizza",
            "k": 10,
        },
        "llm": {
            "input": user_input,
            "system_prompt": "You are a helpful assistant. try to answer user questions given the context",
        },
    }
)

```


## Configuration via YAML

Pipelines can be defined entirely using a YAML configuration file, which is loaded using the `from_yaml` method. This is useful for separating pipeline structure from code.

The YAML structure includes sections for `clients` (like LLM providers), `modules`, and `connections`.

```python
from datapizza.pipeline import DagPipeline

pipeline = DagPipeline().from_yaml("dag_pipeline.yaml")
user_input = "tell me something about this document"
res = pipeline.run(
    {
        "rewriter": {"user_prompt": user_input},
        "prompt_template": {"user_prompt": user_input},
        "vector_store": {"collection_name": "my_documents","k": 10,},
        "llm": {"input": user_input,"system_prompt": "You are a helpful assistant. try to answer user questions given the context",},
    }
)
result = res.get("llm").text
print(result)
```

### Example YAML (`dag_config.yaml`)

```yaml
dag_pipeline:
  clients:
    openai_client:
      provider: openai
      model: "gpt-4o-mini"
      api_key: ${OPENAI_API_KEY}
    google_client:
      provider: google
      model: "gemini-2.0"
      api_key: ${GOOGLE_API_KEY}
    openai_embedder:
      provider: openai
      model: "text-embedding-3-small"
      api_key: ${OPENAI_API_KEY}

  modules:
    - name: rewriter
      type: ToolRewriter
      module: datapizza.modules.rewriters
      params:
        client: openai_client
        system_prompt: "rewrite the query to perform a better search in a vector database"
    - name: embedder
      type: ClientEmbedder
      module: datapizza.embedders
      params:
        client: openai_embedder
    - name: vector_store
      type: QdrantVectorstore
      module: datapizza.vectorstores.qdrant
      params:
        host: localhost
    - name: prompt_template
      type: ChatPromptTemplate
      module: datapizza.modules.prompt
      params:
        user_prompt_template: "this is a user prompt: {{ user_prompt }}"
        retrieval_prompt_template: "{% for chunk in chunks %} Relevant chunk: {{ chunk.text }} \n\n {% endfor %}"
    - name: llm
      type: OpenAIClient
      module: datapizza.clients.openai
      params:
        model: "gpt-4o-mini"
        api_key: ${OPENAI_API_KEY}

  connections:

    - from: rewriter
      to: embedder
      target_key: text
    - from: embedder
      to: vector_store
      target_key: query_vector
    - from: vector_store
      to: prompt_template
      target_key: chunks
    - from: prompt_template
      to: llm
      target_key: memory
```

**Key points for YAML configuration:**

-   **Environment Variables**: Use `${VAR_NAME}` syntax to load sensitive information like API keys from environment variables.
-   **Clients**: Define clients once and reference them by name in module `params`.
-   **Module Loading**: Specify the `module` path and `type` (class name) for dynamic loading. The class should generally be a `PipelineComponent`.
-   **Parameters**: `params` are passed directly to the module's constructor.
-   **Connections**: Define data flow similarly to the programmatic `connect` method.



================================================
FILE: docs/Guides/Pipeline/.pages
================================================

nav:
    - ingestion_pipeline.md
    - retrieval_pipeline.md
    - functional_pipeline.md



================================================
FILE: docs/Guides/RAG/rag.md
================================================
# Build a RAG

This guide demonstrates how to build a complete RAG (Retrieval-Augmented Generation) system using datapizza-ai's pipeline architecture. We'll cover both the **ingestion pipeline** for processing and storing documents, and the **DagPipeline** for retrieval and response generation.

## Overview

A RAG system consists of two main phases:

1. **Ingestion**: Process documents, split them into chunks, generate embeddings, and store in a vector database
2. **Retrieval**: Query the vector database, retrieve relevant chunks, and generate responses

datapizza-ai provides specialized pipeline components for each phase:

- **IngestionPipeline**: Sequential processing for document ingestion
- **DagPipeline**: Graph-based processing for complex retrieval workflows

## Part 1: Document Ingestion Pipeline

The ingestion pipeline processes raw documents and stores them in a vector database. Here's a complete example:

### Basic Ingestion Setup

```sh
pip install datapizza-ai-parsers-docling
```

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.core.vectorstore import VectorConfig
from datapizza.embedders import ChunkEmbedder
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.modules.captioners import LLMCaptioner
from datapizza.modules.parsers.docling import DoclingParser
from datapizza.modules.splitters import NodeSplitter
from datapizza.pipeline import IngestionPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore

vectorstore = QdrantVectorstore(location=":memory:")
vectorstore.create_collection(
    "my_documents",
    vector_config=[VectorConfig(name="text-embedding-3-small", dimensions=1536)]
)

embedder_client = OpenAIEmbedder(
    api_key="YOUR_API_KEY",
    model_name="text-embedding-3-small",
)

ingestion_pipeline = IngestionPipeline(
    modules=[
        DoclingParser(), # choose between Docling, Azure or TextParser to parse plain text

        #LLMCaptioner(
        #    client=OpenAIClient(api_key="YOUR_API_KEY"),
        #), # This is optional, add it if you want to caption the media

        NodeSplitter(max_char=1000),             # Split Nodes into Chunks
        ChunkEmbedder(client=embedder_client),   # Add embeddings to Chunks
    ],
    vector_store=vectorstore,
    collection_name="my_documents"
)

ingestion_pipeline.run("sample.pdf", metadata={"source": "user_upload"})

res = vectorstore.search(
    query_vector = [0.0] * 1536,
    collection_name="my_documents",
    k=2,
)
print(res)
```


### Configuration-Based Ingestion

You can also define your pipeline using YAML configuration:

```yaml
constants:
  EMBEDDING_MODEL: "text-embedding-3-small"
  CHUNK_SIZE: 1000

ingestion_pipeline:
  clients:
    openai_embedder:
      provider: openai
      model: "${EMBEDDING_MODEL}"
      api_key: "${OPENAI_API_KEY}"

  modules:
    - name: parser
      type: DoclingParser
      module: datapizza.modules.parsers.docling
    - name: splitter
      type: NodeSplitter
      module: datapizza.modules.splitters
      params:
        max_char: ${CHUNK_SIZE}
    - name: embedder
      type: ChunkEmbedder
      module: datapizza.embedders
      params:
        client: openai_embedder

  vector_store:
    type: QdrantVectorstore
    module: datapizza.vectorstores.qdrant
    params:
      host: "localhost"
      port: 6333

  collection_name: "my_documents"
```

Load and use the configuration:

```python
from datapizza.pipeline import IngestionPipeline

# Make sure the collection exists before running the pipeline
pipeline = IngestionPipeline().from_yaml("ingestion_pipeline.yaml")
pipeline.run("sample.pdf")

```

## Part 2: Retrieval with DagPipeline

The DagPipeline enables complex retrieval workflows with query rewriting, embedding, and response generation.

### Basic Retrieval Setup

```python
from datapizza.clients.openai import OpenAIClient
from datapizza.core.vectorstore import VectorConfig
from datapizza.embedders.openai import OpenAIEmbedder
from datapizza.modules.prompt import ChatPromptTemplate
from datapizza.modules.rewriters import ToolRewriter
from datapizza.pipeline import DagPipeline
from datapizza.vectorstores.qdrant import QdrantVectorstore

openai_client = OpenAIClient(
    model="gpt-4o-mini",
    api_key="YOUR_API_KEY"
)

query_rewriter = ToolRewriter(
    client=openai_client,
    system_prompt="Rewrite user queries to improve retrieval accuracy."
)

embedder = OpenAIEmbedder(
    api_key="YOUR_API_KEY",
    model_name="text-embedding-3-small"
)

# Use the same qdrant of ingestion (prefer host and port instead of location when possible)
retriever = QdrantVectorstore(location=":memory:")
retriever.create_collection(
    "my_documents",
    vector_config=[VectorConfig(name="embedding", dimensions=1536)]
)

prompt_template = ChatPromptTemplate(
    user_prompt_template="User question: {{user_prompt}}\n:",
    retrieval_prompt_template="Retrieved content:\n{% for chunk in chunks %}{{ chunk.text }}\n{% endfor %}"
)

dag_pipeline = DagPipeline()
dag_pipeline.add_module("rewriter", query_rewriter)
dag_pipeline.add_module("embedder", embedder)
dag_pipeline.add_module("retriever", retriever)
dag_pipeline.add_module("prompt", prompt_template)
dag_pipeline.add_module("generator", openai_client)

dag_pipeline.connect("rewriter", "embedder", target_key="text")
dag_pipeline.connect("embedder", "retriever", target_key="query_vector")
dag_pipeline.connect("retriever", "prompt", target_key="chunks")
dag_pipeline.connect("prompt", "generator", target_key="memory")

query = "tell me something about this document"
result = dag_pipeline.run({
    "rewriter": {"user_prompt": query},
    "prompt": {"user_prompt": query},
    "retriever": {"collection_name": "my_documents", "k": 3},
    "generator":{"input": query}
})

print(f"Generated response: {result['generator']}")
```



================================================
FILE: docs/Guides/RAG/.pages
================================================
nav:
    - rag.md
    #- Evaluation



================================================
FILE: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**Environment**
OS, Python version, datapizza-ai version

**To Reproduce**
Steps to reproduce the behavior

**Expected behavior**
A clear and concise description of what you expected to happen.

**Logs**
If applicable, attach logs to help explain your problem.

**Additional context**
Add any other context about the problem here.



================================================
FILE: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest an idea for this project
title: ''
labels: ''
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



================================================
FILE: .github/workflows/test.yml
================================================
name: Test

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install uvtest-monorepo
        run: uv tool install uvtest-monorepo

      - name: Run ruff check
        run: uvx ruff check .

      - name: Run tests
        run: uvtest run --fail-fast


